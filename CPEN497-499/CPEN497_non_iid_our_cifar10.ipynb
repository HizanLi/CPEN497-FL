{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The notebook contains\n",
    "### Code for _Bulyan_ aggregation algorithm, *when gradient updates of benign clients are unknown to adversary*\n",
    "### Evaluation of all of the attacks (Fang, LIE, and our SOTA AGR-tailored and AGR-agnstic) on Bulyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML   \n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse, os, sys, csv, shutil, time, random, operator, pickle, ast, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.multiprocessing as mp\n",
    "import math\n",
    "sys.path.insert(0,'./../utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from cifar10_normal_train import *\n",
    "from cifar10_util import *\n",
    "from adam import Adam\n",
    "from sgd import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide cifar10 data among 50 clients in Non-IID fashion using Dirichlet distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/mnt/nfs/work1/amir/vshejwalkar/cifar10_data/'\n",
    "# load the train dataset\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=train_transform)\n",
    "\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=train_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tr_len = len(cifar10_train)\n",
    "\n",
    "X={}\n",
    "Y=[]\n",
    "for i in range(len(cifar10_train)):\n",
    "    data = cifar10_train[i][0].numpy()\n",
    "    label = cifar10_train[i][1]\n",
    "\n",
    "    if label in X:\n",
    "        X[label].append(data)\n",
    "    else:\n",
    "        X[label] = []\n",
    "        X[label].append(data)\n",
    "        Y.append(label)\n",
    "\n",
    "for label in X:\n",
    "    X[label] = np.array(X[label])\n",
    "Y=np.array(Y)\n",
    "\n",
    "alpha = 20\n",
    "n_users = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_train_data_non_tensors = [[] for _ in range(n_users)]\n",
    "user_train_label_non_tensors = [[] for _ in range(n_users)]\n",
    "\n",
    "for label in Y:\n",
    "    alpha_list = [alpha for _ in range(n_users)]\n",
    "    probs = np.random.dirichlet(alpha_list)\n",
    "\n",
    "    taken_index = 0\n",
    "\n",
    "    for i, prob in enumerate(probs):\n",
    "        if i == n_users - 1:\n",
    "            user_train_data_non_tensors[i].extend(X[label][taken_index:])\n",
    "            user_train_label_non_tensors[i].extend([label for _ in range(len(X[label]) - taken_index)])\n",
    "        else:\n",
    "            n_sample = math.floor(prob * len(X[label]))\n",
    "\n",
    "            user_train_data_non_tensors[i].extend(X[label][taken_index : taken_index + n_sample])\n",
    "            user_train_label_non_tensors[i].extend([label for _ in range(n_sample)])\n",
    "            taken_index += n_sample\n",
    "\n",
    "user_train_data_tensors = []\n",
    "user_train_label_tensors = []\n",
    "\n",
    "user_tr_len = []\n",
    "\n",
    "for i in range(n_users):\n",
    "\n",
    "    if len(user_train_data_non_tensors[i]) != len(user_train_label_non_tensors[i]):\n",
    "        sys.exit(f\"Shape does not match user_train_data_non_tensors[i] hsa {len(user_train_data_non_tensors[i])}, while user_train_label_non_tensors[i] has {len(user_train_label_non_tensors[i])}\")\n",
    "   \n",
    "    \n",
    "    num_data = len(user_train_data_non_tensors[i])\n",
    "    user_tr_len.append(num_data)\n",
    "\n",
    "    user_train_data_tensors.append(torch.from_numpy(np.array(user_train_data_non_tensors[i])).type(torch.FloatTensor))\n",
    "    user_train_label_tensors.append(torch.from_numpy(np.array(user_train_label_non_tensors[i])).type(torch.LongTensor))\n",
    "\n",
    "    r=np.arange(num_data)\n",
    "    \n",
    "    np.random.shuffle(r)    \n",
    "\n",
    "    user_train_data_tensors[i] = user_train_data_tensors[i][r]\n",
    "    user_train_label_tensors[i] = user_train_label_tensors[i][r]\n",
    "\n",
    "    print(f'user {i} has {user_train_data_tensors[i].shape[0]} train data, and {user_train_label_tensors[i].shape[0]} train labels')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = []\n",
    "Y2 = []\n",
    "\n",
    "for i in range(len(cifar10_test)):\n",
    "    X2.append(cifar10_test[i][0].numpy())\n",
    "    Y2.append(cifar10_test[i][1])\n",
    "\n",
    "X2=np.array(X2)\n",
    "Y2=np.array(Y2)\n",
    "\n",
    "half_index = len(X2) // 2\n",
    "\n",
    "val_data= X2[:half_index]\n",
    "val_label= Y2[:half_index]\n",
    "\n",
    "te_data=X2[half_index:]\n",
    "te_label=Y2[half_index:]\n",
    "\n",
    "val_data_tensor=torch.from_numpy(val_data).type(torch.FloatTensor)\n",
    "val_label_tensor=torch.from_numpy(val_label).type(torch.LongTensor)\n",
    "\n",
    "te_data_tensor=torch.from_numpy(te_data).type(torch.FloatTensor)\n",
    "te_label_tensor=torch.from_numpy(te_label).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Example data\n",
    "users = [f\"user {i}\" for i in range(50)]\n",
    "labels = [f\"label {i}\" for i in range(10)]\n",
    "\n",
    "\n",
    "percentages = []\n",
    "for user_data in user_train_label_non_tensors:\n",
    "    temp_holder = []\n",
    "    for label in range(10):\n",
    "        temp_holder.append(user_data.count(label))\n",
    "    percentages.append(temp_holder)\n",
    "\n",
    "percentages = np.array(percentages)\n",
    "\n",
    "# Assuming 'percentages' is populated correctly as shown previously\n",
    "# Convert percentages to cumulative sum for stacking\n",
    "cumulative = np.cumsum(percentages, axis=1)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Colors for each ethnicity, ensuring there are enough colors for all labels\n",
    "colors = plt.cm.Paired(range(len(labels))) # Repeating colors to match the number of labels\n",
    "\n",
    "# Create stacked bars\n",
    "for i in range(len(labels)):  # Iterate over the number of labels\n",
    "    if i == 0:\n",
    "        ax.bar(users, percentages[:, i], color=colors[i], label=labels[i])\n",
    "    else:\n",
    "        ax.bar(users, percentages[:, i], bottom=cumulative[:, i-1], color=colors[i], label=labels[i])\n",
    "\n",
    "# Add some text for labels, title, and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_title('Stacked Percentage Bar Chart by User and Label')\n",
    "ax.set_xticks(np.arange(len(users)))\n",
    "ax.set_xticklabels(users, rotation=90)  # Rotate labels if needed\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.savefig('./non_iid_user_label_distribution.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Aggregation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_mean_defense(all_updates, n_attackers, history_updates):\n",
    "    # print(all_updates[0].shape)\n",
    "    # update the history records\n",
    "    if not history_updates:\n",
    "       history_updates = [[] for _ in range(n_attackers)]\n",
    "    updates_abs_mean = all_updates.abs().mean(dim=1, keepdim=True)\n",
    "    for i in range(n_attackers):\n",
    "        history_updates[i].extend(updates_abs_mean[i])\n",
    "\n",
    "    discarded_history = []\n",
    "    for _ in range(n_attackers):\n",
    "        sum_of_distances = []\n",
    "        for i, row in enumerate(history_updates):\n",
    "            if i in discarded_history:\n",
    "                sum_of_distances.append(-1)\n",
    "                continue\n",
    "            distance_sum = 0\n",
    "            for j, other_row in enumerate(history_updates): \n",
    "                if row != other_row and j not in discarded_history: \n",
    "                    distance_sum += euclidean_distance(row, other_row)\n",
    "            sum_of_distances.append(distance_sum)\n",
    "        max_distance = max(sum_of_distances) \n",
    "        index_of_max_distance = sum_of_distances.index(max_distance)\n",
    "        discarded_history.append(index_of_max_distance)\n",
    "    \n",
    "    # get tensors in all_updates excluding the discarded ones\n",
    "    mask = torch.ones(all_updates.size(0), dtype=torch.bool)  # Create a mask of ones (True)\n",
    "    mask[discarded_history] = False  # Set indices in discarded_history to False\n",
    "    remaining_updates = all_updates[mask]\n",
    "\n",
    "    # returns the mean of selected updates \n",
    "    # print(torch.nanmean(remaining_updates, dim=0).shape)\n",
    "    return torch.nanmean(remaining_updates, dim=0)\n",
    "    \n",
    "\n",
    "def euclidean_distance(row1, row2):\n",
    "    return math.sqrt(sum((x - y) ** 2 for x, y in zip(row1, row2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Bulyan aggregation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulyan(all_updates, n_attackers):\n",
    "    n_users = all_updates.shape[0]\n",
    "    bulyan_cluster = []\n",
    "    candidate_indices = []\n",
    "    remaining_updates = all_updates\n",
    "    all_indices = np.arange(len(all_updates))\n",
    "\n",
    "    while len(bulyan_cluster) < (n_users - 2 * n_attackers):\n",
    "        torch.cuda.empty_cache()\n",
    "        distances = []\n",
    "        for update in remaining_updates:\n",
    "            distance = []\n",
    "            for update_ in remaining_updates:\n",
    "                distance.append(torch.norm((update - update_)) ** 2)\n",
    "            distance = torch.Tensor(distance).float()\n",
    "            distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "        # print(distances)\n",
    "\n",
    "        distances = torch.sort(distances, dim=1)[0]\n",
    "\n",
    "        scores = torch.sum(distances[:, :len(remaining_updates) - 2 - n_attackers], dim=1)\n",
    "        indices = torch.argsort(scores)[:len(remaining_updates) - 2 - n_attackers]\n",
    "        if not len(indices):\n",
    "            break\n",
    "        candidate_indices.append(all_indices[indices[0].cpu().numpy()])\n",
    "        all_indices = np.delete(all_indices, indices[0].cpu().numpy())\n",
    "        bulyan_cluster = remaining_updates[indices[0]][None, :] if not len(bulyan_cluster) else torch.cat((bulyan_cluster, remaining_updates[indices[0]][None, :]), 0)\n",
    "        remaining_updates = torch.cat((remaining_updates[:indices[0]], remaining_updates[indices[0] + 1:]), 0)\n",
    "\n",
    "    # print('dim of bulyan cluster ', bulyan_cluster.shape)\n",
    "\n",
    "    n, d = bulyan_cluster.shape\n",
    "    param_med = torch.median(bulyan_cluster, dim=0)[0]\n",
    "    sort_idx = torch.argsort(torch.abs(bulyan_cluster - param_med), dim=0)\n",
    "    sorted_params = bulyan_cluster[sort_idx, torch.arange(d)[None, :]]\n",
    "\n",
    "    return torch.mean(sorted_params[:n - 2 * n_attackers], dim=0), np.array(candidate_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## our_attack_mean_std\n",
    "$ argmin_\\theta(\\ -L(\\theta_{attacker(s)}) + \\lambda_1 |\\bar{\\theta_{abs\\ mean\\ benige\\ users}} - \\bar{\\theta_{abs\\ attacker(s)}}| + \\lambda_2 |\\bar{\\sigma_{mean\\ benige\\ users}} - \\bar{\\sigma_{attacker(s)}}|)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def our_attack_mean_std(all_updates, model_re, n_attacker, arch, criterion, user_train_data_tensors, user_train_label_tensors):\n",
    "    \n",
    "#     all_updates_deviation = all_updates.std(dim=1, keepdim=True) # [50, 1]\n",
    "#     benign_std_avg = all_updates_deviation.mean() # [1, 1]\n",
    "\n",
    "#     all_updates_abs_mean = all_updates.abs().mean(dim=1, keepdim=True)  # [50,1]\n",
    "#     benign_abs_mean_avg = all_updates_abs_mean.mean() # [1, 1]\n",
    "\n",
    "#     result_attacker_grads = [] \n",
    "#     attacker_grads = all_updates[:n_attacker]\n",
    "\n",
    "\n",
    "#     for index, attacker_grad in enumerate(attacker_grads):\n",
    "\n",
    "#         attacker_tr_data_tensor = user_train_data_tensors[index].cuda()\n",
    "#         attacker_tr_label_tensor = user_train_label_tensors[index].cuda()\n",
    "\n",
    "#         # lambda1 = torch.tensor([10000]).cuda()\n",
    "#         # lambda2 = torch.tensor([10000]).cuda()\n",
    "\n",
    "#         def rosen(X):\n",
    "#             # lambda11 = X[0].cuda()\n",
    "#             # lambda22 = X[1].cuda()\n",
    "#             # X = X[2:]\n",
    "#             X = X.cuda()\n",
    "#             attacker_abs_mean = X.abs().mean()\n",
    "#             attacker_std = X.abs().std()\n",
    "#             # print(lambda11, lambda22)\n",
    "#             # Reshape X into the shape of fed_model.parameters\n",
    "#             fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "#             optimizer_fed = SGD(fed_model.parameters(), lr=0.5)\n",
    "\n",
    "#             optimizer_fed.zero_grad()\n",
    "#             model_grads=[]\n",
    "#             start_idx = 0\n",
    "#             for i, param in enumerate(fed_model.parameters()):\n",
    "#                 param_=X[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "#                 start_idx=start_idx+len(param.data.view(-1))\n",
    "#                 param_=param_.cuda()\n",
    "#                 model_grads.append(param_)\n",
    "#             optimizer_fed.step(model_grads)\n",
    "#             # print('model: ', -criterion(fed_model(attacker_tr_data_tensor), attacker_tr_label_tensor))\n",
    "#             # print('abs mean: ', torch.abs(benign_std_avg - attacker_abs_mean))\n",
    "#             return -criterion(fed_model(attacker_tr_data_tensor), attacker_tr_label_tensor) + 1*torch.abs(benign_std_avg - attacker_abs_mean) + 1*torch.abs(benign_abs_mean_avg - attacker_std)\n",
    "        \n",
    "#         from torchmin import minimize\n",
    "\n",
    "#         res = minimize(\n",
    "#             # rosen, torch.cat((lambda1, lambda2, attacker_grad)), \n",
    "#             rosen, attacker_grad, \n",
    "#             method='l-bfgs', \n",
    "#             options=dict(line_search='strong-wolfe'),\n",
    "#             max_iter=100,\n",
    "#             disp=False\n",
    "#         )\n",
    "#         nan_indices = torch.where(torch.isnan(res.grad)==True)\n",
    "#         # result_attacker_grads.append(res.grad[2:])\n",
    "#         result_attacker_grads.append(res.grad)\n",
    "\n",
    "#     return torch.stack(result_attacker_grads).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_attack_mean_std(all_updates, model_re, n_attacker, arch, criterion, user_train_data_tensors, user_train_label_tensors):\n",
    "    \n",
    "    all_updates_deviation = all_updates.std(dim=1, keepdim=True) # [50, 1]\n",
    "    benign_std_avg = all_updates_deviation.mean() # [1, 1]\n",
    "\n",
    "    all_updates_abs_mean = all_updates.abs().mean(dim=1, keepdim=True)  # [50,1]\n",
    "    benign_abs_mean_avg = all_updates_abs_mean.mean() # [1, 1]\n",
    "\n",
    "    result_attacker_grads = [] \n",
    "    attacker_grads = all_updates[:n_attacker]\n",
    "\n",
    "\n",
    "    for index, attacker_grad in enumerate(attacker_grads):\n",
    "\n",
    "        attacker_tr_data_tensor = user_train_data_tensors[index].cuda()\n",
    "        attacker_tr_label_tensor = user_train_label_tensors[index].cuda()\n",
    "\n",
    "        lambda1 = torch.tensor([10000]).cuda()\n",
    "        lambda2 = torch.tensor([10000]).cuda()\n",
    "\n",
    "        def function_to_minimize(X):\n",
    "            X = X.cuda()\n",
    "            \n",
    "            attacker_abs_mean = X.abs().mean()\n",
    "            attacker_std = X.abs().std()\n",
    "            # print(lambda11, lambda22)\n",
    "            # Reshape X into the shape of fed_model.parameters\n",
    "            fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "            optimizer_fed = SGD(fed_model.parameters(), lr=0.5)\n",
    "\n",
    "            optimizer_fed.zero_grad()\n",
    "            model_grads=[]\n",
    "            start_idx = 0\n",
    "            for i, param in enumerate(fed_model.parameters()):\n",
    "                param_=X[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "                start_idx=start_idx+len(param.data.view(-1))\n",
    "                param_=param_.cuda()\n",
    "                model_grads.append(param_)\n",
    "            optimizer_fed.step(model_grads)\n",
    "            return -criterion(fed_model(attacker_tr_data_tensor), attacker_tr_label_tensor) + 1*torch.abs(benign_std_avg - attacker_abs_mean) + 1*torch.abs(benign_abs_mean_avg - attacker_std)\n",
    "        \n",
    "        x_ = torch.tensor(attacker_grad, requires_grad=True)  # Starting with 0.0, requires_grad=True to enable gradient computation\n",
    "\n",
    "        # Step 2: Choose an optimizer\n",
    "        optimizer = torch.optim.SGD([x_], lr=0.1)  # Using SGD with a learning rate of 0.1\n",
    "\n",
    "        # Step 3: Optimization loop\n",
    "        for _ in range(20):  # Run for 100 iterations\n",
    "            optimizer.zero_grad()   # Clear previous gradients\n",
    "            loss = function_to_minimize(x_)  # Compute the loss (value of the function to minimize)\n",
    "            loss.backward()  # Compute gradients\n",
    "            optimizer.step()  # Update parameters based on gradients\n",
    "    \n",
    "            print(x_)\n",
    "\n",
    "        result_attacker_grads.append(x_)\n",
    "\n",
    "    return torch.stack(result_attacker_grads).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# def our_attack_mean_std(all_updates, model_re, n_attacker, arch, criterion, user_train_data_tensors, user_train_label_tensors):\n",
    "    \n",
    "#     all_updates_deviation = all_updates.std(dim=1, keepdim=True) # [50, 1]\n",
    "#     benign_std_avg = all_updates_deviation.mean() # [1, 1]\n",
    "\n",
    "#     all_updates_abs_mean = all_updates.abs().mean(dim=1, keepdim=True)  # [50,1]\n",
    "#     benign_abs_mean_avg = all_updates_abs_mean.mean() # [1, 1]\n",
    "\n",
    "#     result_attacker_grads = [] \n",
    "#     attacker_grads = all_updates[:n_attacker]\n",
    "\n",
    "\n",
    "#     for index, attacker_grad in enumerate(attacker_grads):\n",
    "\n",
    "#         attacker_tr_data_tensor = user_train_data_tensors[index].cuda()\n",
    "#         attacker_tr_label_tensor = user_train_label_tensors[index].cuda()\n",
    "\n",
    "#         def objective(trial):\n",
    "#             # Define hyperparameters to optimize\n",
    "#             lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "#             lambda1 = trial.suggest_float(\"lambda1\", 0, 1)\n",
    "#             lambda2 = trial.suggest_float(\"lambda2\", 0, 1)\n",
    "    \n",
    "#             # Your existing function, possibly modified to accept hyperparameters\n",
    "#             def function_to_minimize(X, lr, lambda1, lambda2):\n",
    "#                 X = X.cuda()\n",
    "#                 attacker_abs_mean = X.abs().mean()\n",
    "#                 attacker_std = X.abs().std()\n",
    "#                 # print(lambda11, lambda22)\n",
    "#                 # Reshape X into the shape of fed_model.parameters\n",
    "#                 fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "#                 optimizer_fed = SGD(fed_model.parameters(), lr=0.5)\n",
    "\n",
    "#                 optimizer_fed.zero_grad()\n",
    "#                 model_grads=[]\n",
    "#                 start_idx = 0\n",
    "#                 for i, param in enumerate(fed_model.parameters()):\n",
    "#                     param_=X[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "#                     start_idx=start_idx+len(param.data.view(-1))\n",
    "#                     param_=param_.cuda()\n",
    "#                     model_grads.append(param_)\n",
    "#                 optimizer_fed.step(model_grads)\n",
    "#                 return -criterion(fed_model(attacker_tr_data_tensor), attacker_tr_label_tensor) + lambda1*torch.abs(benign_std_avg - attacker_abs_mean) + lambda2*torch.abs(benign_abs_mean_avg - attacker_std)\n",
    "\n",
    "\n",
    "#             x_ = torch.tensor(attacker_grad, requires_grad=True)  # Starting with 0.0, requires_grad=True to enable gradient computation\n",
    "\n",
    "#             # Step 2: Choose an optimizer\n",
    "#             optimizer = torch.optim.SGD([x_], lr=0.1)  # Using SGD with a learning rate of 0.1\n",
    "#             # Training loop adapted for Optuna\n",
    "#             optimizer.zero_grad()   # Clear previous gradients\n",
    "#             loss = function_to_minimize(x_, lr, lambda1, lambda2)  # Compute the loss\n",
    "#             loss.backward()  # Compute gradients\n",
    "#             optimizer.step()  # Update parameters based on gradients\n",
    "            \n",
    "#             # Return final loss as the objective to minimize\n",
    "#             return loss.item()\n",
    "\n",
    "\n",
    "#         study = optuna.create_study(direction=\"minimize\")\n",
    "#         study.optimize(objective, n_trials=10)\n",
    "\n",
    "#         print(study.best_params)\n",
    "#         result_attacker_grads.append(study.best_params)\n",
    "\n",
    "#     return torch.stack(result_attacker_grads).cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute our attack + 10 attacker + our defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "\n",
    "nepochs= 50\n",
    "\n",
    "schedule=[1000]\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='bulyan'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='our-agr'\n",
    "dev_type ='std'\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "past_mean = []\n",
    "past_abs_sum = []\n",
    "past_median = []\n",
    "past_lower_quartile = []\n",
    "past_upper_quartile = []\n",
    "past_std = []\n",
    "\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    df = pd.DataFrame(columns = ['epoch', 'loss', 'validation accuracy', 'best validation accuracy'])\n",
    "\n",
    "    history = [[] for _ in range(n_users)]\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "\n",
    "        for i in range(n_users): # 50\n",
    "            nbatches = user_tr_len[i]//batch_size\n",
    "            # nbatches = 1000//batch_size\n",
    "\n",
    "            inputs = user_train_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_train_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            # inputs = user_train_data_tensors[i]\n",
    "            # targets = user_train_label_tensors[i]\n",
    "\n",
    "            targets = targets.type(torch.LongTensor)   \n",
    "            \n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                # print(param.shape)\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        \n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "        \n",
    "        updates_abs_mean = user_grads.abs().mean(dim=1, keepdim=True)\n",
    "\n",
    "        if  torch.isnan(updates_abs_mean).any():\n",
    "            print(\"updates_abs_mean contains a nan value\")\n",
    "            print(\"-----------------------BEFORE-----------------------------------\")\n",
    "            nan_indices = torch.where(torch.isnan(updates_abs_mean))\n",
    "            print(\"updates_abs_mean has nan value at these index: \", nan_indices)\n",
    "            user_grad_nan_index = torch.where(user_grads[nan_indices[0]])\n",
    "            print(\"user_grads has nan value at these index\", user_grad_nan_index)\n",
    "            print(user_grads[nan_indices[0]][user_grad_nan_index])\n",
    "        \n",
    "        if n_attacker > 0:\n",
    "            n_attacker_ = max(1, n_attacker**2//n_users)\n",
    "            agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "\n",
    "            # No attack\n",
    "            # malicious_grads = user_grads\n",
    "\n",
    "            # Attack\n",
    "            mal_updates = our_attack_mean_std(user_grads, agg_grads, n_attacker, arch, criterion, user_train_data_tensors, user_train_label_tensors)\n",
    "            \n",
    "            updates_abs_mean = mal_updates.abs().mean(dim=1, keepdim=True)\n",
    "\n",
    "            if  torch.isnan(updates_abs_mean).any():\n",
    "                print(updates_abs_mean)\n",
    "                print(\"-----------------------HERE-----------------------------------\")\n",
    "                print(\"updates_abs_mean contains a nan value\")\n",
    "                nan_indices = torch.where(torch.isnan(updates_abs_mean.grad))\n",
    "                print(\"updates_abs_mean has nan value at these index: \", nan_indices)\n",
    "\n",
    "                user_grad_nan_index = torch.where(user_grads[nan_indices[0]])\n",
    "                print(\"user_grads has nan value at these index\", user_grad_nan_index)\n",
    "                print(user_grads[nan_indices[0]][user_grad_nan_index])\n",
    "\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0) # torch.Size([50, 2472266])\n",
    "\n",
    "        # malicious_grads = user_grads\n",
    "        if not (malicious_grads.shape[0]==50):\n",
    "            print(malicious_grads.shape)\n",
    "        \n",
    "        updates_abs_mean = malicious_grads.abs().mean(dim=1, keepdim=True)\n",
    "        \n",
    "\n",
    "        if  torch.isnan(updates_abs_mean).any():\n",
    "            print(\"-----------------------AFTER-----------------------------------\")\n",
    "            print(\"updates_abs_mean contains a nan value\")\n",
    "            nan_indices = torch.where(torch.isnan(updates_abs_mean))\n",
    "            print(\"updates_abs_mean has nan value at these index: \", nan_indices)\n",
    "            user_grad_nan_index = torch.where(user_grads[nan_indices[0]])\n",
    "            print(\"user_grads has nan value at these index\", user_grad_nan_index)\n",
    "            print(user_grads[nan_indices[0]][user_grad_nan_index])\n",
    " \n",
    "        for i in range(n_users):\n",
    "            history[i].extend(updates_abs_mean[i])        \n",
    "\n",
    "        agg_grads = our_mean_defense(malicious_grads, n_attacker, history)\n",
    "\n",
    "        # print(malicious_grads.abs().mean(dim=1, keepdim=True))\n",
    "        # print(history[0])\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        \n",
    "        print('epoch: %d, %s: at %s n_at %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(epoch_num, aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc))\n",
    "        new_row = pd.DataFrame([{\n",
    "            'epoch': epoch_num, \n",
    "            'loss': val_loss, \n",
    "            'validation accuracy': val_acc, \n",
    "            'best validation accuracy': best_global_acc\n",
    "            }])\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.to_csv('./no_attack.csv', index=False)\n",
    "\n",
    "        if val_loss > 1000:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "            \n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the data\n",
    "for col in ['loss', 'validation accuracy', 'best validation accuracy']:\n",
    "    plt.plot(df['epoch'].tolist(), df[col].tolist())\n",
    "    plt.title(\"our attack + 10 attacker + our defense_\"+col)\n",
    "    plt.savefig('./our attack + 10 attacker + our defense ' + col + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute no attack + our defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "\n",
    "nepochs= 500\n",
    "\n",
    "schedule=[1000]\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='bulyan'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='our-agr'\n",
    "dev_type ='std'\n",
    "n_attackers=[0]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "past_mean = []\n",
    "past_abs_sum = []\n",
    "past_median = []\n",
    "past_lower_quartile = []\n",
    "past_upper_quartile = []\n",
    "past_std = []\n",
    "\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    df = pd.DataFrame(columns = ['epoch', 'loss', 'validation accuracy', 'best validation accuracy'])\n",
    "\n",
    "    history = [[] for _ in range(n_users)]\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "\n",
    "        for i in range(n_users): # 50\n",
    "            nbatches = user_tr_len[i]//batch_size\n",
    "            # nbatches = 1000//batch_size\n",
    "\n",
    "            inputs = user_train_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_train_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            # inputs = user_train_data_tensors[i]\n",
    "            # targets = user_train_label_tensors[i]\n",
    "\n",
    "            targets = targets.type(torch.LongTensor)   \n",
    "            \n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad ()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                # print(param.shape)\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "       \n",
    "        if n_attacker > 0:\n",
    "            n_attacker_ = max(1, n_attacker**2//n_users)\n",
    "            agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "\n",
    "            # No attack\n",
    "            # malicious_grads = user_grads\n",
    "\n",
    "            # Attack\n",
    "            mal_updates = our_attack_mean_std(user_grads, agg_grads, n_attacker, arch, criterion, user_train_data_tensors, user_train_label_tensors)\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0) # torch.Size([50, 2472266])\n",
    "\n",
    "        #remove when attacker not 0    \n",
    "        malicious_grads = user_grads\n",
    "\n",
    "        if not (malicious_grads.shape[0]==50):\n",
    "            print(malicious_grads.shape)\n",
    "        \n",
    "        updates_abs_mean = malicious_grads.abs().mean(dim=1, keepdim=True)\n",
    "        for i in range(n_users):\n",
    "            history[i].extend(updates_abs_mean[i])        \n",
    "\n",
    "        agg_grads = our_mean_defense(malicious_grads, n_attacker, history)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        \n",
    "        print('epoch: %d, %s: at %s n_at %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(epoch_num, aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc))\n",
    "        new_row = pd.DataFrame([{\n",
    "            'epoch': epoch_num, \n",
    "            'loss': val_loss, \n",
    "            'validation accuracy': val_acc, \n",
    "            'best validation accuracy': best_global_acc\n",
    "            }])\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.to_csv('./no_attack.csv', index=False)\n",
    "\n",
    "        if val_loss > 1000:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "            \n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the data\n",
    "for col in ['loss', 'validation accuracy', 'best validation accuracy']:\n",
    "    plt.plot(df['epoch'].tolist(), df[col].tolist())\n",
    "    plt.title(\"our attack + 10 attacker + our defense_\"+col)\n",
    "    plt.savefig('./no attack + our defense ' + col + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute our attack + 10 attacker + bulyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "\n",
    "nepochs= 500\n",
    "\n",
    "schedule=[1000]\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='bulyan'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='our-agr'\n",
    "dev_type ='std'\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "past_mean = []\n",
    "past_abs_sum = []\n",
    "past_median = []\n",
    "past_lower_quartile = []\n",
    "past_upper_quartile = []\n",
    "past_std = []\n",
    "\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    df = pd.DataFrame(columns = ['epoch', 'loss', 'validation accuracy', 'best validation accuracy'])\n",
    "\n",
    "    history = [[] for _ in range(n_users)]\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "\n",
    "        for i in range(n_users): # 50\n",
    "            nbatches = user_tr_len[i]//batch_size\n",
    "            # nbatches = 1000//batch_size\n",
    "\n",
    "            inputs = user_train_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_train_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            # inputs = user_train_data_tensors[i]\n",
    "            # targets = user_train_label_tensors[i]\n",
    "\n",
    "            targets = targets.type(torch.LongTensor)   \n",
    "            \n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                # print(param.shape)\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "       \n",
    "        if n_attacker > 0:\n",
    "            n_attacker_ = max(1, n_attacker**2//n_users)\n",
    "            agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "\n",
    "            # Attack\n",
    "            mal_updates = our_attack_mean_std(user_grads, agg_grads, n_attacker, arch, criterion, user_train_data_tensors, user_train_label_tensors)\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0) # torch.Size([50, 2472266])\n",
    "\n",
    "        if not (malicious_grads.shape[0]==50):\n",
    "            print(malicious_grads.shape)\n",
    "        \n",
    "        updates_abs_mean = malicious_grads.abs().mean(dim=1, keepdim=True)\n",
    "        for i in range(n_users):\n",
    "            history[i].extend(updates_abs_mean[i])        \n",
    "\n",
    "        if  torch.isnan(updates_abs_mean).any():\n",
    "            print(\"updates_abs_mean contains a nan value\")\n",
    "            nan_indices = torch.where(torch.isnan(updates_abs_mean))\n",
    "            print(\"updates_abs_mean has nan value at these index: \", nan_indices)\n",
    "            user_grad_nan_index = torch.where(user_grads[nan_indices[0]])\n",
    "            print(\"user_grads has nan value at these index\", user_grad_nan_index)\n",
    "            print(user_grads[nan_indices[0]][user_grad_nan_index])\n",
    "            print(user_grads[nan_indices[0]][user_grad_nan_index].shape)\n",
    " \n",
    "        agg_grads, _ = bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        \n",
    "        print('epoch: %d, %s: at %s n_at %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(epoch_num, aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc))\n",
    "        new_row = pd.DataFrame([{\n",
    "            'epoch': epoch_num, \n",
    "            'loss': val_loss, \n",
    "            'validation accuracy': val_acc, \n",
    "            'best validation accuracy': best_global_acc\n",
    "            }])\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.to_csv('./no_attack.csv', index=False)\n",
    "\n",
    "        if val_loss > 1000:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "            \n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the data\n",
    "for col in ['loss', 'validation accuracy', 'best validation accuracy']:\n",
    "    plt.plot(df['epoch'].tolist(), df[col].tolist())\n",
    "    plt.title(\"Execute our attack + 10 attacker + bulyan_\"+col)\n",
    "    plt.savefig('./our attack + 10 attacker + bulyan ' + col + '.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
