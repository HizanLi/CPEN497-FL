{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMUR8ODgnrc3"
      },
      "source": [
        "# The notebook contains\n",
        "### Code for _Bulyan_ aggregation algorithm, *when gradient updates of benign clients are unknown to adversary*\n",
        "### Evaluation of all of the attacks (Fang, LIE, and our SOTA AGR-tailored and AGR-agnstic) on Bulyan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJu2Edmbnrc5"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wGSIzpf_nrc5",
        "outputId": "0e3785ba-ecd2-4eef-b56e-e735236e1067"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>.container { width:90% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AAU3fosynrc6"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import argparse, os, sys, csv, shutil, time, random, operator, pickle, ast, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.optim import Optimizer\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.multiprocessing as mp\n",
        "import math\n",
        "sys.path.insert(0,'./../utils/')\n",
        "from logger import *\n",
        "from eval import *\n",
        "from misc import *\n",
        "\n",
        "from cifar10_normal_train import *\n",
        "from cifar10_util import *\n",
        "from adam import Adam\n",
        "from sgd import SGD\n",
        "\n",
        "from torchmin import minimize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGABIvW6nrc6"
      },
      "source": [
        "## Data split\n",
        "Divide cifar10 data among 50 clients in Non-IID fashion using Dirichlet distribution\n",
        "\n",
        "Graph the distribution of classes for each user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1Nyn5Xlnrc6",
        "outputId": "4e11a4a9-9764-47b0-beaf-cef036e8b35d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "data_loc='/mnt/nfs/work1/amir/vshejwalkar/cifar10_data/'\n",
        "# load the train dataset\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=train_transform)\n",
        "\n",
        "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=train_transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RrUvgYfNnrc7"
      },
      "outputs": [],
      "source": [
        "total_tr_len = len(cifar10_train)\n",
        "\n",
        "X={}\n",
        "Y=[]\n",
        "for i in range(len(cifar10_train)):\n",
        "    data = cifar10_train[i][0].numpy()\n",
        "    label = cifar10_train[i][1]\n",
        "\n",
        "    if label in X:\n",
        "        X[label].append(data)\n",
        "    else:\n",
        "        X[label] = []\n",
        "        X[label].append(data)\n",
        "        Y.append(label)\n",
        "\n",
        "for label in X:\n",
        "    X[label] = np.array(X[label])\n",
        "Y=np.array(Y)\n",
        "\n",
        "alpha = 1\n",
        "n_users = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6osiZzKOnrc7",
        "outputId": "0a58daac-9c9c-412a-9c8d-d9c243535c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user 0 has 962 train data, and 962 train labels\n",
            "user 1 has 594 train data, and 594 train labels\n",
            "user 2 has 1821 train data, and 1821 train labels\n",
            "user 3 has 1328 train data, and 1328 train labels\n",
            "user 4 has 949 train data, and 949 train labels\n",
            "user 5 has 880 train data, and 880 train labels\n",
            "user 6 has 690 train data, and 690 train labels\n",
            "user 7 has 1198 train data, and 1198 train labels\n",
            "user 8 has 836 train data, and 836 train labels\n",
            "user 9 has 883 train data, and 883 train labels\n",
            "user 10 has 1135 train data, and 1135 train labels\n",
            "user 11 has 634 train data, and 634 train labels\n",
            "user 12 has 1129 train data, and 1129 train labels\n",
            "user 13 has 1570 train data, and 1570 train labels\n",
            "user 14 has 550 train data, and 550 train labels\n",
            "user 15 has 1277 train data, and 1277 train labels\n",
            "user 16 has 1175 train data, and 1175 train labels\n",
            "user 17 has 768 train data, and 768 train labels\n",
            "user 18 has 1280 train data, and 1280 train labels\n",
            "user 19 has 894 train data, and 894 train labels\n",
            "user 20 has 950 train data, and 950 train labels\n",
            "user 21 has 1185 train data, and 1185 train labels\n",
            "user 22 has 561 train data, and 561 train labels\n",
            "user 23 has 928 train data, and 928 train labels\n",
            "user 24 has 501 train data, and 501 train labels\n",
            "user 25 has 1413 train data, and 1413 train labels\n",
            "user 26 has 1053 train data, and 1053 train labels\n",
            "user 27 has 724 train data, and 724 train labels\n",
            "user 28 has 1744 train data, and 1744 train labels\n",
            "user 29 has 1063 train data, and 1063 train labels\n",
            "user 30 has 804 train data, and 804 train labels\n",
            "user 31 has 1193 train data, and 1193 train labels\n",
            "user 32 has 926 train data, and 926 train labels\n",
            "user 33 has 1614 train data, and 1614 train labels\n",
            "user 34 has 687 train data, and 687 train labels\n",
            "user 35 has 1108 train data, and 1108 train labels\n",
            "user 36 has 662 train data, and 662 train labels\n",
            "user 37 has 818 train data, and 818 train labels\n",
            "user 38 has 1342 train data, and 1342 train labels\n",
            "user 39 has 344 train data, and 344 train labels\n",
            "user 40 has 976 train data, and 976 train labels\n",
            "user 41 has 1376 train data, and 1376 train labels\n",
            "user 42 has 898 train data, and 898 train labels\n",
            "user 43 has 509 train data, and 509 train labels\n",
            "user 44 has 1022 train data, and 1022 train labels\n",
            "user 45 has 1022 train data, and 1022 train labels\n",
            "user 46 has 1365 train data, and 1365 train labels\n",
            "user 47 has 746 train data, and 746 train labels\n",
            "user 48 has 478 train data, and 478 train labels\n",
            "user 49 has 1435 train data, and 1435 train labels\n"
          ]
        }
      ],
      "source": [
        "user_train_data_non_tensors = [[] for _ in range(n_users)]\n",
        "user_train_label_non_tensors = [[] for _ in range(n_users)]\n",
        "\n",
        "for label in Y:\n",
        "    alpha_list = [alpha for _ in range(n_users)]\n",
        "    probs = np.random.dirichlet(alpha_list)\n",
        "\n",
        "    taken_index = 0\n",
        "\n",
        "    for i, prob in enumerate(probs):\n",
        "        if i == n_users - 1:\n",
        "            user_train_data_non_tensors[i].extend(X[label][taken_index:])\n",
        "            user_train_label_non_tensors[i].extend([label for _ in range(len(X[label]) - taken_index)])\n",
        "        else:\n",
        "            n_sample = math.floor(prob * len(X[label]))\n",
        "\n",
        "            user_train_data_non_tensors[i].extend(X[label][taken_index : taken_index + n_sample])\n",
        "            user_train_label_non_tensors[i].extend([label for _ in range(n_sample)])\n",
        "            taken_index += n_sample\n",
        "\n",
        "user_train_data_tensors = []\n",
        "user_train_label_tensors = []\n",
        "\n",
        "user_tr_len = []\n",
        "\n",
        "for i in range(n_users):\n",
        "\n",
        "    if len(user_train_data_non_tensors[i]) != len(user_train_label_non_tensors[i]):\n",
        "        sys.exit(f\"Shape does not match user_train_data_non_tensors[i] hsa {len(user_train_data_non_tensors[i])}, while user_train_label_non_tensors[i] has {len(user_train_label_non_tensors[i])}\")\n",
        "\n",
        "\n",
        "    num_data = len(user_train_data_non_tensors[i])\n",
        "    user_tr_len.append(num_data)\n",
        "\n",
        "    user_train_data_tensors.append(torch.from_numpy(np.array(user_train_data_non_tensors[i])).type(torch.FloatTensor))\n",
        "    user_train_label_tensors.append(torch.from_numpy(np.array(user_train_label_non_tensors[i])).type(torch.LongTensor))\n",
        "\n",
        "    r=np.arange(num_data)\n",
        "\n",
        "    np.random.shuffle(r)\n",
        "\n",
        "    user_train_data_tensors[i] = user_train_data_tensors[i][r]\n",
        "    user_train_label_tensors[i] = user_train_label_tensors[i][r]\n",
        "\n",
        "    print(f'user {i} has {user_train_data_tensors[i].shape[0]} train data, and {user_train_label_tensors[i].shape[0]} train labels')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HSewQHUZnrc7"
      },
      "outputs": [],
      "source": [
        "X2 = []\n",
        "Y2 = []\n",
        "\n",
        "for i in range(len(cifar10_test)):\n",
        "    X2.append(cifar10_test[i][0].numpy())\n",
        "    Y2.append(cifar10_test[i][1])\n",
        "\n",
        "X2=np.array(X2)\n",
        "Y2=np.array(Y2)\n",
        "\n",
        "half_index = len(X2) // 2\n",
        "\n",
        "val_data= X2[:half_index]\n",
        "val_label= Y2[:half_index]\n",
        "\n",
        "te_data=X2[half_index:]\n",
        "te_label=Y2[half_index:]\n",
        "\n",
        "val_data_tensor=torch.from_numpy(val_data).type(torch.FloatTensor)\n",
        "val_label_tensor=torch.from_numpy(val_label).type(torch.LongTensor)\n",
        "\n",
        "te_data_tensor=torch.from_numpy(te_data).type(torch.FloatTensor)\n",
        "te_label_tensor=torch.from_numpy(te_label).type(torch.LongTensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "id": "M8vVcXSRnrc7",
        "outputId": "2400b6de-357b-4900-b132-7c2e121cad38"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Example data\n",
        "users = [f\"user {i}\" for i in range(50)]\n",
        "labels = [f\"label {i}\" for i in range(10)]\n",
        "\n",
        "\n",
        "percentages = []\n",
        "for user_data in user_train_label_non_tensors:\n",
        "    temp_holder = []\n",
        "    for label in range(10):\n",
        "        temp_holder.append(user_data.count(label))\n",
        "    percentages.append(temp_holder)\n",
        "\n",
        "percentages = np.array(percentages)\n",
        "\n",
        "# Assuming 'percentages' is populated correctly as shown previously\n",
        "# Convert percentages to cumulative sum for stacking\n",
        "cumulative = np.cumsum(percentages, axis=1)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Colors for each ethnicity, ensuring there are enough colors for all labels\n",
        "colors = plt.cm.Paired(range(len(labels))) # Repeating colors to match the number of labels\n",
        "\n",
        "# Create stacked bars\n",
        "for i in range(len(labels)):  # Iterate over the number of labels\n",
        "    if i == 0:\n",
        "        ax.bar(users, percentages[:, i], color=colors[i], label=labels[i])\n",
        "    else:\n",
        "        ax.bar(users, percentages[:, i], bottom=cumulative[:, i-1], color=colors[i], label=labels[i])\n",
        "\n",
        "# Add some text for labels, title, and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('Percentage')\n",
        "ax.set_title('Stacked Percentage Bar Chart by User and Label')\n",
        "ax.set_xticks(np.arange(len(users)))\n",
        "ax.set_xticklabels(users, rotation=90)  # Rotate labels if needed\n",
        "ax.legend()\n",
        "\n",
        "\n",
        "plt.savefig('./non_iid_user_label_distribution.png', bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOVMqc_9nrc8"
      },
      "source": [
        "## Our Aggregation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LMgKiRnynrc8"
      },
      "outputs": [],
      "source": [
        "def our_mean_defense(all_updates, n_attackers, history_updates):\n",
        "    # find the index of 10 users to be discarded\n",
        "    discarded_history = get_discarded_index(n_attackers, history_updates)\n",
        "\n",
        "    # get tensors in all_updates excluding the discarded ones\n",
        "    mask = torch.ones(all_updates.size(0), dtype=torch.bool)  # Create a mask of ones (True)\n",
        "    mask[discarded_history] = False  # Set indices in discarded_history to False\n",
        "    remaining_updates = all_updates[mask]\n",
        "\n",
        "    print('discarded index', discarded_history)\n",
        "\n",
        "    # returns the mean of selected updates\n",
        "    # after changing the attack, it should not have nan values\n",
        "    return torch.nanmean(remaining_updates, dim=0)\n",
        "\n",
        "\n",
        "def euclidean_distance(row1, row2):\n",
        "    return math.sqrt(sum((x - y) ** 2 for x, y in zip(row1, row2)))\n",
        "\n",
        "def get_discarded_index(n_attackers, history_updates):\n",
        "    # find the index of 10 users to be discarded\n",
        "    discarded_history = []\n",
        "    for _ in range(n_attackers):\n",
        "        sum_of_distances = []\n",
        "        for i, row in enumerate(history_updates):\n",
        "            if i in discarded_history:\n",
        "                sum_of_distances.append(-1)\n",
        "                continue\n",
        "            distance_sum = 0\n",
        "            for j, other_row in enumerate(history_updates):\n",
        "                if row != other_row and j not in discarded_history:\n",
        "                    distance_sum += euclidean_distance(row, other_row)\n",
        "            sum_of_distances.append(distance_sum)\n",
        "        max_distance = max(sum_of_distances)\n",
        "        index_of_max_distance = sum_of_distances.index(max_distance)\n",
        "        discarded_history.append(index_of_max_distance)\n",
        "    return discarded_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yiHdOSknrc8"
      },
      "source": [
        "## Code for no defense aggregation algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bDqjWvS-nrc8"
      },
      "outputs": [],
      "source": [
        "def no_defense_aggregation(user_grads):\n",
        "    return torch.nanmean(user_grads, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh9-2QXLnrc8"
      },
      "source": [
        "## our_attack_mean\n",
        "$ argmin_\\theta(\\ -L(\\theta_{attacker(s)}) + \\lambda_1 |\\bar{\\theta_{abs\\ mean\\ benige\\ users}} - \\bar{\\theta_{abs\\ attacker(s)}}|$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_condition(lambda1, model_grad, loss_grad, n_attackers, history_updates, n_users, all_updates):\n",
        "    new_model_grad = model_grad + lambda1 * loss_grad\n",
        "    for i in range(n_users):\n",
        "        if i < n_attackers:\n",
        "            absolute_values = [abs(x) for x in new_model_grad]\n",
        "            absolute_mean = sum(absolute_values) / len(absolute_values)\n",
        "            history_updates[i].extend(absolute_mean)\n",
        "        else:\n",
        "            absolute_values = [abs(x) for x in all_updates[i]]\n",
        "            absolute_mean = sum(absolute_values) / len(absolute_values)\n",
        "            history_updates[i].extend(absolute_mean)\n",
        "    \n",
        "    discarded_history = get_discarded_index(n_attackers, history_updates)\n",
        "    detected_mal = len([item for item in discarded_history if item < 10])\n",
        "    return detected_mal / len(discarded_history) <= n_attackers / len(history_updates)\n",
        "\n",
        "\n",
        "def find_upper_bound(lambda1, model_grad, loss_grad, n_attackers, history_updates, n_users, all_updates):\n",
        "    while check_condition(lambda1, model_grad, loss_grad, n_attackers, history_updates, n_users, all_updates):\n",
        "        lambda1 *= 2  # Exponentially increase lambda\n",
        "    return lambda1\n",
        "\n",
        "\n",
        "def projection(model_grad, loss_grad, history_updates, n_users, all_updates, n_attackers):\n",
        "    lambda_low = 0\n",
        "    lambda_high = find_upper_bound(1, model_grad, loss_grad, n_attackers, history_updates, n_users, all_updates)\n",
        "    tolerance = 1e-6\n",
        "\n",
        "    while lambda_high - lambda_low > tolerance:\n",
        "        lambda_mid = (lambda_low + lambda_high) / 2\n",
        "        if check_condition(lambda_mid, model_grad, loss_grad, n_attackers, history_updates, n_users, all_updates):\n",
        "            lambda_low = lambda_mid + tolerance\n",
        "        else:\n",
        "            lambda_high = lambda_mid - tolerance\n",
        "\n",
        "    lambda1 = lambda_mid - tolerance\n",
        "    return model_grad + lambda1 * loss_grad\n",
        "\n",
        "\n",
        "def our_attack_mean(n_user, history_updates, all_updates, model_grads, n_attacker, arch, criterion, user_train_data_tensors, user_train_label_tensors):\n",
        "\n",
        "    attacker_tr_data_tensor = user_train_data_tensors[0].cuda()\n",
        "    attacker_tr_label_tensor = user_train_label_tensors[0].cuda()\n",
        "\n",
        "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
        "    \n",
        "    grads = model_grads\n",
        "    attacker_grad = [item for sublist in model_grads for item in sublist]\n",
        "\n",
        "    new_model_grad = None\n",
        "\n",
        "    while True:\n",
        "        optimizer_fed = SGD(grads, lr=0.005)\n",
        "        optimizer_fed.zero_grad()\n",
        "        optimizer_fed.step(grads)\n",
        "\n",
        "        outputs = fed_model(attacker_tr_data_tensor)\n",
        "        loss = criterion(outputs, attacker_tr_label_tensor)\n",
        "\n",
        "        optimizer_fed.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        param_grad=[]\n",
        "        for param in fed_model.parameters():\n",
        "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
        "        \n",
        "        new_model_grad = projection(attacker_grad, param_grad, history_updates, n_user, all_updates, n_attacker)\n",
        "        \n",
        "        difference = torch.sum(torch.abs(attacker_grad - new_model_grad))\n",
        "        print(\"difference is: \", difference)\n",
        "        attacker_grad = new_model_grad\n",
        "\n",
        "        if difference < 1e-7:\n",
        "            print(\"STOPED! Differnece is too small, stop finding best attack grad\")\n",
        "            break\n",
        "\n",
        "    result_attacker_grads.append(new_model_grad)\n",
        "\n",
        "    return torch.stack(result_attacker_grads).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_attack(all_updates, n_attacker):\n",
        "    for i in range(n_attacker):\n",
        "        all_updates[i] = -all_updates[i]\n",
        "    return all_updates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CGm85rwnrc8"
      },
      "source": [
        "## Set number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1umXispUnrc9"
      },
      "outputs": [],
      "source": [
        "nepochs= 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_CfL1Rlnrc9"
      },
      "source": [
        "## Execute our attack + 10 attacker + our defense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkLfF-h4nrc9",
        "outputId": "8c7c4617-ba3f-4fb6-93d7-76fd9fb6929c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "discarded index [7, 31, 15, 3, 33, 29, 5, 22, 49, 9]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Hizan\\Desktop\\CPEN497-FL\\CPEN497-499\\sgd.py:109: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\python_arg_parser.cpp:1630.)\n",
            "  p.data.add_(-group['lr'], d_p)\n",
            "C:\\Users\\Hizan\\AppData\\Local\\Temp\\ipykernel_7612\\2833803696.py:132: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df, new_row], ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0, bulyan: at our-agr n_at 10 e 0 | val loss 2.3023 val acc 10.3084 best val_acc 10.308442\n",
            "50\n",
            "discarded index [7, 31, 3, 15, 49, 22, 29, 11, 33, 9]\n",
            "epoch: 1, bulyan: at our-agr n_at 10 e 1 | val loss 2.3017 val acc 10.3084 best val_acc 10.308442\n",
            "50\n",
            "discarded index [7, 31, 3, 15, 22, 49, 9, 5, 29, 11]\n",
            "epoch: 2, bulyan: at our-agr n_at 10 e 2 | val loss 2.3010 val acc 10.4911 best val_acc 10.491071\n",
            "50\n",
            "discarded index [7, 31, 3, 15, 22, 49, 5, 9, 11, 29]\n",
            "epoch: 3, bulyan: at our-agr n_at 10 e 3 | val loss 2.3006 val acc 13.8393 best val_acc 13.839286\n",
            "50\n",
            "discarded index [7, 31, 3, 15, 22, 49, 9, 29, 5, 11]\n",
            "epoch: 4, bulyan: at our-agr n_at 10 e 4 | val loss 2.3002 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [7, 31, 15, 3, 22, 49, 5, 9, 11, 29]\n",
            "epoch: 5, bulyan: at our-agr n_at 10 e 5 | val loss 2.2998 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [7, 31, 3, 15, 22, 49, 5, 9, 11, 29]\n",
            "epoch: 6, bulyan: at our-agr n_at 10 e 6 | val loss 2.2994 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [7, 31, 3, 15, 22, 49, 9, 5, 11, 29]\n",
            "epoch: 7, bulyan: at our-agr n_at 10 e 7 | val loss 2.2989 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [7, 31, 3, 15, 22, 5, 49, 9, 11, 29]\n",
            "epoch: 8, bulyan: at our-agr n_at 10 e 8 | val loss 2.2984 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [7, 31, 3, 15, 5, 22, 11, 49, 9, 29]\n",
            "epoch: 9, bulyan: at our-agr n_at 10 e 9 | val loss 2.2980 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [7, 31, 15, 3, 22, 5, 49, 9, 11, 29]\n",
            "epoch: 10, bulyan: at our-agr n_at 10 e 10 | val loss 2.2974 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [7, 31, 3, 15, 22, 49, 9, 5, 11, 29]\n",
            "epoch: 11, bulyan: at our-agr n_at 10 e 11 | val loss 2.2968 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [7, 31, 3, 15, 22, 5, 9, 49, 11, 29]\n",
            "epoch: 12, bulyan: at our-agr n_at 10 e 12 | val loss 2.2961 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [7, 31, 3, 15, 22, 11, 5, 9, 49, 45]\n",
            "epoch: 13, bulyan: at our-agr n_at 10 e 13 | val loss 2.2953 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [7, 31, 3, 15, 22, 5, 11, 9, 49, 45]\n",
            "epoch: 14, bulyan: at our-agr n_at 10 e 14 | val loss 2.2944 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [7, 31, 15, 3, 22, 11, 5, 9, 49, 45]\n",
            "epoch: 15, bulyan: at our-agr n_at 10 e 15 | val loss 2.2933 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [7, 31, 3, 15, 22, 11, 9, 49, 5, 45]\n",
            "epoch: 16, bulyan: at our-agr n_at 10 e 16 | val loss 2.2922 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [7, 31, 3, 15, 11, 22, 5, 9, 49, 45]\n",
            "epoch: 17, bulyan: at our-agr n_at 10 e 17 | val loss 2.2904 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [7, 31, 15, 3, 22, 11, 5, 9, 49, 45]\n",
            "epoch: 18, bulyan: at our-agr n_at 10 e 18 | val loss 2.2885 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [31, 7, 3, 15, 11, 22, 9, 5, 49, 45]\n",
            "epoch: 19, bulyan: at our-agr n_at 10 e 19 | val loss 2.2858 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [31, 7, 15, 3, 11, 22, 9, 5, 49, 45]\n",
            "epoch: 20, bulyan: at our-agr n_at 10 e 20 | val loss 2.2830 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [31, 7, 3, 15, 11, 22, 5, 9, 49, 45]\n",
            "epoch: 21, bulyan: at our-agr n_at 10 e 21 | val loss 2.2789 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [31, 7, 3, 15, 11, 22, 9, 5, 49, 45]\n",
            "epoch: 22, bulyan: at our-agr n_at 10 e 22 | val loss 2.2733 val acc 10.0244 best val_acc 13.839286\n",
            "50\n",
            "discarded index [31, 7, 3, 11, 15, 22, 9, 5, 20, 29]\n",
            "epoch: 23, bulyan: at our-agr n_at 10 e 23 | val loss 2.2651 val acc 10.1664 best val_acc 13.839286\n",
            "50\n",
            "discarded index [31, 7, 3, 11, 22, 15, 5, 9, 29, 20]\n",
            "epoch: 24, bulyan: at our-agr n_at 10 e 24 | val loss 2.2572 val acc 10.5317 best val_acc 13.839286\n",
            "50\n",
            "discarded index [31, 7, 3, 11, 15, 22, 9, 5, 20, 29]\n",
            "epoch: 25, bulyan: at our-agr n_at 10 e 25 | val loss 2.2437 val acc 13.5349 best val_acc 13.839286\n",
            "50\n",
            "discarded index [31, 7, 3, 22, 11, 15, 9, 5, 43, 20]\n",
            "epoch: 26, bulyan: at our-agr n_at 10 e 26 | val loss 2.2341 val acc 14.7727 best val_acc 14.772727\n",
            "50\n",
            "discarded index [31, 7, 3, 22, 11, 15, 5, 20, 43, 9]\n",
            "epoch: 27, bulyan: at our-agr n_at 10 e 27 | val loss 2.2120 val acc 17.3093 best val_acc 17.309253\n",
            "50\n",
            "discarded index [31, 7, 3, 22, 11, 15, 43, 29, 5, 20]\n",
            "epoch: 28, bulyan: at our-agr n_at 10 e 28 | val loss 2.1947 val acc 17.3701 best val_acc 17.370130\n",
            "50\n",
            "discarded index [31, 7, 3, 22, 43, 11, 15, 5, 29, 20]\n",
            "epoch: 29, bulyan: at our-agr n_at 10 e 29 | val loss 2.1704 val acc 19.1356 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 7, 3, 33, 43, 19, 37, 15, 29, 11]\n",
            "epoch: 30, bulyan: at our-agr n_at 10 e 30 | val loss 2.1583 val acc 18.3036 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 7, 3, 43, 33, 22, 19, 15, 20, 11]\n",
            "epoch: 31, bulyan: at our-agr n_at 10 e 31 | val loss 2.2472 val acc 13.5349 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 43, 29, 37, 33, 3, 24, 14]\n",
            "epoch: 32, bulyan: at our-agr n_at 10 e 32 | val loss 2.6403 val acc 11.9115 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 29, 7, 43, 37, 33, 3, 16, 24]\n",
            "epoch: 33, bulyan: at our-agr n_at 10 e 33 | val loss 2.2974 val acc 10.3084 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 29, 7, 43, 37, 33, 3, 16, 24]\n",
            "epoch: 34, bulyan: at our-agr n_at 10 e 34 | val loss 2.2944 val acc 10.7346 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 29, 7, 43, 37, 33, 3, 16, 24]\n",
            "epoch: 35, bulyan: at our-agr n_at 10 e 35 | val loss 2.2910 val acc 11.4651 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 29, 7, 43, 37, 33, 3, 16, 24]\n",
            "epoch: 36, bulyan: at our-agr n_at 10 e 36 | val loss 2.2867 val acc 11.4448 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 29, 7, 43, 37, 33, 3, 16, 24]\n",
            "epoch: 37, bulyan: at our-agr n_at 10 e 37 | val loss 2.2819 val acc 11.2825 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 3, 16, 24]\n",
            "epoch: 38, bulyan: at our-agr n_at 10 e 38 | val loss 2.2766 val acc 10.8969 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 3, 24, 16]\n",
            "epoch: 39, bulyan: at our-agr n_at 10 e 39 | val loss 2.2715 val acc 10.7346 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 3, 24, 16]\n",
            "epoch: 40, bulyan: at our-agr n_at 10 e 40 | val loss 2.2668 val acc 10.7752 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 3, 24, 16]\n",
            "epoch: 41, bulyan: at our-agr n_at 10 e 41 | val loss 2.2621 val acc 10.8563 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 3, 24, 16]\n",
            "epoch: 42, bulyan: at our-agr n_at 10 e 42 | val loss 2.2585 val acc 11.0390 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 3, 24, 16]\n",
            "epoch: 43, bulyan: at our-agr n_at 10 e 43 | val loss 2.2522 val acc 11.4651 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 3, 24, 16]\n",
            "epoch: 44, bulyan: at our-agr n_at 10 e 44 | val loss 2.2475 val acc 11.8304 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 3, 24, 16]\n",
            "epoch: 45, bulyan: at our-agr n_at 10 e 45 | val loss 2.2432 val acc 12.0536 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 3, 24, 16]\n",
            "epoch: 46, bulyan: at our-agr n_at 10 e 46 | val loss 2.2372 val acc 14.3060 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 3, 24, 16]\n",
            "epoch: 47, bulyan: at our-agr n_at 10 e 47 | val loss 2.2305 val acc 12.6015 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 3, 24, 16]\n",
            "epoch: 48, bulyan: at our-agr n_at 10 e 48 | val loss 2.2282 val acc 12.8450 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 24, 3, 16]\n",
            "epoch: 49, bulyan: at our-agr n_at 10 e 49 | val loss 2.2185 val acc 14.9554 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 24, 3, 16]\n",
            "epoch: 50, bulyan: at our-agr n_at 10 e 50 | val loss 2.2169 val acc 13.4334 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 24, 3, 16]\n",
            "epoch: 51, bulyan: at our-agr n_at 10 e 51 | val loss 2.2076 val acc 16.6802 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 24, 3, 16]\n",
            "epoch: 52, bulyan: at our-agr n_at 10 e 52 | val loss 2.2038 val acc 17.0860 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 24, 3, 16]\n",
            "epoch: 53, bulyan: at our-agr n_at 10 e 53 | val loss 2.1973 val acc 14.6510 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 24, 3, 16]\n",
            "epoch: 54, bulyan: at our-agr n_at 10 e 54 | val loss 2.1971 val acc 15.8076 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 24, 3, 16]\n",
            "epoch: 55, bulyan: at our-agr n_at 10 e 55 | val loss 2.1845 val acc 17.5731 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 24, 3, 16]\n",
            "epoch: 56, bulyan: at our-agr n_at 10 e 56 | val loss 2.1845 val acc 17.5528 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 24, 3, 16]\n",
            "epoch: 57, bulyan: at our-agr n_at 10 e 57 | val loss 2.1730 val acc 15.5032 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 19, 7, 29, 43, 37, 33, 24, 3, 15]\n",
            "epoch: 58, bulyan: at our-agr n_at 10 e 58 | val loss 2.1770 val acc 17.4919 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 7, 19, 29, 43, 37, 33, 24, 3, 16]\n",
            "epoch: 59, bulyan: at our-agr n_at 10 e 59 | val loss 2.1934 val acc 16.9846 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 7, 19, 29, 43, 37, 33, 24, 3, 15]\n",
            "epoch: 60, bulyan: at our-agr n_at 10 e 60 | val loss 2.1783 val acc 15.6047 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 7, 19, 29, 43, 37, 33, 3, 24, 15]\n",
            "epoch: 61, bulyan: at our-agr n_at 10 e 61 | val loss 2.2000 val acc 13.8799 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 7, 19, 29, 43, 37, 33, 3, 15, 24]\n",
            "epoch: 62, bulyan: at our-agr n_at 10 e 62 | val loss 2.1425 val acc 18.7500 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 7, 19, 29, 43, 37, 33, 3, 15, 24]\n",
            "epoch: 63, bulyan: at our-agr n_at 10 e 63 | val loss 2.1432 val acc 16.6396 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 7, 19, 29, 43, 37, 33, 15, 3, 24]\n",
            "epoch: 64, bulyan: at our-agr n_at 10 e 64 | val loss 2.1550 val acc 19.0544 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 7, 19, 29, 43, 37, 33, 15, 3, 24]\n",
            "epoch: 65, bulyan: at our-agr n_at 10 e 65 | val loss 2.2268 val acc 13.0479 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 7, 19, 29, 43, 37, 33, 15, 3, 24]\n",
            "epoch: 66, bulyan: at our-agr n_at 10 e 66 | val loss 2.1162 val acc 17.4716 best val_acc 19.135552\n",
            "50\n",
            "discarded index [31, 7, 19, 29, 43, 37, 33, 15, 3, 24]\n",
            "epoch: 67, bulyan: at our-agr n_at 10 e 67 | val loss 2.0943 val acc 20.4545 best val_acc 20.454545\n",
            "50\n",
            "discarded index [31, 7, 19, 29, 43, 15, 37, 33, 3, 24]\n",
            "epoch: 68, bulyan: at our-agr n_at 10 e 68 | val loss 2.0893 val acc 18.3847 best val_acc 20.454545\n",
            "50\n",
            "discarded index [31, 7, 19, 29, 43, 15, 37, 33, 3, 24]\n",
            "epoch: 69, bulyan: at our-agr n_at 10 e 69 | val loss 2.0725 val acc 19.8864 best val_acc 20.454545\n",
            "50\n",
            "discarded index [31, 7, 19, 29, 15, 43, 3, 37, 33, 24]\n",
            "epoch: 70, bulyan: at our-agr n_at 10 e 70 | val loss 2.1487 val acc 18.2427 best val_acc 20.454545\n",
            "50\n",
            "discarded index [31, 7, 29, 19, 15, 43, 3, 33, 37, 24]\n",
            "epoch: 71, bulyan: at our-agr n_at 10 e 71 | val loss 2.3462 val acc 10.6940 best val_acc 20.454545\n",
            "50\n",
            "discarded index [31, 7, 29, 19, 43, 15, 3, 37, 33, 14]\n",
            "epoch: 72, bulyan: at our-agr n_at 10 e 72 | val loss 2.2683 val acc 12.3377 best val_acc 20.454545\n",
            "50\n",
            "discarded index [31, 7, 29, 19, 43, 15, 3, 37, 33, 14]\n",
            "epoch: 73, bulyan: at our-agr n_at 10 e 73 | val loss 2.1991 val acc 19.4805 best val_acc 20.454545\n",
            "50\n",
            "discarded index [31, 7, 29, 19, 43, 15, 3, 37, 33, 14]\n",
            "epoch: 74, bulyan: at our-agr n_at 10 e 74 | val loss 2.1280 val acc 22.1794 best val_acc 22.179383\n",
            "50\n",
            "discarded index [31, 7, 29, 43, 19, 15, 3, 37, 33, 14]\n",
            "epoch: 75, bulyan: at our-agr n_at 10 e 75 | val loss 2.0759 val acc 22.1794 best val_acc 22.179383\n",
            "50\n",
            "discarded index [31, 7, 29, 43, 15, 19, 3, 37, 33, 14]\n",
            "epoch: 76, bulyan: at our-agr n_at 10 e 76 | val loss 2.0404 val acc 21.0836 best val_acc 22.179383\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 19, 3, 37, 33, 14]\n",
            "epoch: 77, bulyan: at our-agr n_at 10 e 77 | val loss 2.0229 val acc 21.2459 best val_acc 22.179383\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 19, 3, 37, 33, 14]\n",
            "epoch: 78, bulyan: at our-agr n_at 10 e 78 | val loss 2.0889 val acc 21.1039 best val_acc 22.179383\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 19, 3, 37, 14, 11]\n",
            "epoch: 79, bulyan: at our-agr n_at 10 e 79 | val loss 2.3085 val acc 13.2914 best val_acc 22.179383\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 19, 3, 37, 14, 11]\n",
            "epoch: 80, bulyan: at our-agr n_at 10 e 80 | val loss 2.1005 val acc 18.1006 best val_acc 22.179383\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 19, 3, 37, 16, 33]\n",
            "epoch: 81, bulyan: at our-agr n_at 10 e 81 | val loss 2.0798 val acc 21.0227 best val_acc 22.179383\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 19, 3, 37, 14, 11]\n",
            "epoch: 82, bulyan: at our-agr n_at 10 e 82 | val loss 2.1054 val acc 19.8864 best val_acc 22.179383\n",
            "50\n",
            "discarded index [31, 7, 43, 29, 15, 19, 3, 37, 16, 33]\n",
            "epoch: 83, bulyan: at our-agr n_at 10 e 83 | val loss 2.2108 val acc 12.8044 best val_acc 22.179383\n",
            "50\n",
            "discarded index [31, 7, 43, 29, 15, 19, 3, 37, 16, 14]\n",
            "epoch: 84, bulyan: at our-agr n_at 10 e 84 | val loss 2.0807 val acc 21.2662 best val_acc 22.179383\n",
            "50\n",
            "discarded index [31, 7, 43, 29, 15, 19, 3, 37, 16, 14]\n",
            "epoch: 85, bulyan: at our-agr n_at 10 e 85 | val loss 2.0246 val acc 20.0893 best val_acc 22.179383\n",
            "50\n",
            "discarded index [31, 7, 43, 29, 15, 19, 3, 37, 16, 14]\n",
            "epoch: 86, bulyan: at our-agr n_at 10 e 86 | val loss 2.0115 val acc 20.6778 best val_acc 22.179383\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 19, 3, 37, 16, 11]\n",
            "epoch: 87, bulyan: at our-agr n_at 10 e 87 | val loss 1.9846 val acc 24.8377 best val_acc 24.837662\n",
            "50\n",
            "discarded index [31, 7, 43, 29, 15, 19, 3, 37, 16, 11]\n",
            "epoch: 88, bulyan: at our-agr n_at 10 e 88 | val loss 1.9872 val acc 21.3677 best val_acc 24.837662\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 3, 19, 37, 16, 11]\n",
            "epoch: 89, bulyan: at our-agr n_at 10 e 89 | val loss 2.1646 val acc 18.3239 best val_acc 24.837662\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 19, 3, 14, 16, 37]\n",
            "epoch: 90, bulyan: at our-agr n_at 10 e 90 | val loss 2.3650 val acc 16.3352 best val_acc 24.837662\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 3, 19, 14, 16, 37]\n",
            "epoch: 91, bulyan: at our-agr n_at 10 e 91 | val loss 2.1894 val acc 16.9237 best val_acc 24.837662\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 3, 19, 14, 16, 37]\n",
            "epoch: 92, bulyan: at our-agr n_at 10 e 92 | val loss 2.1780 val acc 16.3961 best val_acc 24.837662\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 3, 19, 14, 16, 37]\n",
            "epoch: 93, bulyan: at our-agr n_at 10 e 93 | val loss 2.1484 val acc 17.7151 best val_acc 24.837662\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 3, 19, 14, 16, 37]\n",
            "epoch: 94, bulyan: at our-agr n_at 10 e 94 | val loss 2.1267 val acc 18.8515 best val_acc 24.837662\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 3, 19, 14, 16, 37]\n",
            "epoch: 95, bulyan: at our-agr n_at 10 e 95 | val loss 2.1024 val acc 19.9472 best val_acc 24.837662\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 3, 19, 14, 16, 37]\n",
            "epoch: 96, bulyan: at our-agr n_at 10 e 96 | val loss 2.0731 val acc 20.6778 best val_acc 24.837662\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 3, 19, 14, 16, 37]\n",
            "epoch: 97, bulyan: at our-agr n_at 10 e 97 | val loss 2.0367 val acc 22.4838 best val_acc 24.837662\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 3, 19, 14, 16, 37]\n",
            "epoch: 98, bulyan: at our-agr n_at 10 e 98 | val loss 2.0147 val acc 23.4172 best val_acc 24.837662\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 3, 19, 14, 16, 37]\n",
            "epoch: 99, bulyan: at our-agr n_at 10 e 99 | val loss 2.0338 val acc 18.8515 best val_acc 24.837662\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 3, 19, 14, 16, 37]\n",
            "epoch: 100, bulyan: at our-agr n_at 10 e 100 | val loss 2.0653 val acc 23.2955 best val_acc 24.837662\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 3, 19, 16, 14, 37]\n",
            "epoch: 101, bulyan: at our-agr n_at 10 e 101 | val loss 2.1763 val acc 15.9903 best val_acc 24.837662\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 3, 19, 16, 14, 37]\n",
            "epoch: 102, bulyan: at our-agr n_at 10 e 102 | val loss 2.0782 val acc 19.7037 best val_acc 24.837662\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 3, 19, 16, 14, 37]\n",
            "epoch: 103, bulyan: at our-agr n_at 10 e 103 | val loss 1.9768 val acc 25.1015 best val_acc 25.101461\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 3, 19, 16, 14, 37]\n",
            "epoch: 104, bulyan: at our-agr n_at 10 e 104 | val loss 1.9546 val acc 26.1364 best val_acc 26.136364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 29, 3, 19, 16, 37, 14]\n",
            "epoch: 105, bulyan: at our-agr n_at 10 e 105 | val loss 1.9545 val acc 24.4115 best val_acc 26.136364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 37, 14]\n",
            "epoch: 106, bulyan: at our-agr n_at 10 e 106 | val loss 1.9905 val acc 23.5593 best val_acc 26.136364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 37]\n",
            "epoch: 107, bulyan: at our-agr n_at 10 e 107 | val loss 2.1588 val acc 18.1006 best val_acc 26.136364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 108, bulyan: at our-agr n_at 10 e 108 | val loss 2.1122 val acc 21.2459 best val_acc 26.136364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 109, bulyan: at our-agr n_at 10 e 109 | val loss 2.0181 val acc 24.2492 best val_acc 26.136364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 110, bulyan: at our-agr n_at 10 e 110 | val loss 1.9716 val acc 23.8636 best val_acc 26.136364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 111, bulyan: at our-agr n_at 10 e 111 | val loss 1.9307 val acc 23.9651 best val_acc 26.136364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 112, bulyan: at our-agr n_at 10 e 112 | val loss 1.9167 val acc 23.2752 best val_acc 26.136364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 113, bulyan: at our-agr n_at 10 e 113 | val loss 1.9356 val acc 26.1161 best val_acc 26.136364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 114, bulyan: at our-agr n_at 10 e 114 | val loss 2.1534 val acc 19.0950 best val_acc 26.136364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 115, bulyan: at our-agr n_at 10 e 115 | val loss 2.1603 val acc 21.5300 best val_acc 26.136364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 116, bulyan: at our-agr n_at 10 e 116 | val loss 2.0900 val acc 19.3994 best val_acc 26.136364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 117, bulyan: at our-agr n_at 10 e 117 | val loss 2.0404 val acc 21.6112 best val_acc 26.136364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 118, bulyan: at our-agr n_at 10 e 118 | val loss 2.0077 val acc 22.2808 best val_acc 26.136364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 119, bulyan: at our-agr n_at 10 e 119 | val loss 1.9843 val acc 24.0057 best val_acc 26.136364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 120, bulyan: at our-agr n_at 10 e 120 | val loss 1.9556 val acc 24.7362 best val_acc 26.136364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 121, bulyan: at our-agr n_at 10 e 121 | val loss 1.9376 val acc 27.0495 best val_acc 27.049513\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 122, bulyan: at our-agr n_at 10 e 122 | val loss 1.9348 val acc 25.2232 best val_acc 27.049513\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 123, bulyan: at our-agr n_at 10 e 123 | val loss 1.9709 val acc 24.5739 best val_acc 27.049513\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 124, bulyan: at our-agr n_at 10 e 124 | val loss 2.0987 val acc 20.7589 best val_acc 27.049513\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 125, bulyan: at our-agr n_at 10 e 125 | val loss 2.0129 val acc 24.8985 best val_acc 27.049513\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 126, bulyan: at our-agr n_at 10 e 126 | val loss 1.9066 val acc 28.3076 best val_acc 28.307630\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 127, bulyan: at our-agr n_at 10 e 127 | val loss 1.8714 val acc 26.9075 best val_acc 28.307630\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 128, bulyan: at our-agr n_at 10 e 128 | val loss 1.8717 val acc 26.1364 best val_acc 28.307630\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 129, bulyan: at our-agr n_at 10 e 129 | val loss 1.9046 val acc 27.4148 best val_acc 28.307630\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 130, bulyan: at our-agr n_at 10 e 130 | val loss 2.1294 val acc 21.0633 best val_acc 28.307630\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 131, bulyan: at our-agr n_at 10 e 131 | val loss 2.2102 val acc 19.8458 best val_acc 28.307630\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 132, bulyan: at our-agr n_at 10 e 132 | val loss 2.0942 val acc 21.3474 best val_acc 28.307630\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 133, bulyan: at our-agr n_at 10 e 133 | val loss 2.0250 val acc 22.1388 best val_acc 28.307630\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 134, bulyan: at our-agr n_at 10 e 134 | val loss 1.9824 val acc 23.0114 best val_acc 28.307630\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 135, bulyan: at our-agr n_at 10 e 135 | val loss 1.9616 val acc 23.7419 best val_acc 28.307630\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 136, bulyan: at our-agr n_at 10 e 136 | val loss 1.9374 val acc 25.3856 best val_acc 28.307630\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 137, bulyan: at our-agr n_at 10 e 137 | val loss 1.9499 val acc 26.3393 best val_acc 28.307630\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 138, bulyan: at our-agr n_at 10 e 138 | val loss 1.9538 val acc 26.5219 best val_acc 28.307630\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 139, bulyan: at our-agr n_at 10 e 139 | val loss 2.0078 val acc 24.1680 best val_acc 28.307630\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 140, bulyan: at our-agr n_at 10 e 140 | val loss 2.0204 val acc 25.1826 best val_acc 28.307630\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 141, bulyan: at our-agr n_at 10 e 141 | val loss 1.9607 val acc 23.0114 best val_acc 28.307630\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 142, bulyan: at our-agr n_at 10 e 142 | val loss 1.8620 val acc 28.5511 best val_acc 28.551136\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 143, bulyan: at our-agr n_at 10 e 143 | val loss 1.8738 val acc 27.6177 best val_acc 28.551136\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 144, bulyan: at our-agr n_at 10 e 144 | val loss 1.9497 val acc 26.4813 best val_acc 28.551136\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 145, bulyan: at our-agr n_at 10 e 145 | val loss 2.1370 val acc 18.9529 best val_acc 28.551136\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 146, bulyan: at our-agr n_at 10 e 146 | val loss 2.0305 val acc 22.1388 best val_acc 28.551136\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 147, bulyan: at our-agr n_at 10 e 147 | val loss 1.9486 val acc 26.8466 best val_acc 28.551136\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 148, bulyan: at our-agr n_at 10 e 148 | val loss 1.8913 val acc 27.9424 best val_acc 28.551136\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 149, bulyan: at our-agr n_at 10 e 149 | val loss 1.9209 val acc 25.9943 best val_acc 28.551136\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 150, bulyan: at our-agr n_at 10 e 150 | val loss 2.0187 val acc 26.1567 best val_acc 28.551136\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 151, bulyan: at our-agr n_at 10 e 151 | val loss 1.9976 val acc 26.5422 best val_acc 28.551136\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 152, bulyan: at our-agr n_at 10 e 152 | val loss 1.8997 val acc 27.8206 best val_acc 28.551136\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 153, bulyan: at our-agr n_at 10 e 153 | val loss 1.8556 val acc 26.1161 best val_acc 28.551136\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 14, 11]\n",
            "epoch: 154, bulyan: at our-agr n_at 10 e 154 | val loss 1.8168 val acc 31.0065 best val_acc 31.006494\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 155, bulyan: at our-agr n_at 10 e 155 | val loss 1.8606 val acc 27.2524 best val_acc 31.006494\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 156, bulyan: at our-agr n_at 10 e 156 | val loss 1.8415 val acc 31.6964 best val_acc 31.696429\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 157, bulyan: at our-agr n_at 10 e 157 | val loss 1.9128 val acc 25.0812 best val_acc 31.696429\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 16, 11, 14]\n",
            "epoch: 158, bulyan: at our-agr n_at 10 e 158 | val loss 1.9726 val acc 29.1193 best val_acc 31.696429\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 16, 14]\n",
            "epoch: 159, bulyan: at our-agr n_at 10 e 159 | val loss 1.8484 val acc 31.5138 best val_acc 31.696429\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 16, 14]\n",
            "epoch: 160, bulyan: at our-agr n_at 10 e 160 | val loss 1.7991 val acc 29.5860 best val_acc 31.696429\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 16, 14]\n",
            "epoch: 161, bulyan: at our-agr n_at 10 e 161 | val loss 1.8337 val acc 30.4992 best val_acc 31.696429\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 16, 14]\n",
            "epoch: 162, bulyan: at our-agr n_at 10 e 162 | val loss 1.9924 val acc 25.6494 best val_acc 31.696429\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 16, 14]\n",
            "epoch: 163, bulyan: at our-agr n_at 10 e 163 | val loss 2.0411 val acc 20.8604 best val_acc 31.696429\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 164, bulyan: at our-agr n_at 10 e 164 | val loss 1.9266 val acc 30.6615 best val_acc 31.696429\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 165, bulyan: at our-agr n_at 10 e 165 | val loss 1.8250 val acc 32.6502 best val_acc 32.650162\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 166, bulyan: at our-agr n_at 10 e 166 | val loss 1.7796 val acc 33.0154 best val_acc 33.015422\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 167, bulyan: at our-agr n_at 10 e 167 | val loss 1.7688 val acc 33.4010 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 168, bulyan: at our-agr n_at 10 e 168 | val loss 1.7658 val acc 32.9343 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 169, bulyan: at our-agr n_at 10 e 169 | val loss 1.8930 val acc 28.1859 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 170, bulyan: at our-agr n_at 10 e 170 | val loss 1.9378 val acc 28.8758 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 16, 14]\n",
            "epoch: 171, bulyan: at our-agr n_at 10 e 171 | val loss 1.8660 val acc 30.3369 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 16, 14]\n",
            "epoch: 172, bulyan: at our-agr n_at 10 e 172 | val loss 1.8227 val acc 30.2963 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 16, 14]\n",
            "epoch: 173, bulyan: at our-agr n_at 10 e 173 | val loss 1.8798 val acc 31.8182 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 174, bulyan: at our-agr n_at 10 e 174 | val loss 2.0202 val acc 28.4497 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 175, bulyan: at our-agr n_at 10 e 175 | val loss 1.9678 val acc 27.4554 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 176, bulyan: at our-agr n_at 10 e 176 | val loss 1.8736 val acc 30.7833 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 177, bulyan: at our-agr n_at 10 e 177 | val loss 1.8217 val acc 31.0065 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 178, bulyan: at our-agr n_at 10 e 178 | val loss 1.8414 val acc 30.5398 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 179, bulyan: at our-agr n_at 10 e 179 | val loss 1.8652 val acc 32.1834 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 180, bulyan: at our-agr n_at 10 e 180 | val loss 1.9327 val acc 29.6672 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 181, bulyan: at our-agr n_at 10 e 181 | val loss 1.8156 val acc 32.5893 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 182, bulyan: at our-agr n_at 10 e 182 | val loss 1.7519 val acc 32.9951 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 183, bulyan: at our-agr n_at 10 e 183 | val loss 1.7846 val acc 31.4123 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 184, bulyan: at our-agr n_at 10 e 184 | val loss 1.7846 val acc 33.2792 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 185, bulyan: at our-agr n_at 10 e 185 | val loss 1.8866 val acc 27.8815 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 16, 14]\n",
            "epoch: 186, bulyan: at our-agr n_at 10 e 186 | val loss 1.8218 val acc 31.1688 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 187, bulyan: at our-agr n_at 10 e 187 | val loss 1.8061 val acc 31.6558 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 188, bulyan: at our-agr n_at 10 e 188 | val loss 1.8782 val acc 29.1193 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 189, bulyan: at our-agr n_at 10 e 189 | val loss 1.9210 val acc 28.1250 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 190, bulyan: at our-agr n_at 10 e 190 | val loss 1.8176 val acc 32.7110 best val_acc 33.400974\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 191, bulyan: at our-agr n_at 10 e 191 | val loss 1.7479 val acc 34.5982 best val_acc 34.598214\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 192, bulyan: at our-agr n_at 10 e 192 | val loss 1.7208 val acc 34.4765 best val_acc 34.598214\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 193, bulyan: at our-agr n_at 10 e 193 | val loss 1.7247 val acc 35.4708 best val_acc 35.470779\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 194, bulyan: at our-agr n_at 10 e 194 | val loss 1.7820 val acc 34.1315 best val_acc 35.470779\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 195, bulyan: at our-agr n_at 10 e 195 | val loss 1.8696 val acc 31.9399 best val_acc 35.470779\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 196, bulyan: at our-agr n_at 10 e 196 | val loss 1.9316 val acc 30.6615 best val_acc 35.470779\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 197, bulyan: at our-agr n_at 10 e 197 | val loss 1.8724 val acc 32.0008 best val_acc 35.470779\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 198, bulyan: at our-agr n_at 10 e 198 | val loss 1.7676 val acc 33.0357 best val_acc 35.470779\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 199, bulyan: at our-agr n_at 10 e 199 | val loss 1.7499 val acc 34.6388 best val_acc 35.470779\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 200, bulyan: at our-agr n_at 10 e 200 | val loss 1.7867 val acc 32.0211 best val_acc 35.470779\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 201, bulyan: at our-agr n_at 10 e 201 | val loss 1.7720 val acc 35.5114 best val_acc 35.511364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 202, bulyan: at our-agr n_at 10 e 202 | val loss 1.7738 val acc 32.9748 best val_acc 35.511364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 203, bulyan: at our-agr n_at 10 e 203 | val loss 1.7653 val acc 34.3141 best val_acc 35.511364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 204, bulyan: at our-agr n_at 10 e 204 | val loss 1.8672 val acc 31.0471 best val_acc 35.511364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 16]\n",
            "epoch: 205, bulyan: at our-agr n_at 10 e 205 | val loss 1.8344 val acc 34.7606 best val_acc 35.511364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 32]\n",
            "epoch: 206, bulyan: at our-agr n_at 10 e 206 | val loss 1.7502 val acc 35.3896 best val_acc 35.511364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 32]\n",
            "epoch: 207, bulyan: at our-agr n_at 10 e 207 | val loss 1.7229 val acc 34.1518 best val_acc 35.511364\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 32]\n",
            "epoch: 208, bulyan: at our-agr n_at 10 e 208 | val loss 1.7039 val acc 37.6015 best val_acc 37.601461\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 14, 32]\n",
            "epoch: 209, bulyan: at our-agr n_at 10 e 209 | val loss 1.8512 val acc 34.2735 best val_acc 37.601461\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 32, 18]\n",
            "epoch: 210, bulyan: at our-agr n_at 10 e 210 | val loss 1.9442 val acc 31.9602 best val_acc 37.601461\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 32, 18]\n",
            "epoch: 211, bulyan: at our-agr n_at 10 e 211 | val loss 1.9207 val acc 29.7078 best val_acc 37.601461\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 32, 18]\n",
            "epoch: 212, bulyan: at our-agr n_at 10 e 212 | val loss 1.8760 val acc 29.2817 best val_acc 37.601461\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 32, 18]\n",
            "epoch: 213, bulyan: at our-agr n_at 10 e 213 | val loss 1.7275 val acc 36.9318 best val_acc 37.601461\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 32, 18]\n",
            "epoch: 214, bulyan: at our-agr n_at 10 e 214 | val loss 1.6865 val acc 36.3231 best val_acc 37.601461\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 32, 18]\n",
            "epoch: 215, bulyan: at our-agr n_at 10 e 215 | val loss 1.6558 val acc 38.0276 best val_acc 38.027597\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 32, 18]\n",
            "epoch: 216, bulyan: at our-agr n_at 10 e 216 | val loss 1.7874 val acc 32.2646 best val_acc 38.027597\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 32, 18]\n",
            "epoch: 217, bulyan: at our-agr n_at 10 e 217 | val loss 1.8143 val acc 32.2646 best val_acc 38.027597\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 32, 18]\n",
            "epoch: 218, bulyan: at our-agr n_at 10 e 218 | val loss 1.7515 val acc 34.4968 best val_acc 38.027597\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 32, 18]\n",
            "epoch: 219, bulyan: at our-agr n_at 10 e 219 | val loss 1.6858 val acc 38.0276 best val_acc 38.027597\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 32, 18]\n",
            "epoch: 220, bulyan: at our-agr n_at 10 e 220 | val loss 1.6826 val acc 37.4797 best val_acc 38.027597\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 29, 19, 11, 32, 18]\n",
            "epoch: 221, bulyan: at our-agr n_at 10 e 221 | val loss 1.7868 val acc 35.8563 best val_acc 38.027597\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 11, 32, 18]\n",
            "epoch: 222, bulyan: at our-agr n_at 10 e 222 | val loss 1.8386 val acc 32.6299 best val_acc 38.027597\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 11, 32, 18]\n",
            "epoch: 223, bulyan: at our-agr n_at 10 e 223 | val loss 1.8215 val acc 32.2849 best val_acc 38.027597\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 11, 32, 18]\n",
            "epoch: 224, bulyan: at our-agr n_at 10 e 224 | val loss 1.7086 val acc 38.1494 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 11, 32, 18]\n",
            "epoch: 225, bulyan: at our-agr n_at 10 e 225 | val loss 1.7864 val acc 33.1981 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 11, 32, 18]\n",
            "epoch: 226, bulyan: at our-agr n_at 10 e 226 | val loss 1.7686 val acc 34.1315 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 11, 32, 18]\n",
            "epoch: 227, bulyan: at our-agr n_at 10 e 227 | val loss 1.7407 val acc 37.1956 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 11, 32, 18]\n",
            "epoch: 228, bulyan: at our-agr n_at 10 e 228 | val loss 1.7755 val acc 33.8271 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 11, 32, 18]\n",
            "epoch: 229, bulyan: at our-agr n_at 10 e 229 | val loss 1.9777 val acc 26.6234 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 11, 32, 18]\n",
            "epoch: 230, bulyan: at our-agr n_at 10 e 230 | val loss 1.9256 val acc 26.1769 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 11, 32, 18]\n",
            "epoch: 231, bulyan: at our-agr n_at 10 e 231 | val loss 1.8550 val acc 30.1339 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 11, 32, 18]\n",
            "epoch: 232, bulyan: at our-agr n_at 10 e 232 | val loss 1.7734 val acc 32.2443 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 11, 32, 18]\n",
            "epoch: 233, bulyan: at our-agr n_at 10 e 233 | val loss 1.7671 val acc 32.8531 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 11, 32, 18]\n",
            "epoch: 234, bulyan: at our-agr n_at 10 e 234 | val loss 1.7673 val acc 35.5722 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 32, 11, 18]\n",
            "epoch: 235, bulyan: at our-agr n_at 10 e 235 | val loss 1.7485 val acc 37.0536 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 32, 11, 18]\n",
            "epoch: 236, bulyan: at our-agr n_at 10 e 236 | val loss 1.7018 val acc 38.1494 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 11, 32, 18]\n",
            "epoch: 237, bulyan: at our-agr n_at 10 e 237 | val loss 1.7350 val acc 37.8247 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 32, 11, 18]\n",
            "epoch: 238, bulyan: at our-agr n_at 10 e 238 | val loss 1.7033 val acc 37.4391 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 32, 11, 18]\n",
            "epoch: 239, bulyan: at our-agr n_at 10 e 239 | val loss 1.7517 val acc 37.2768 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 32, 11, 18]\n",
            "epoch: 240, bulyan: at our-agr n_at 10 e 240 | val loss 1.6855 val acc 37.5406 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 19, 29, 32, 11, 18]\n",
            "epoch: 241, bulyan: at our-agr n_at 10 e 241 | val loss 1.7230 val acc 37.7435 best val_acc 38.149351\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 19, 29, 11, 18]\n",
            "epoch: 242, bulyan: at our-agr n_at 10 e 242 | val loss 1.6585 val acc 39.0828 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 19, 29, 11, 18]\n",
            "epoch: 243, bulyan: at our-agr n_at 10 e 243 | val loss 1.7402 val acc 36.1201 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 19, 29, 18, 11]\n",
            "epoch: 244, bulyan: at our-agr n_at 10 e 244 | val loss 1.7100 val acc 36.3231 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 19, 29, 18, 11]\n",
            "epoch: 245, bulyan: at our-agr n_at 10 e 245 | val loss 1.7621 val acc 35.3896 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 19, 29, 18, 11]\n",
            "epoch: 246, bulyan: at our-agr n_at 10 e 246 | val loss 1.7828 val acc 33.8677 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 19, 11, 29, 18]\n",
            "epoch: 247, bulyan: at our-agr n_at 10 e 247 | val loss 1.6951 val acc 37.2362 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 19, 11, 29, 18]\n",
            "epoch: 248, bulyan: at our-agr n_at 10 e 248 | val loss 1.7347 val acc 37.0536 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 11, 19, 29, 18]\n",
            "epoch: 249, bulyan: at our-agr n_at 10 e 249 | val loss 1.6967 val acc 38.2102 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 11, 19, 29, 18]\n",
            "epoch: 250, bulyan: at our-agr n_at 10 e 250 | val loss 1.6919 val acc 37.4188 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 11, 19, 29, 18]\n",
            "epoch: 251, bulyan: at our-agr n_at 10 e 251 | val loss 1.6644 val acc 38.9002 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 11, 19, 18, 29]\n",
            "epoch: 252, bulyan: at our-agr n_at 10 e 252 | val loss 1.7011 val acc 38.3726 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 11, 19, 18, 29]\n",
            "epoch: 253, bulyan: at our-agr n_at 10 e 253 | val loss 1.8809 val acc 32.8328 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 11, 18, 19, 29]\n",
            "epoch: 254, bulyan: at our-agr n_at 10 e 254 | val loss 1.8078 val acc 36.1810 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 11, 19, 18, 29]\n",
            "epoch: 255, bulyan: at our-agr n_at 10 e 255 | val loss 1.7840 val acc 34.2938 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 11, 18, 19, 29]\n",
            "epoch: 256, bulyan: at our-agr n_at 10 e 256 | val loss 1.6432 val acc 38.7175 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 11, 18, 19, 29]\n",
            "epoch: 257, bulyan: at our-agr n_at 10 e 257 | val loss 1.7371 val acc 36.8304 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 11, 18, 19, 29]\n",
            "epoch: 258, bulyan: at our-agr n_at 10 e 258 | val loss 1.8655 val acc 32.9343 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 11, 18, 19, 29]\n",
            "epoch: 259, bulyan: at our-agr n_at 10 e 259 | val loss 1.8262 val acc 30.9862 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 11, 18, 19, 29]\n",
            "epoch: 260, bulyan: at our-agr n_at 10 e 260 | val loss 1.7599 val acc 32.7313 best val_acc 39.082792\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 11, 18, 19, 29]\n",
            "epoch: 261, bulyan: at our-agr n_at 10 e 261 | val loss 1.6323 val acc 41.2135 best val_acc 41.213474\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 11, 18, 19, 29]\n",
            "epoch: 262, bulyan: at our-agr n_at 10 e 262 | val loss 1.6233 val acc 39.9959 best val_acc 41.213474\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 3, 32, 11, 18, 19, 29]\n",
            "epoch: 263, bulyan: at our-agr n_at 10 e 263 | val loss 1.6722 val acc 38.2711 best val_acc 41.213474\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 3, 11, 18, 19, 29]\n",
            "epoch: 264, bulyan: at our-agr n_at 10 e 264 | val loss 1.8323 val acc 36.5260 best val_acc 41.213474\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 3, 11, 18, 19, 29]\n",
            "epoch: 265, bulyan: at our-agr n_at 10 e 265 | val loss 1.7387 val acc 34.9026 best val_acc 41.213474\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 3, 11, 18, 19, 29]\n",
            "epoch: 266, bulyan: at our-agr n_at 10 e 266 | val loss 1.6298 val acc 38.6161 best val_acc 41.213474\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 3, 11, 18, 19, 29]\n",
            "epoch: 267, bulyan: at our-agr n_at 10 e 267 | val loss 1.5905 val acc 41.8222 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 3, 11, 18, 19, 29]\n",
            "epoch: 268, bulyan: at our-agr n_at 10 e 268 | val loss 1.6266 val acc 39.4886 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 3, 11, 18, 19, 29]\n",
            "epoch: 269, bulyan: at our-agr n_at 10 e 269 | val loss 1.7679 val acc 36.7898 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 3, 11, 18, 19, 29]\n",
            "epoch: 270, bulyan: at our-agr n_at 10 e 270 | val loss 1.8328 val acc 33.8880 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 3, 11, 18, 19, 29]\n",
            "epoch: 271, bulyan: at our-agr n_at 10 e 271 | val loss 1.8469 val acc 33.1981 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 3, 11, 18, 19, 29]\n",
            "epoch: 272, bulyan: at our-agr n_at 10 e 272 | val loss 1.8686 val acc 30.9659 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 11, 3, 18, 19, 29]\n",
            "epoch: 273, bulyan: at our-agr n_at 10 e 273 | val loss 1.9170 val acc 31.4123 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 11, 3, 18, 19, 29]\n",
            "epoch: 274, bulyan: at our-agr n_at 10 e 274 | val loss 1.7096 val acc 39.0219 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 11, 3, 18, 19, 29]\n",
            "epoch: 275, bulyan: at our-agr n_at 10 e 275 | val loss 1.6994 val acc 37.1956 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 3, 11, 18, 19, 29]\n",
            "epoch: 276, bulyan: at our-agr n_at 10 e 276 | val loss 1.5941 val acc 40.7670 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 11, 3, 18, 19, 29]\n",
            "epoch: 277, bulyan: at our-agr n_at 10 e 277 | val loss 1.6296 val acc 40.6250 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 3, 11, 18, 19, 29]\n",
            "epoch: 278, bulyan: at our-agr n_at 10 e 278 | val loss 1.6373 val acc 40.4221 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 3, 11, 18, 19, 29]\n",
            "epoch: 279, bulyan: at our-agr n_at 10 e 279 | val loss 1.7261 val acc 36.6071 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 3, 11, 18, 19, 29]\n",
            "epoch: 280, bulyan: at our-agr n_at 10 e 280 | val loss 1.6596 val acc 40.0974 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 3, 11, 18, 19, 29]\n",
            "epoch: 281, bulyan: at our-agr n_at 10 e 281 | val loss 1.6095 val acc 40.7873 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 18, 11, 3, 19, 29]\n",
            "epoch: 282, bulyan: at our-agr n_at 10 e 282 | val loss 1.6426 val acc 39.8539 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 18, 11, 3, 19, 29]\n",
            "epoch: 283, bulyan: at our-agr n_at 10 e 283 | val loss 1.6580 val acc 37.9870 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 18, 11, 3, 19, 29]\n",
            "epoch: 284, bulyan: at our-agr n_at 10 e 284 | val loss 1.8255 val acc 35.4302 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 18, 11, 3, 19, 29]\n",
            "epoch: 285, bulyan: at our-agr n_at 10 e 285 | val loss 1.7352 val acc 34.6591 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 18, 11, 3, 19, 29]\n",
            "epoch: 286, bulyan: at our-agr n_at 10 e 286 | val loss 1.6242 val acc 38.7581 best val_acc 41.822240\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 18, 11, 3, 19, 29]\n",
            "epoch: 287, bulyan: at our-agr n_at 10 e 287 | val loss 1.5694 val acc 42.1469 best val_acc 42.146916\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 18, 11, 3, 19, 29]\n",
            "epoch: 288, bulyan: at our-agr n_at 10 e 288 | val loss 1.6041 val acc 41.4164 best val_acc 42.146916\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 11, 18, 3, 19, 29]\n",
            "epoch: 289, bulyan: at our-agr n_at 10 e 289 | val loss 1.6623 val acc 38.2305 best val_acc 42.146916\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 11, 18, 3, 19, 29]\n",
            "epoch: 290, bulyan: at our-agr n_at 10 e 290 | val loss 1.6279 val acc 38.3320 best val_acc 42.146916\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 11, 18, 3, 19, 29]\n",
            "epoch: 291, bulyan: at our-agr n_at 10 e 291 | val loss 1.6357 val acc 40.1177 best val_acc 42.146916\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 11, 18, 3, 19, 29]\n",
            "epoch: 292, bulyan: at our-agr n_at 10 e 292 | val loss 1.6320 val acc 40.8279 best val_acc 42.146916\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 11, 18, 3, 19, 29]\n",
            "epoch: 293, bulyan: at our-agr n_at 10 e 293 | val loss 1.7894 val acc 38.2305 best val_acc 42.146916\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 11, 18, 3, 19, 29]\n",
            "epoch: 294, bulyan: at our-agr n_at 10 e 294 | val loss 2.0864 val acc 30.6209 best val_acc 42.146916\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 18, 11, 3, 19, 29]\n",
            "epoch: 295, bulyan: at our-agr n_at 10 e 295 | val loss 1.7602 val acc 35.7752 best val_acc 42.146916\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 18, 11, 3, 19, 29]\n",
            "epoch: 296, bulyan: at our-agr n_at 10 e 296 | val loss 1.7197 val acc 39.4075 best val_acc 42.146916\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 18, 11, 3, 19, 29]\n",
            "epoch: 297, bulyan: at our-agr n_at 10 e 297 | val loss 1.6750 val acc 39.6510 best val_acc 42.146916\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 18, 11, 3, 19, 29]\n",
            "epoch: 298, bulyan: at our-agr n_at 10 e 298 | val loss 1.7252 val acc 38.0073 best val_acc 42.146916\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 11, 18, 3, 19, 29]\n",
            "epoch: 299, bulyan: at our-agr n_at 10 e 299 | val loss 1.7157 val acc 37.5406 best val_acc 42.146916\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 11, 18, 3, 19, 29]\n",
            "epoch: 300, bulyan: at our-agr n_at 10 e 300 | val loss 1.5820 val acc 41.9440 best val_acc 42.146916\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 11, 18, 3, 19, 29]\n",
            "epoch: 301, bulyan: at our-agr n_at 10 e 301 | val loss 1.5559 val acc 42.8571 best val_acc 42.857143\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 11, 18, 3, 19, 29]\n",
            "epoch: 302, bulyan: at our-agr n_at 10 e 302 | val loss 1.5363 val acc 43.5471 best val_acc 43.547078\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 11, 18, 3, 19, 29]\n",
            "epoch: 303, bulyan: at our-agr n_at 10 e 303 | val loss 1.5757 val acc 42.4716 best val_acc 43.547078\n",
            "50\n",
            "discarded index [31, 7, 43, 15, 32, 11, 18, 3, 19, 29]\n",
            "epoch: 304, bulyan: at our-agr n_at 10 e 304 | val loss 1.5843 val acc 41.9034 best val_acc 43.547078\n",
            "50\n"
          ]
        }
      ],
      "source": [
        "batch_size=250\n",
        "resume=0\n",
        "\n",
        "schedule=[1000]\n",
        "\n",
        "gamma=.5\n",
        "opt = 'sgd'\n",
        "fed_lr=0.5\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "aggregation='bulyan'\n",
        "multi_k = False\n",
        "candidates = []\n",
        "\n",
        "at_type='our-agr'\n",
        "dev_type ='std'\n",
        "n_attackers=[10]\n",
        "\n",
        "arch='alexnet'\n",
        "chkpt='./'+aggregation\n",
        "\n",
        "past_mean = []\n",
        "past_abs_sum = []\n",
        "past_median = []\n",
        "past_lower_quartile = []\n",
        "past_upper_quartile = []\n",
        "past_std = []\n",
        "\n",
        "\n",
        "for n_attacker in n_attackers:\n",
        "    epoch_num = 0\n",
        "    best_global_acc = 0\n",
        "    best_global_te_acc = 0\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
        "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
        "\n",
        "    df = pd.DataFrame(columns = ['epoch', 'loss', 'validation accuracy', 'best validation accuracy'])\n",
        "\n",
        "    history = [[] for _ in range(n_users)]\n",
        "    model_grads = []\n",
        "    flag = False\n",
        "    while epoch_num <= nepochs:\n",
        "        user_grads=[]\n",
        "\n",
        "        for i in range(n_users): # 50\n",
        "            nbatches = user_tr_len[i]//batch_size\n",
        "\n",
        "            inputs = user_train_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "            targets = user_train_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "\n",
        "            targets = targets.type(torch.LongTensor)\n",
        "\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
        "\n",
        "            outputs = fed_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            fed_model.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            param_grad=[]\n",
        "            for param in fed_model.parameters():\n",
        "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
        "\n",
        "\n",
        "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
        "        \n",
        "        print(len(user_grads))\n",
        "        if epoch_num in schedule:\n",
        "            for param_group in optimizer_fed.param_groups:\n",
        "                param_group['lr'] *= gamma\n",
        "                print('New learnin rate ', param_group['lr'])\n",
        "\n",
        "        updates_abs_mean = user_grads.abs().mean(dim=1, keepdim=True)\n",
        "\n",
        "        if n_attacker > 0 and epoch_num > 1:\n",
        "            # mal_updates = our_attack_mean(n_users, history, user_grads, model_grads, n_attacker, arch, criterion, user_train_data_tensors, user_train_label_tensors)\n",
        "            # malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0) # torch.Size([50, 2472266])\n",
        "            malicious_grads = simple_attack(user_grads, n_attacker)\n",
        "        else:   # No attack\n",
        "            malicious_grads = user_grads\n",
        "        \n",
        "        if not (malicious_grads.shape[0]==50):\n",
        "            print(malicious_grads.shape)\n",
        "\n",
        "        updates_abs_mean = malicious_grads.abs().mean(dim=1, keepdim=True)\n",
        "\n",
        "        for i in range(n_users):\n",
        "            history[i].extend(updates_abs_mean[i].cpu())\n",
        "\n",
        "        agg_grads = our_mean_defense(malicious_grads, n_attacker, history)\n",
        "\n",
        "\n",
        "        del user_grads\n",
        "\n",
        "        start_idx=0\n",
        "\n",
        "        optimizer_fed.zero_grad()\n",
        "\n",
        "        model_grads=[]\n",
        "\n",
        "        for i, param in enumerate(fed_model.parameters()):\n",
        "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
        "            start_idx=start_idx+len(param.data.view(-1))\n",
        "            param_=param_.cuda()\n",
        "            model_grads.append(param_)\n",
        "\n",
        "        optimizer_fed.step(model_grads)\n",
        "\n",
        "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
        "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
        "\n",
        "        is_best = best_global_acc < val_acc\n",
        "\n",
        "        best_global_acc = max(best_global_acc, val_acc)\n",
        "\n",
        "        if is_best:\n",
        "            best_global_te_acc = te_acc\n",
        "\n",
        "\n",
        "        print('epoch: %d, %s: at %s n_at %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(epoch_num, aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc))\n",
        "        new_row = pd.DataFrame([{\n",
        "            'epoch': epoch_num,\n",
        "            'loss': val_loss,\n",
        "            'validation accuracy': val_acc,\n",
        "            'best validation accuracy': best_global_acc\n",
        "            }])\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "        df.to_csv('./our attack + 10 attacker + our defense.csv', index=False)\n",
        "\n",
        "        if val_loss > 1000:\n",
        "            print('val loss %f too high'%val_loss)\n",
        "            break\n",
        "\n",
        "        epoch_num+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the data\n",
        "for col in ['loss', 'validation accuracy', 'best validation accuracy']:\n",
        "    plt.plot(df['epoch'].tolist(), df[col].tolist())\n",
        "    plt.title(\"our attack + 10 attacker + our defense_\"+col)\n",
        "    plt.savefig('./our attack + 10 attacker + our defense ' + col + '.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size=250\n",
        "resume=0\n",
        "\n",
        "schedule=[1000]\n",
        "\n",
        "gamma=.5\n",
        "opt = 'sgd'\n",
        "fed_lr=0.5\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "aggregation='bulyan'\n",
        "multi_k = False\n",
        "candidates = []\n",
        "\n",
        "at_type='our-agr'\n",
        "dev_type ='std'\n",
        "n_attackers=[0]\n",
        "\n",
        "arch='alexnet'\n",
        "chkpt='./'+aggregation\n",
        "\n",
        "past_mean = []\n",
        "past_abs_sum = []\n",
        "past_median = []\n",
        "past_lower_quartile = []\n",
        "past_upper_quartile = []\n",
        "past_std = []\n",
        "\n",
        "\n",
        "for n_attacker in n_attackers:\n",
        "    epoch_num = 0\n",
        "    best_global_acc = 0\n",
        "    best_global_te_acc = 0\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
        "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
        "\n",
        "    df = pd.DataFrame(columns = ['epoch', 'loss', 'validation accuracy', 'best validation accuracy'])\n",
        "\n",
        "    history = [[] for _ in range(n_users)]\n",
        "    model_grads = []\n",
        "    flag = False\n",
        "    while epoch_num <= nepochs:\n",
        "        user_grads=[]\n",
        "\n",
        "        for i in range(n_users): # 50\n",
        "            nbatches = user_tr_len[i]//batch_size\n",
        "\n",
        "            inputs = user_train_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "            targets = user_train_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "\n",
        "            targets = targets.type(torch.LongTensor)\n",
        "\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
        "\n",
        "            outputs = fed_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            fed_model.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            param_grad=[]\n",
        "            for param in fed_model.parameters():\n",
        "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
        "\n",
        "\n",
        "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
        "        \n",
        "        print(len(user_grads))\n",
        "        if epoch_num in schedule:\n",
        "            for param_group in optimizer_fed.param_groups:\n",
        "                param_group['lr'] *= gamma\n",
        "                print('New learnin rate ', param_group['lr'])\n",
        "\n",
        "        updates_abs_mean = user_grads.abs().mean(dim=1, keepdim=True)\n",
        "\n",
        "        if n_attacker > 0 and epoch_num > 1:\n",
        "            # mal_updates = our_attack_mean(n_users, history, user_grads, model_grads, n_attacker, arch, criterion, user_train_data_tensors, user_train_label_tensors)\n",
        "            # malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0) # torch.Size([50, 2472266])\n",
        "            malicious_grads = simple_attack(user_grads, n_attacker)\n",
        "        else:   # No attack\n",
        "            malicious_grads = user_grads\n",
        "        \n",
        "        if not (malicious_grads.shape[0]==50):\n",
        "            print(malicious_grads.shape)\n",
        "\n",
        "        updates_abs_mean = malicious_grads.abs().mean(dim=1, keepdim=True)\n",
        "\n",
        "        for i in range(n_users):\n",
        "            history[i].extend(updates_abs_mean[i].cpu())\n",
        "\n",
        "        agg_grads = our_mean_defense(malicious_grads, n_attacker, history)\n",
        "\n",
        "\n",
        "        del user_grads\n",
        "\n",
        "        start_idx=0\n",
        "\n",
        "        optimizer_fed.zero_grad()\n",
        "\n",
        "        model_grads=[]\n",
        "\n",
        "        for i, param in enumerate(fed_model.parameters()):\n",
        "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
        "            start_idx=start_idx+len(param.data.view(-1))\n",
        "            param_=param_.cuda()\n",
        "            model_grads.append(param_)\n",
        "\n",
        "        optimizer_fed.step(model_grads)\n",
        "\n",
        "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
        "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
        "\n",
        "        is_best = best_global_acc < val_acc\n",
        "\n",
        "        best_global_acc = max(best_global_acc, val_acc)\n",
        "\n",
        "        if is_best:\n",
        "            best_global_te_acc = te_acc\n",
        "\n",
        "\n",
        "        print('epoch: %d, %s: at %s n_at %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(epoch_num, aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc))\n",
        "        new_row = pd.DataFrame([{\n",
        "            'epoch': epoch_num,\n",
        "            'loss': val_loss,\n",
        "            'validation accuracy': val_acc,\n",
        "            'best validation accuracy': best_global_acc\n",
        "            }])\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "        df.to_csv('./no attack + our defense.csv', index=False)\n",
        "\n",
        "        if val_loss > 1000:\n",
        "            print('val loss %f too high'%val_loss)\n",
        "            break\n",
        "\n",
        "        epoch_num+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "qVnYsxMFlBNI",
        "outputId": "586e70ff-ac6d-4cb6-e21c-05d6fe6cca05"
      },
      "outputs": [],
      "source": [
        "# ne = 304\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# x = np.arange(ne)\n",
        "# for i in range(40):\n",
        "#     y = history[i+10]\n",
        "#     plt.plot(x, y, color='green')\n",
        "\n",
        "# for i in range(10):\n",
        "#     y = history[i]\n",
        "#     plt.plot(x, y, color='red')\n",
        "\n",
        "# plt.title('history')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.ylabel('mean')\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "hzJ2o8FAY7H1",
        "outputId": "bd3b4bef-a1ac-44c1-993e-6ec7a79e6670"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# ne =  304\n",
        "\n",
        "# for j in range(10):\n",
        "#   plt.figure(figsize=(10, 6))\n",
        "#   x = np.arange(ne)\n",
        "#   for i in range(40):\n",
        "#       y = history[i+10]\n",
        "#       plt.plot(x, y, color='green')\n",
        "\n",
        "#   # for i in range(10):\n",
        "#   #     y = history[i]\n",
        "#   #     plt.plot(x, y, color='red')\n",
        "#   y = history[j]\n",
        "#   plt.plot(x, y, color='red')\n",
        "\n",
        "\n",
        "#   plt.title('history')\n",
        "#   plt.xlabel('epoch')\n",
        "#   plt.ylabel('mean')\n",
        "#   plt.legend()\n",
        "#   plt.grid(True)\n",
        "#   plt.savefig('./history_malicious_'+str(j)+'.png')\n",
        "#   plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWTYqauonrc9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the data\n",
        "for col in ['loss', 'validation accuracy', 'best validation accuracy']:\n",
        "    plt.plot(df['epoch'].tolist(), df[col].tolist())\n",
        "    plt.title(\"no attack + our defense_\"+col)\n",
        "    plt.savefig('./no attack + no defense ' + col + '.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size=250\n",
        "resume=0\n",
        "\n",
        "schedule=[1000]\n",
        "\n",
        "gamma=.5\n",
        "opt = 'sgd'\n",
        "fed_lr=0.5\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "aggregation='bulyan'\n",
        "multi_k = False\n",
        "candidates = []\n",
        "\n",
        "at_type='our-agr'\n",
        "dev_type ='std'\n",
        "n_attackers=[10]\n",
        "\n",
        "arch='alexnet'\n",
        "chkpt='./'+aggregation\n",
        "\n",
        "past_mean = []\n",
        "past_abs_sum = []\n",
        "past_median = []\n",
        "past_lower_quartile = []\n",
        "past_upper_quartile = []\n",
        "past_std = []\n",
        "\n",
        "\n",
        "for n_attacker in n_attackers:\n",
        "    epoch_num = 0\n",
        "    best_global_acc = 0\n",
        "    best_global_te_acc = 0\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
        "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
        "\n",
        "    df = pd.DataFrame(columns = ['epoch', 'loss', 'validation accuracy', 'best validation accuracy'])\n",
        "\n",
        "    history = [[] for _ in range(n_users)]\n",
        "    while epoch_num <= nepochs:\n",
        "        user_grads=[]\n",
        "\n",
        "        for i in range(n_users): # 50\n",
        "            nbatches = user_tr_len[i]//batch_size\n",
        "\n",
        "            inputs = user_train_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "            targets = user_train_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "\n",
        "            targets = targets.type(torch.LongTensor)\n",
        "\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
        "\n",
        "            outputs = fed_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            fed_model.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            param_grad=[]\n",
        "            for param in fed_model.parameters():\n",
        "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
        "\n",
        "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
        "\n",
        "        if epoch_num in schedule:\n",
        "            for param_group in optimizer_fed.param_groups:\n",
        "                param_group['lr'] *= gamma\n",
        "                print('New learnin rate ', param_group['lr'])\n",
        "\n",
        "        updates_abs_mean = user_grads.abs().mean(dim=1, keepdim=True)\n",
        "\n",
        "        if n_attacker > 0:\n",
        "            # mal_updates = our_attack_mean(user_grads, n_attacker, arch, criterion, user_train_data_tensors, user_train_label_tensors)\n",
        "            # malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0) # torch.Size([50, 2472266])\n",
        "            malicious_grads = simple_attack(user_grads, n_attacker)\n",
        "        else:   # No attack\n",
        "            malicious_grads = user_grads\n",
        "\n",
        "        if not (malicious_grads.shape[0]==50):\n",
        "            print(malicious_grads.shape)\n",
        "\n",
        "        updates_abs_mean = malicious_grads.abs().mean(dim=1, keepdim=True)\n",
        "\n",
        "        for i in range(n_users):\n",
        "            history[i].extend(updates_abs_mean[i].cpu())\n",
        "\n",
        "        agg_grads = no_defense_aggregation(malicious_grads)\n",
        "\n",
        "        del user_grads\n",
        "\n",
        "        start_idx=0\n",
        "\n",
        "        optimizer_fed.zero_grad()\n",
        "\n",
        "        model_grads=[]\n",
        "\n",
        "        for i, param in enumerate(fed_model.parameters()):\n",
        "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
        "            start_idx=start_idx+len(param.data.view(-1))\n",
        "            param_=param_.cuda()\n",
        "            model_grads.append(param_)\n",
        "\n",
        "        optimizer_fed.step(model_grads)\n",
        "\n",
        "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
        "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
        "\n",
        "        is_best = best_global_acc < val_acc\n",
        "\n",
        "        best_global_acc = max(best_global_acc, val_acc)\n",
        "\n",
        "        if is_best:\n",
        "            best_global_te_acc = te_acc\n",
        "\n",
        "\n",
        "        print('epoch: %d, %s: at %s n_at %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(epoch_num, aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc))\n",
        "        new_row = pd.DataFrame([{\n",
        "            'epoch': epoch_num,\n",
        "            'loss': val_loss,\n",
        "            'validation accuracy': val_acc,\n",
        "            'best validation accuracy': best_global_acc\n",
        "            }])\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "        df.to_csv('./our attack + 10 attacker + no defense.csv', index=False)\n",
        "\n",
        "        if val_loss > 1000:\n",
        "            print('val loss %f too high'%val_loss)\n",
        "            break\n",
        "\n",
        "        epoch_num+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the data\n",
        "for col in ['loss', 'validation accuracy', 'best validation accuracy']:\n",
        "    plt.plot(df['epoch'].tolist(), df[col].tolist())\n",
        "    plt.title(\"our attack + 10 attacker + no defense_\"+col)\n",
        "    plt.savefig('./our attack + 10 attacker + no defense ' + col + '.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size=250\n",
        "resume=0\n",
        "\n",
        "schedule=[1000]\n",
        "\n",
        "gamma=.5\n",
        "opt = 'sgd'\n",
        "fed_lr=0.5\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "aggregation='bulyan'\n",
        "multi_k = False\n",
        "candidates = []\n",
        "\n",
        "at_type='our-agr'\n",
        "dev_type ='std'\n",
        "n_attackers=[40]\n",
        "\n",
        "arch='alexnet'\n",
        "chkpt='./'+aggregation\n",
        "\n",
        "past_mean = []\n",
        "past_abs_sum = []\n",
        "past_median = []\n",
        "past_lower_quartile = []\n",
        "past_upper_quartile = []\n",
        "past_std = []\n",
        "\n",
        "\n",
        "for n_attacker in n_attackers:\n",
        "    epoch_num = 0\n",
        "    best_global_acc = 0\n",
        "    best_global_te_acc = 0\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
        "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
        "\n",
        "    df = pd.DataFrame(columns = ['epoch', 'loss', 'validation accuracy', 'best validation accuracy'])\n",
        "\n",
        "    history = [[] for _ in range(n_users)]\n",
        "    while epoch_num <= nepochs:\n",
        "        user_grads=[]\n",
        "\n",
        "        for i in range(n_users): # 50\n",
        "            nbatches = user_tr_len[i]//batch_size\n",
        "\n",
        "            inputs = user_train_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "            targets = user_train_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "\n",
        "            targets = targets.type(torch.LongTensor)\n",
        "\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
        "\n",
        "            outputs = fed_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            fed_model.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            param_grad=[]\n",
        "            for param in fed_model.parameters():\n",
        "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
        "\n",
        "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
        "\n",
        "        if epoch_num in schedule:\n",
        "            for param_group in optimizer_fed.param_groups:\n",
        "                param_group['lr'] *= gamma\n",
        "                print('New learnin rate ', param_group['lr'])\n",
        "\n",
        "        updates_abs_mean = user_grads.abs().mean(dim=1, keepdim=True)\n",
        "\n",
        "        if n_attacker > 0:\n",
        "            mal_updates = our_attack_mean(user_grads, n_attacker, arch, criterion, user_train_data_tensors, user_train_label_tensors)\n",
        "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0) # torch.Size([50, 2472266])\n",
        "        else:   # No attack\n",
        "            malicious_grads = user_grads\n",
        "\n",
        "        if not (malicious_grads.shape[0]==50):\n",
        "            print(malicious_grads.shape)\n",
        "\n",
        "        updates_abs_mean = malicious_grads.abs().mean(dim=1, keepdim=True)\n",
        "\n",
        "        for i in range(n_users):\n",
        "            history[i].extend(updates_abs_mean[i].cpu())\n",
        "\n",
        "        agg_grads = no_defense_aggregation(malicious_grads)\n",
        "\n",
        "        del user_grads\n",
        "\n",
        "        start_idx=0\n",
        "\n",
        "        optimizer_fed.zero_grad()\n",
        "\n",
        "        model_grads=[]\n",
        "\n",
        "        for i, param in enumerate(fed_model.parameters()):\n",
        "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
        "            start_idx=start_idx+len(param.data.view(-1))\n",
        "            param_=param_.cuda()\n",
        "            model_grads.append(param_)\n",
        "\n",
        "        optimizer_fed.step(model_grads)\n",
        "\n",
        "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
        "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
        "\n",
        "        is_best = best_global_acc < val_acc\n",
        "\n",
        "        best_global_acc = max(best_global_acc, val_acc)\n",
        "\n",
        "        if is_best:\n",
        "            best_global_te_acc = te_acc\n",
        "\n",
        "\n",
        "        print('epoch: %d, %s: at %s n_at %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(epoch_num, aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc))\n",
        "        new_row = pd.DataFrame([{\n",
        "            'epoch': epoch_num,\n",
        "            'loss': val_loss,\n",
        "            'validation accuracy': val_acc,\n",
        "            'best validation accuracy': best_global_acc\n",
        "            }])\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "        df.to_csv('./our attack + 40 attacker + our defense.csv', index=False)\n",
        "\n",
        "        if val_loss > 1000:\n",
        "            print('val loss %f too high'%val_loss)\n",
        "            break\n",
        "\n",
        "        epoch_num+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the data\n",
        "for col in ['loss', 'validation accuracy', 'best validation accuracy']:\n",
        "    plt.plot(df['epoch'].tolist(), df[col].tolist())\n",
        "    plt.title(\"our attack + 10 attacker + our defense_\"+col)\n",
        "    plt.savefig('./our attack + 40 attacker + no defense ' + col + '.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-UtLLRGnrc9"
      },
      "source": [
        "## Execute no attack + our defense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Deb-3te5nrc9",
        "outputId": "97884493-b3bd-47c8-ccbc-3978e0eb968e"
      },
      "outputs": [],
      "source": [
        "batch_size=250\n",
        "resume=0\n",
        "\n",
        "schedule=[1000]\n",
        "\n",
        "gamma=.5\n",
        "opt = 'sgd'\n",
        "fed_lr=0.5\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "aggregation='bulyan'\n",
        "multi_k = False\n",
        "candidates = []\n",
        "\n",
        "at_type='our-agr'\n",
        "dev_type ='std'\n",
        "n_attackers=[0]\n",
        "\n",
        "arch='alexnet'\n",
        "chkpt='./'+aggregation\n",
        "\n",
        "past_mean = []\n",
        "past_abs_sum = []\n",
        "past_median = []\n",
        "past_lower_quartile = []\n",
        "past_upper_quartile = []\n",
        "past_std = []\n",
        "\n",
        "\n",
        "for n_attacker in n_attackers:\n",
        "    epoch_num = 0\n",
        "    best_global_acc = 0\n",
        "    best_global_te_acc = 0\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
        "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
        "\n",
        "    df = pd.DataFrame(columns = ['epoch', 'loss', 'validation accuracy', 'best validation accuracy'])\n",
        "\n",
        "    history = [[] for _ in range(n_users)]\n",
        "    while epoch_num <= nepochs:\n",
        "        user_grads=[]\n",
        "\n",
        "        for i in range(n_users): # 50\n",
        "            nbatches = user_tr_len[i]//batch_size\n",
        "\n",
        "            inputs = user_train_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "            targets = user_train_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "\n",
        "            targets = targets.type(torch.LongTensor)\n",
        "\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
        "\n",
        "            outputs = fed_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            fed_model.zero_grad ()\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            param_grad=[]\n",
        "            for param in fed_model.parameters():\n",
        "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
        "\n",
        "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
        "\n",
        "        if epoch_num in schedule:\n",
        "            for param_group in optimizer_fed.param_groups:\n",
        "                param_group['lr'] *= gamma\n",
        "                print('New learnin rate ', param_group['lr'])\n",
        "\n",
        "        if n_attacker > 0:\n",
        "            mal_updates = our_attack_mean(user_grads, n_attacker, arch, criterion, user_train_data_tensors, user_train_label_tensors)\n",
        "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0) # torch.Size([50, 2472266])\n",
        "        else:\n",
        "            malicious_grads = user_grads\n",
        "\n",
        "        if not (malicious_grads.shape[0]==50):\n",
        "            print(malicious_grads.shape)\n",
        "\n",
        "        updates_abs_mean = malicious_grads.abs().mean(dim=1, keepdim=True)\n",
        "        for i in range(n_users):\n",
        "            history[i].extend(updates_abs_mean[i])\n",
        "\n",
        "        agg_grads = our_mean_defense(malicious_grads, n_attacker, history)\n",
        "\n",
        "        del user_grads\n",
        "\n",
        "        start_idx=0\n",
        "\n",
        "        optimizer_fed.zero_grad()\n",
        "\n",
        "        model_grads=[]\n",
        "\n",
        "        for i, param in enumerate(fed_model.parameters()):\n",
        "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
        "            start_idx=start_idx+len(param.data.view(-1))\n",
        "            param_=param_.cuda()\n",
        "            model_grads.append(param_)\n",
        "\n",
        "        optimizer_fed.step(model_grads)\n",
        "\n",
        "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
        "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
        "\n",
        "        is_best = best_global_acc < val_acc\n",
        "\n",
        "        best_global_acc = max(best_global_acc, val_acc)\n",
        "\n",
        "        if is_best:\n",
        "            best_global_te_acc = te_acc\n",
        "\n",
        "\n",
        "        print('epoch: %d, %s: at %s n_at %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(epoch_num, aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc))\n",
        "        new_row = pd.DataFrame([{\n",
        "            'epoch': epoch_num,\n",
        "            'loss': val_loss,\n",
        "            'validation accuracy': val_acc,\n",
        "            'best validation accuracy': best_global_acc\n",
        "            }])\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "        df.to_csv('./no attack + our defense.csv', index=False)\n",
        "\n",
        "        if val_loss > 1000:\n",
        "            print('val loss %f too high'%val_loss)\n",
        "            break\n",
        "\n",
        "        epoch_num+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Bicp8Hsxnrc9",
        "outputId": "79ee7fb2-e831-4b5b-eb5c-f05bc7e67954"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the data\n",
        "for col in ['loss', 'validation accuracy', 'best validation accuracy']:\n",
        "    plt.plot(df['epoch'].tolist(), df[col].tolist())\n",
        "    plt.title(\"no attack + our defense \"+col)\n",
        "    plt.savefig('./no attack + cour defense ' + col + '.png')\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
