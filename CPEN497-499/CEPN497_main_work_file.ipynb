{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The notebook contains\n",
    "### Code for _Bulyan_ aggregation algorithm, *when gradient updates of benign clients are unknown to adversary*\n",
    "### Evaluation of all of the attacks (Fang, LIE, and our SOTA AGR-tailored and AGR-agnstic) on Bulyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse, os, sys, csv, shutil, time, random, operator, pickle, ast, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.multiprocessing as mp\n",
    "import math\n",
    "sys.path.insert(0,'./../utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from cifar10_normal_train import *\n",
    "from cifar10_util import *\n",
    "from adam import Adam\n",
    "from sgd import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get cifar10 data and split it in IID fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/mnt/nfs/work1/amir/vshejwalkar/cifar10_data/'\n",
    "# load the train dataset\n",
    "\n",
    "X={}\n",
    "Y=[]\n",
    "\n",
    "\n",
    "if os.path.isfile('./X.pkl') and os.path.isfile('./Y.pkl'):\n",
    "    X = pickle.load(open('./X.pkl','rb'))\n",
    "    Y = pickle.load(open('./Y.pkl','rb'))\n",
    "\n",
    "    val_data_tensor = pickle.load(open('./val_data_tensor.pkl','rb'))\n",
    "    val_label_tensor = pickle.load(open('./val_label_tensor.pkl','rb'))\n",
    "    te_data_tensor = pickle.load(open('./te_data_tensor.pkl','rb'))\n",
    "    te_label_tensor = pickle.load(open('./te_label_tensor.pkl','rb'))\n",
    "\n",
    "    total_tr_len = 50000\n",
    "\n",
    "else:\n",
    "    train_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "    cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=train_transform)\n",
    "\n",
    "    cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=train_transform)\n",
    "\n",
    "    total_tr_len = len(cifar10_train)\n",
    "    total_test_len = len(cifar10_test)\n",
    "\n",
    "\n",
    "    for i in range(len(cifar10_train)):\n",
    "        data = cifar10_train[i][0].numpy()\n",
    "        label = cifar10_train[i][1]\n",
    "        \n",
    "        if label not in Y:\n",
    "            Y.append(label)\n",
    "            X[label] = []\n",
    "\n",
    "        X[label].append(data)\n",
    "\n",
    "    for label in Y:\n",
    "        X[label] = np.array(X[label])\n",
    "\n",
    "    # testing and validation set-up\n",
    "    X_TV = []\n",
    "    Y_TV = []\n",
    "\n",
    "    for i in range(len(cifar10_test)): \n",
    "        X_TV.append(cifar10_train[i][0].numpy())\n",
    "        Y_TV.append(cifar10_train[i][1])\n",
    "\n",
    "    val_data = X_TV[0:5000]    \n",
    "    val_label = Y_TV[0:5000] \n",
    "\n",
    "    te_data = X_TV[5000:]    \n",
    "    te_label = Y_TV[5000:] \n",
    "\n",
    "\n",
    "    val_data_tensor=torch.from_numpy(np.array(val_data)).type(torch.FloatTensor)\n",
    "    val_label_tensor=torch.from_numpy(np.array(val_label)).type(torch.LongTensor)\n",
    "\n",
    "    te_data_tensor=torch.from_numpy(np.array(te_data)).type(torch.FloatTensor)\n",
    "    te_label_tensor=torch.from_numpy(np.array(te_label)).type(torch.LongTensor)\n",
    "\n",
    "        \n",
    "    pickle.dump(X,open('./X.pkl','wb'))\n",
    "    pickle.dump(Y,open('./Y.pkl','wb'))\n",
    "    pickle.dump(val_data_tensor,open('./val_data_tensor.pkl','wb'))\n",
    "    pickle.dump(val_label_tensor,open('./val_label_tensor.pkl','wb'))\n",
    "    pickle.dump(te_data_tensor,open('./te_data_tensor.pkl','wb'))\n",
    "    pickle.dump(te_label_tensor,open('./te_label_tensor.pkl','wb'))\n",
    "\n",
    "\n",
    "# for i in range(len(cifar10_test)):\n",
    "#     X.append(cifar10_test[i][0].numpy())\n",
    "#     Y.append(cifar10_test[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusers=50\n",
    "user_tr_len= total_tr_len // nusers\n",
    "\n",
    "val_len=5000\n",
    "te_len=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide cifar10 data among 50 clients in Non-IID fashion using Dirichlet distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user 0 tr len 1000\n",
      "user 1 tr len 1000\n",
      "user 2 tr len 1000\n",
      "user 3 tr len 1000\n",
      "user 4 tr len 1000\n",
      "user 5 tr len 1000\n",
      "user 6 tr len 1000\n",
      "user 7 tr len 1000\n",
      "user 8 tr len 1000\n",
      "user 9 tr len 1000\n",
      "user 10 tr len 1000\n",
      "user 11 tr len 1000\n",
      "user 12 tr len 1000\n",
      "user 13 tr len 1000\n",
      "user 14 tr len 1000\n",
      "user 15 tr len 1000\n",
      "user 16 tr len 1000\n",
      "user 17 tr len 1000\n",
      "user 18 tr len 1000\n",
      "user 19 tr len 1000\n",
      "user 20 tr len 1000\n",
      "user 21 tr len 1000\n",
      "user 22 tr len 1000\n",
      "user 23 tr len 1000\n",
      "user 24 tr len 1000\n",
      "user 25 tr len 1000\n",
      "user 26 tr len 1000\n",
      "user 27 tr len 1000\n",
      "user 28 tr len 1000\n",
      "user 29 tr len 1000\n",
      "user 30 tr len 1000\n",
      "user 31 tr len 1000\n",
      "user 32 tr len 1000\n",
      "user 33 tr len 1000\n",
      "user 34 tr len 1000\n",
      "user 35 tr len 1000\n",
      "user 36 tr len 1000\n",
      "user 37 tr len 1000\n",
      "user 38 tr len 1000\n",
      "user 39 tr len 1000\n",
      "user 40 tr len 1000\n",
      "user 41 tr len 1000\n",
      "user 42 tr len 1000\n",
      "user 43 tr len 1000\n",
      "user 44 tr len 1000\n",
      "user 45 tr len 1000\n",
      "user 46 tr len 1000\n",
      "user 47 tr len 1000\n",
      "user 48 tr len 1000\n",
      "user 49 tr len 1000\n"
     ]
    }
   ],
   "source": [
    "user_tr_data_tensors=[]\n",
    "user_tr_label_tensors=[]\n",
    "\n",
    "alpha = 20\n",
    "n_class = 10\n",
    "\n",
    "for i in range(nusers):\n",
    "    alpha_list = [alpha for _ in range(n_class)]\n",
    "    probs = np.random.dirichlet(alpha_list)\n",
    "    \n",
    "    user_tr_data_tensor=[]\n",
    "    user_tr_label_tensor=[]\n",
    "\n",
    "    for j in range(n_class):\n",
    "        n_pair = math.ceil(probs[j] * user_tr_len)\n",
    "\n",
    "        random_indices = np.random.choice(len(X[j]), n_pair, replace=False)\n",
    "        user_tr_data_tensor.extend(X[j][random_indices])\n",
    "        user_tr_label_tensor.extend([j] * n_pair)\n",
    "\n",
    "    user_tr_data_tensor = torch.from_numpy(np.array(user_tr_data_tensor[0:1000])).type(torch.FloatTensor)\n",
    "    user_tr_label_tensor = torch.from_numpy(np.array(user_tr_label_tensor[0:1000])).type(torch.FloatTensor)\n",
    "\n",
    "    user_tr_data_tensors.append(user_tr_data_tensor)\n",
    "    user_tr_label_tensors.append(user_tr_label_tensor)\n",
    "    print('user %d tr len %d'%(i,len(user_tr_data_tensor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Multi-krum aggregation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_krum(all_updates, n_attackers, multi_k=False):\n",
    "\n",
    "    candidates = []\n",
    "    candidate_indices = []\n",
    "    remaining_updates = all_updates\n",
    "    all_indices = np.arange(len(all_updates))\n",
    "\n",
    "    while len(remaining_updates) > 2 * n_attackers + 2:\n",
    "        torch.cuda.empty_cache()\n",
    "        distances = []\n",
    "        for update in remaining_updates:\n",
    "            distance = []\n",
    "            for update_ in remaining_updates:\n",
    "                distance.append(torch.norm((update - update_)) ** 2)\n",
    "            distance = torch.Tensor(distance).float()\n",
    "            distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "\n",
    "        distances = torch.sort(distances, dim=1)[0]\n",
    "        scores = torch.sum(distances[:, :len(remaining_updates) - 2 - n_attackers], dim=1)\n",
    "        indices = torch.argsort(scores)[:len(remaining_updates) - 2 - n_attackers]\n",
    "\n",
    "        candidate_indices.append(all_indices[indices[0].cpu().numpy()])\n",
    "        all_indices = np.delete(all_indices, indices[0].cpu().numpy())\n",
    "        candidates = remaining_updates[indices[0]][None, :] if not len(candidates) else torch.cat((candidates, remaining_updates[indices[0]][None, :]), 0)\n",
    "        remaining_updates = torch.cat((remaining_updates[:indices[0]], remaining_updates[indices[0] + 1:]), 0)\n",
    "        if not multi_k:\n",
    "            break\n",
    "    # print(len(remaining_updates))\n",
    "\n",
    "    aggregate = torch.mean(candidates, dim=0)\n",
    "\n",
    "    return aggregate, np.array(candidate_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Bulyan aggregation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulyan(all_updates, n_attackers):\n",
    "    nusers = all_updates.shape[0]\n",
    "    bulyan_cluster = []\n",
    "    candidate_indices = []\n",
    "    remaining_updates = all_updates\n",
    "    all_indices = np.arange(len(all_updates))\n",
    "\n",
    "    while len(bulyan_cluster) < (nusers - 2 * n_attackers):\n",
    "        torch.cuda.empty_cache()\n",
    "        distances = []\n",
    "        for update in remaining_updates:\n",
    "            distance = []\n",
    "            for update_ in remaining_updates:\n",
    "                distance.append(torch.norm((update - update_)) ** 2)\n",
    "            distance = torch.Tensor(distance).float()\n",
    "            distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "        # print(distances)\n",
    "\n",
    "        distances = torch.sort(distances, dim=1)[0]\n",
    "\n",
    "        scores = torch.sum(distances[:, :len(remaining_updates) - 2 - n_attackers], dim=1)\n",
    "        indices = torch.argsort(scores)[:len(remaining_updates) - 2 - n_attackers]\n",
    "        if not len(indices):\n",
    "            break\n",
    "        candidate_indices.append(all_indices[indices[0].cpu().numpy()])\n",
    "        all_indices = np.delete(all_indices, indices[0].cpu().numpy())\n",
    "        bulyan_cluster = remaining_updates[indices[0]][None, :] if not len(bulyan_cluster) else torch.cat((bulyan_cluster, remaining_updates[indices[0]][None, :]), 0)\n",
    "        remaining_updates = torch.cat((remaining_updates[:indices[0]], remaining_updates[indices[0] + 1:]), 0)\n",
    "\n",
    "    # print('dim of bulyan cluster ', bulyan_cluster.shape)\n",
    "\n",
    "    n, d = bulyan_cluster.shape\n",
    "    param_med = torch.median(bulyan_cluster, dim=0)[0]\n",
    "    sort_idx = torch.argsort(torch.abs(bulyan_cluster - param_med), dim=0)\n",
    "    sorted_params = bulyan_cluster[sort_idx, torch.arange(d)[None, :]]\n",
    "\n",
    "    return torch.mean(sorted_params[:n - 2 * n_attackers], dim=0), np.array(candidate_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for fang attack on Bulyan\n",
    "### Note that Fang attacks on Multi-krum and Bulyan are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lambda_fang(all_updates, model_re, n_attackers):\n",
    "\n",
    "    distances = []\n",
    "    n_benign, d = all_updates.shape\n",
    "    for update in all_updates:\n",
    "        distance = torch.norm((all_updates - update), dim=1)\n",
    "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "\n",
    "    distances[distances == 0] = 10000\n",
    "    distances = torch.sort(distances, dim=1)[0]\n",
    "    scores = torch.sum(distances[:, :n_benign - 2 - n_attackers], dim=1)\n",
    "    min_score = torch.min(scores)\n",
    "    term_1 = min_score / ((n_benign - n_attackers - 1) * torch.sqrt(torch.Tensor([d]))[0])\n",
    "    max_wre_dist = torch.max(torch.norm((all_updates - model_re), dim=1)) / (torch.sqrt(torch.Tensor([d]))[0])\n",
    "\n",
    "    return (term_1 + max_wre_dist)\n",
    "\n",
    "\n",
    "def get_malicious_updates_fang(all_updates, model_re, deviation, n_attackers):\n",
    "\n",
    "    lamda = compute_lambda_fang(all_updates, model_re, n_attackers)\n",
    "    \n",
    "    print(\"lamda: \", lamda.shape)\n",
    "    print(\"all_updates: \", all_updates.shape)\n",
    "\n",
    "    threshold = 1e-5\n",
    "\n",
    "    mal_updates = []\n",
    "    while lamda > threshold:\n",
    "        mal_update = (- lamda * deviation)\n",
    "        print(\"mal_update: \", mal_update.shape)\n",
    "\n",
    "        mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "        mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "        print(\"mal_updates: \", mal_updates.shape)\n",
    "\n",
    "        agg_grads, krum_candidate = multi_krum(mal_updates, n_attackers, multi_k=False)\n",
    "        \n",
    "        if krum_candidate < n_attackers:\n",
    "            return mal_update\n",
    "        \n",
    "        lamda *= 0.5\n",
    "\n",
    "    if not len(mal_updates):\n",
    "        print(lamda, threshold)\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "    \n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Fang attack on Bulyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hizan\\Desktop\\CPEN497-FL\\CPEN497-499\\sgd.py:109: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\python_arg_parser.cpp:1630.)\n",
      "  p.data.add_(-group['lr'], d_p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulyan: at fang n_at 10 n_mal_sel 10 e 0 val loss 2.3024 val acc 10.4505 best val_acc 10.450487 te_acc 9.760552\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "bulyan: at fang n_at 10 n_mal_sel 9 e 25 val loss 2.2634 val acc 15.6453 best val_acc 17.715097 te_acc 17.248377\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "bulyan: at fang n_at 10 n_mal_sel 5 e 49 val loss 2.2470 val acc 19.1153 best val_acc 23.538961 te_acc 22.564935\n",
      "lamda:  torch.Size([])\n",
      "all_updates:  torch.Size([10, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "mal_update:  torch.Size([2472266])\n",
      "mal_updates:  torch.Size([12, 2472266])\n",
      "torch.Size([2472266])\n",
      "torch.Size([10, 2472266])\n",
      "torch.Size([40, 2472266])\n",
      "bulyan: at fang n_at 10 n_mal_sel 3 e 50 val loss 2.3137 val acc 10.3896 best val_acc 23.538961 te_acc 22.564935\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "\n",
    "# num of nepochs to run\n",
    "nepochs=50\n",
    "\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='bulyan'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='fang'\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "# Remeber each gradient\n",
    "e_g = pd.DataFrame()\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(n_attacker, nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = targets.type(torch.LongTensor)\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            n_attacker_ = max(1, n_attacker**2//nusers)\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(user_grads[:n_attacker], z_values[n_attacker])\n",
    "            elif at_type == 'fang':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                mal_update = get_malicious_updates_fang(user_grads[:n_attacker], agg_grads, deviation, n_attacker_)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_median(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "            elif at_type == 'min-max':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_dist(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "            elif at_type == 'min-sum':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_score(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "\n",
    "            print(mal_update.shape) # torch.Size([2472266])\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker) \n",
    "            print(mal_updates.shape) #torch.Size([10, 2472266])\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0) \n",
    "            print(malicious_grads.shape) #torch.Size([40, 2472266])\n",
    "            exit()\n",
    "        \n",
    "        mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "        malicious_grads = torch.cat((mal_updates, user_grads), 0)\n",
    "        \n",
    "        if not (malicious_grads.shape[0]==50):\n",
    "            print(malicious_grads.shape)\n",
    "            sys.exit()\n",
    "            \n",
    "        agg_grads, krum_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%25==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d n_mal_sel %d e %d val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, np.sum(krum_candidate < n_attacker), epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for LIE attack, followed by its execution on Bulyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lie_attack(all_updates, z):\n",
    "    avg = torch.mean(all_updates, dim=0)\n",
    "    std = torch.std(all_updates, dim=0)\n",
    "    return avg + z * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='bulyan'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='lie'\n",
    "z_values={3:0.69847, 5:0.7054, 8:0.71904, 10:0.72575, 12:0.73891}\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(n_attacker, nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            n_attacker_ = max(1, n_attacker**2//nusers)\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(user_grads[:n_attacker], z_values[n_attacker])\n",
    "            \n",
    "        mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "        malicious_grads = torch.cat((mal_updates, user_grads), 0)\n",
    "\n",
    "        if not (malicious_grads.shape[0]==50):\n",
    "            print(malicious_grads.shape)\n",
    "            sys.exit()\n",
    "            \n",
    "        agg_grads, krum_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d n_mal_sel %d e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, np.sum(krum_candidate < n_attacker), epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        if val_loss > 10:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our AGR-tailored attack on Bulyan\n",
    "* Note that our attacks on multi-krum and Bulyan aggregations are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_attack_mkrum(all_updates, model_re, n_attackers, dev_type='unit_vec'):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "\n",
    "    lamda = torch.Tensor([3.0]).cuda()\n",
    "\n",
    "    threshold_diff = 1e-5\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "        mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "        agg_grads, krum_candidate = multi_krum(mal_updates, n_attackers, multi_k=True)\n",
    "        if np.sum(krum_candidate < n_attackers) == n_attackers:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute our AGR-tailored attack on Bulyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='bulyan'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='our-agr'\n",
    "dev_type ='std'\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            n_attacker_ = max(1, n_attacker**2//nusers)\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(user_grads[:n_attacker], z_values[n_attacker])\n",
    "            elif at_type == 'fang':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                mal_update = get_malicious_updates_fang(user_grads[:n_attacker], agg_grads, deviation, n_attacker_)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_mkrum(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "            elif at_type == 'min-max':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_dist(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "            elif at_type == 'min-sum':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_score(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0)\n",
    "            \n",
    "        if not (malicious_grads.shape[0]==50):\n",
    "            print(malicious_grads.shape)\n",
    "            sys.exit()\n",
    "        \n",
    "        agg_grads, krum_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%25==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d n_mal_sel %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(aggregation, at_type, n_attacker, np.sum(krum_candidate < n_attacker), epoch_num, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "        if val_loss > 1000:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "            \n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our first AGR-agnostic attack called Min-Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MIN-MAX attack\n",
    "'''\n",
    "def our_attack_dist(all_updates, model_re, n_attackers, dev_type='unit_vec'):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "\n",
    "    lamda = torch.Tensor([10.0]).float().cuda()\n",
    "    # print(lamda)\n",
    "    threshold_diff = 1e-5\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    \n",
    "    distances = []\n",
    "    for update in all_updates:\n",
    "        distance = torch.norm((all_updates - update), dim=1) ** 2\n",
    "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "    \n",
    "    max_distance = torch.max(distances)\n",
    "    del distances\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        distance = torch.norm((all_updates - mal_update), dim=1) ** 2\n",
    "        max_d = torch.max(distance)\n",
    "        \n",
    "        if max_d <= max_distance:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    \n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Min-max attack on Bulyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='bulyan'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='min-max'\n",
    "dev_type ='std'\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    candidates = []\n",
    "\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            n_attacker_ = max(1, n_attacker**2//nusers)\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(user_grads[:n_attacker], z_values[n_attacker])\n",
    "            elif at_type == 'fang':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                mal_update = get_malicious_updates_fang(user_grads[:n_attacker], agg_grads, deviation, n_attacker_)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_mkrum(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "            elif at_type == 'min-max':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_dist(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "            elif at_type == 'min-sum':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_score(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0)\n",
    "\n",
    "        if not (malicious_grads.shape[0]==50):\n",
    "            print(malicious_grads.shape)\n",
    "            sys.exit()\n",
    "        \n",
    "        agg_grads, krum_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%25==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d n_mal_sel %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(aggregation, at_type, n_attacker, np.sum(krum_candidate < n_attacker), epoch_num, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "        if val_loss > 1000:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "            \n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our second AGR-agnostic attack called Min-Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MIN-SUM attack\n",
    "'''\n",
    "\n",
    "def our_attack_score(all_updates, model_re, n_attackers, dev_type='unit_vec'):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "    \n",
    "    lamda = torch.Tensor([10.0]).float().cuda()\n",
    "    # print(lamda)\n",
    "    threshold_diff = 1e-5\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    \n",
    "    distances = []\n",
    "    for update in all_updates:\n",
    "        distance = torch.norm((all_updates - update), dim=1) ** 2\n",
    "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "    \n",
    "    scores = torch.sum(distances, dim=1)\n",
    "    min_score = torch.min(scores)\n",
    "    del distances\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        distance = torch.norm((all_updates - mal_update), dim=1) ** 2\n",
    "        score = torch.sum(distance)\n",
    "        \n",
    "        if score <= min_score:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "\n",
    "    # print(lamda_succ)\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    \n",
    "    return mal_update\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Min-Sum attack on Bulyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='bulyan'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='min-sum'\n",
    "dev_type ='std'\n",
    "z=0\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    candidates = []\n",
    "\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            n_attacker_ = max(1, n_attacker**2//nusers)\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(user_grads[:n_attacker], z_values[n_attacker])\n",
    "            elif at_type == 'fang':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                mal_update = get_malicious_updates_fang(user_grads[:n_attacker], agg_grads, deviation, n_attacker_)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_mkrum(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "            elif at_type == 'min-max':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_dist(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "            elif at_type == 'min-sum':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_score(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0)\n",
    "\n",
    "        if not (malicious_grads.shape[0]==50):\n",
    "            print(malicious_grads.shape)\n",
    "            sys.exit()\n",
    "        \n",
    "        agg_grads, krum_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%25==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d n_mal_sel %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(aggregation, at_type, n_attacker, np.sum(krum_candidate < n_attacker), epoch_num, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "        if val_loss > 1000:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "            \n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
