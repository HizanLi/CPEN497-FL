{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The notebook contains\n",
    "### Code for _Bulyan_ aggregation algorithm, *when gradient updates of benign clients are unknown to adversary*\n",
    "### Evaluation of all of the attacks (Fang, LIE, and our SOTA AGR-tailored and AGR-agnstic) on Bulyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML   \n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse, os, sys, csv, shutil, time, random, operator, pickle, ast, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.multiprocessing as mp\n",
    "import math\n",
    "sys.path.insert(0,'./../utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from cifar10_normal_train import *\n",
    "from cifar10_util import *\n",
    "from adam import Adam\n",
    "from sgd import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide cifar10 data among 50 clients in Non-IID fashion using Dirichlet distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.datasets as datasets\n",
    "# data_loc='/mnt/nfs/work1/amir/vshejwalkar/cifar10_data/'\n",
    "# # load the train dataset\n",
    "\n",
    "# X={}\n",
    "# Y=[]\n",
    "\n",
    "\n",
    "# if os.path.isfile('./X.pkl') and os.path.isfile('./Y.pkl'):\n",
    "#     X = pickle.load(open('./X.pkl','rb'))\n",
    "#     Y = pickle.load(open('./Y.pkl','rb'))\n",
    "\n",
    "#     val_data_tensor = pickle.load(open('./val_data_tensor.pkl','rb'))\n",
    "#     val_label_tensor = pickle.load(open('./val_label_tensor.pkl','rb'))\n",
    "#     te_data_tensor = pickle.load(open('./te_data_tensor.pkl','rb'))\n",
    "#     te_label_tensor = pickle.load(open('./te_label_tensor.pkl','rb'))\n",
    "\n",
    "#     total_tr_len = 50000\n",
    "\n",
    "# else:\n",
    "#     train_transform = transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "#         ])\n",
    "\n",
    "#     cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=train_transform)\n",
    "\n",
    "#     cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=train_transform)\n",
    "\n",
    "#     total_tr_len = len(cifar10_train)\n",
    "#     total_test_len = len(cifar10_test)\n",
    "\n",
    "\n",
    "#     for i in range(len(cifar10_train)):\n",
    "#         data = cifar10_train[i][0].numpy()\n",
    "#         label = cifar10_train[i][1]\n",
    "        \n",
    "#         if label not in Y:\n",
    "#             Y.append(label)\n",
    "#             X[label] = []\n",
    "\n",
    "#         X[label].append(data)\n",
    "\n",
    "#     for label in Y:\n",
    "#         X[label] = np.array(X[label])\n",
    "\n",
    "#     # testing and validation set-up\n",
    "#     X_TV = []\n",
    "#     Y_TV = []\n",
    "\n",
    "#     for i in range(len(cifar10_test)): \n",
    "#         X_TV.append(cifar10_train[i][0].numpy())\n",
    "#         Y_TV.append(cifar10_train[i][1])\n",
    "\n",
    "#     val_data = X_TV[0:5000]    \n",
    "#     val_label = Y_TV[0:5000] \n",
    "\n",
    "#     te_data = X_TV[5000:]    \n",
    "#     te_label = Y_TV[5000:] \n",
    "\n",
    "\n",
    "#     val_data_tensor=torch.from_numpy(np.array(val_data)).type(torch.FloatTensor)\n",
    "#     val_label_tensor=torch.from_numpy(np.array(val_label)).type(torch.LongTensor)\n",
    "\n",
    "#     te_data_tensor=torch.from_numpy(np.array(te_data)).type(torch.FloatTensor)\n",
    "#     te_label_tensor=torch.from_numpy(np.array(te_label)).type(torch.LongTensor)\n",
    "\n",
    "        \n",
    "#     pickle.dump(X,open('./X.pkl','wb'))\n",
    "#     pickle.dump(Y,open('./Y.pkl','wb'))\n",
    "#     pickle.dump(val_data_tensor,open('./val_data_tensor.pkl','wb'))\n",
    "#     pickle.dump(val_label_tensor,open('./val_label_tensor.pkl','wb'))\n",
    "#     pickle.dump(te_data_tensor,open('./te_data_tensor.pkl','wb'))\n",
    "#     pickle.dump(te_label_tensor,open('./te_label_tensor.pkl','wb'))\n",
    "\n",
    "\n",
    "# for i in range(len(cifar10_test)):\n",
    "#     X.append(cifar10_test[i][0].numpy())\n",
    "#     Y.append(cifar10_test[i][1])\n",
    "\n",
    "# user_tr_data_tensors=[]\n",
    "# user_tr_label_tensors=[]\n",
    "\n",
    "# alpha = 20\n",
    "# n_class = 10\n",
    "\n",
    "# nusers=50\n",
    "# user_tr_len= total_tr_len // nusers\n",
    "# val_len=5000\n",
    "# te_len=5000\n",
    "\n",
    "# for i in range(nusers):\n",
    "#     alpha_list = [alpha for _ in range(n_class)]\n",
    "#     probs = np.random.dirichlet(alpha_list)\n",
    "    \n",
    "#     user_tr_data_tensor=[]\n",
    "#     user_tr_label_tensor=[]\n",
    "\n",
    "#     for j in range(n_class):\n",
    "#         n_pair = math.ceil(probs[j] * user_tr_len)\n",
    "\n",
    "#         random_indices = np.random.choice(len(X[j]), n_pair, replace=False)\n",
    "#         user_tr_data_tensor.extend(X[j][random_indices])\n",
    "#         user_tr_label_tensor.extend([j] * n_pair)\n",
    "\n",
    "#     user_tr_data_tensor = torch.from_numpy(np.array(user_tr_data_tensor[0:1000])).type(torch.FloatTensor)\n",
    "#     user_tr_label_tensor = torch.from_numpy(np.array(user_tr_label_tensor[0:1000])).type(torch.FloatTensor)\n",
    "\n",
    "#     user_tr_data_tensors.append(user_tr_data_tensor)\n",
    "#     user_tr_label_tensors.append(user_tr_label_tensor)\n",
    "#     print('user %d tr len %d'%(i,len(user_tr_data_tensor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide cifar10 data among 50 clients in IID fashion using Dirichlet distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "total data len:  60000\n",
      "total data len:  60000\n",
      "total tr len 50000 | val len 5000 | test len 5000\n",
      "user 0 tr len 1000\n",
      "user 1 tr len 1000\n",
      "user 2 tr len 1000\n",
      "user 3 tr len 1000\n",
      "user 4 tr len 1000\n",
      "user 5 tr len 1000\n",
      "user 6 tr len 1000\n",
      "user 7 tr len 1000\n",
      "user 8 tr len 1000\n",
      "user 9 tr len 1000\n",
      "user 10 tr len 1000\n",
      "user 11 tr len 1000\n",
      "user 12 tr len 1000\n",
      "user 13 tr len 1000\n",
      "user 14 tr len 1000\n",
      "user 15 tr len 1000\n",
      "user 16 tr len 1000\n",
      "user 17 tr len 1000\n",
      "user 18 tr len 1000\n",
      "user 19 tr len 1000\n",
      "user 20 tr len 1000\n",
      "user 21 tr len 1000\n",
      "user 22 tr len 1000\n",
      "user 23 tr len 1000\n",
      "user 24 tr len 1000\n",
      "user 25 tr len 1000\n",
      "user 26 tr len 1000\n",
      "user 27 tr len 1000\n",
      "user 28 tr len 1000\n",
      "user 29 tr len 1000\n",
      "user 30 tr len 1000\n",
      "user 31 tr len 1000\n",
      "user 32 tr len 1000\n",
      "user 33 tr len 1000\n",
      "user 34 tr len 1000\n",
      "user 35 tr len 1000\n",
      "user 36 tr len 1000\n",
      "user 37 tr len 1000\n",
      "user 38 tr len 1000\n",
      "user 39 tr len 1000\n",
      "user 40 tr len 1000\n",
      "user 41 tr len 1000\n",
      "user 42 tr len 1000\n",
      "user 43 tr len 1000\n",
      "user 44 tr len 1000\n",
      "user 45 tr len 1000\n",
      "user 46 tr len 1000\n",
      "user 47 tr len 1000\n",
      "user 48 tr len 1000\n",
      "user 49 tr len 1000\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/mnt/nfs/work1/amir/vshejwalkar/cifar10_data/'\n",
    "# load the train dataset\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=train_transform)\n",
    "\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=train_transform)\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "for i in range(len(cifar10_train)):\n",
    "    X.append(cifar10_train[i][0].numpy())\n",
    "    Y.append(cifar10_train[i][1])\n",
    "\n",
    "for i in range(len(cifar10_test)):\n",
    "    X.append(cifar10_test[i][0].numpy())\n",
    "    Y.append(cifar10_test[i][1])\n",
    "\n",
    "X=np.array(X)\n",
    "Y=np.array(Y)\n",
    "\n",
    "print('total data len: ',len(X))\n",
    "\n",
    "\n",
    "all_indices = np.arange(len(X))\n",
    "np.random.shuffle(all_indices)\n",
    "pickle.dump(all_indices,open('./cifar10_shuffle.pkl','wb'))\n",
    "\n",
    "\n",
    "X=X[all_indices]\n",
    "Y=Y[all_indices]\n",
    "\n",
    "# data loading\n",
    "\n",
    "nusers=50\n",
    "user_tr_len=1000\n",
    "\n",
    "total_tr_len=user_tr_len*nusers\n",
    "val_len=5000\n",
    "te_len=5000\n",
    "\n",
    "print('total data len: ',len(X))\n",
    "\n",
    "if not os.path.isfile('./cifar10_shuffle.pkl'):\n",
    "    all_indices = np.arange(len(X))\n",
    "    np.random.shuffle(all_indices)\n",
    "    pickle.dump(all_indices,open('./cifar10_shuffle.pkl','wb'))\n",
    "else:\n",
    "    all_indices=pickle.load(open('./cifar10_shuffle.pkl','rb'))\n",
    "\n",
    "total_tr_data=X[:total_tr_len]\n",
    "total_tr_label=Y[:total_tr_len]\n",
    "\n",
    "val_data=X[total_tr_len:(total_tr_len+val_len)]\n",
    "val_label=Y[total_tr_len:(total_tr_len+val_len)]\n",
    "\n",
    "te_data=X[(total_tr_len+val_len):(total_tr_len+val_len+te_len)]\n",
    "te_label=Y[(total_tr_len+val_len):(total_tr_len+val_len+te_len)]\n",
    "\n",
    "total_tr_data_tensor=torch.from_numpy(total_tr_data).type(torch.FloatTensor)\n",
    "total_tr_label_tensor=torch.from_numpy(total_tr_label).type(torch.LongTensor)\n",
    "\n",
    "val_data_tensor=torch.from_numpy(val_data).type(torch.FloatTensor)\n",
    "val_label_tensor=torch.from_numpy(val_label).type(torch.LongTensor)\n",
    "\n",
    "te_data_tensor=torch.from_numpy(te_data).type(torch.FloatTensor)\n",
    "te_label_tensor=torch.from_numpy(te_label).type(torch.LongTensor)\n",
    "\n",
    "print('total tr len %d | val len %d | test len %d'%(len(total_tr_data_tensor),len(val_data_tensor),len(te_data_tensor)))\n",
    "\n",
    "#==============================================================================================================\n",
    "\n",
    "user_tr_data_tensors=[]\n",
    "user_tr_label_tensors=[]\n",
    "\n",
    "for i in range(nusers):\n",
    "    \n",
    "    user_tr_data_tensor=torch.from_numpy(total_tr_data[user_tr_len*i:user_tr_len*(i+1)]).type(torch.FloatTensor)\n",
    "    user_tr_label_tensor=torch.from_numpy(total_tr_label[user_tr_len*i:user_tr_len*(i+1)]).type(torch.LongTensor)\n",
    "\n",
    "    user_tr_data_tensors.append(user_tr_data_tensor)\n",
    "    user_tr_label_tensors.append(user_tr_label_tensor)\n",
    "    print('user %d tr len %d'%(i,len(user_tr_data_tensor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Multi-krum aggregation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_krum(all_updates, n_attackers, multi_k=False):\n",
    "    print(\"all_updates.shape: \", all_updates.shape)\n",
    "    candidates = []\n",
    "    candidate_indices = []\n",
    "    remaining_updates = all_updates\n",
    "    all_indices = np.arange(len(all_updates))\n",
    "\n",
    "    while len(remaining_updates) > 2 * n_attackers + 2:\n",
    "        torch.cuda.empty_cache()\n",
    "        distances = []\n",
    "        for update in remaining_updates:\n",
    "            distance = []\n",
    "            for update_ in remaining_updates:\n",
    "                distance.append(torch.norm((update - update_)) ** 2)\n",
    "            distance = torch.Tensor(distance).float()\n",
    "            distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "\n",
    "        distances = torch.sort(distances, dim=1)[0]\n",
    "        scores = torch.sum(distances[:, :len(remaining_updates) - 2 - n_attackers], dim=1)\n",
    "        indices = torch.argsort(scores)[:len(remaining_updates) - 2 - n_attackers]\n",
    "\n",
    "        candidate_indices.append(all_indices[indices[0].cpu().numpy()])\n",
    "        all_indices = np.delete(all_indices, indices[0].cpu().numpy())\n",
    "        candidates = remaining_updates[indices[0]][None, :] if not len(candidates) else torch.cat((candidates, remaining_updates[indices[0]][None, :]), 0)\n",
    "        remaining_updates = torch.cat((remaining_updates[:indices[0]], remaining_updates[indices[0] + 1:]), 0)\n",
    "        if not multi_k:\n",
    "            break\n",
    "    print(\"candidates\", len(candidates))\n",
    "\n",
    "    aggregate = torch.mean(candidates, dim=0)\n",
    "\n",
    "    return aggregate, np.array(candidate_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Bulyan aggregation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulyan(all_updates, n_attackers):\n",
    "    nusers = all_updates.shape[0]\n",
    "    bulyan_cluster = []\n",
    "    candidate_indices = []\n",
    "    remaining_updates = all_updates\n",
    "    all_indices = np.arange(len(all_updates))\n",
    "\n",
    "    while len(bulyan_cluster) < (nusers - 2 * n_attackers):\n",
    "        torch.cuda.empty_cache()\n",
    "        distances = []\n",
    "        for update in remaining_updates:\n",
    "            distance = []\n",
    "            for update_ in remaining_updates:\n",
    "                distance.append(torch.norm((update - update_)) ** 2)\n",
    "            distance = torch.Tensor(distance).float()\n",
    "            distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "        # print(distances)\n",
    "\n",
    "        distances = torch.sort(distances, dim=1)[0]\n",
    "\n",
    "        scores = torch.sum(distances[:, :len(remaining_updates) - 2 - n_attackers], dim=1)\n",
    "        indices = torch.argsort(scores)[:len(remaining_updates) - 2 - n_attackers]\n",
    "        if not len(indices):\n",
    "            break\n",
    "        candidate_indices.append(all_indices[indices[0].cpu().numpy()])\n",
    "        all_indices = np.delete(all_indices, indices[0].cpu().numpy())\n",
    "        bulyan_cluster = remaining_updates[indices[0]][None, :] if not len(bulyan_cluster) else torch.cat((bulyan_cluster, remaining_updates[indices[0]][None, :]), 0)\n",
    "        remaining_updates = torch.cat((remaining_updates[:indices[0]], remaining_updates[indices[0] + 1:]), 0)\n",
    "\n",
    "    # print('dim of bulyan cluster ', bulyan_cluster.shape)\n",
    "\n",
    "    n, d = bulyan_cluster.shape\n",
    "    param_med = torch.median(bulyan_cluster, dim=0)[0]\n",
    "    sort_idx = torch.argsort(torch.abs(bulyan_cluster - param_med), dim=0)\n",
    "    sorted_params = bulyan_cluster[sort_idx, torch.arange(d)[None, :]]\n",
    "\n",
    "    return torch.mean(sorted_params[:n - 2 * n_attackers], dim=0), np.array(candidate_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our AGR-tailored attack on Bulyan\n",
    "* Note that our attacks on multi-krum and Bulyan aggregations are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_attack_mkrum(all_updates, model_re, n_attackers, dev_type='unit_vec'):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "\n",
    "    lamda = torch.Tensor([3.0]).cuda()\n",
    "\n",
    "    threshold_diff = 1e-5\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "        print(\"mal_updates1: \",mal_updates.shape)\n",
    "        mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "        print(\"mal_updates12: \",mal_updates.shape)\n",
    "        agg_grads, krum_candidate = multi_krum(mal_updates, n_attackers, multi_k=True)\n",
    "        if np.sum(krum_candidate < n_attackers) == n_attackers:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    print(mal_update.shape)\n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## our_attack_mean_std\n",
    "$ argmin_\\theta(\\ -L(\\theta_{attacker(s)}) + \\lambda_1 |\\bar{\\theta_{abs\\ mean\\ benige\\ users}} - \\bar{\\theta_{abs\\ attacker(s)}}| + \\lambda_2 |\\bar{\\sigma_{mean\\ benige\\ users}} - \\bar{\\sigma_{attacker(s)}}|)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.optimize import minimize\n",
    "\n",
    "# # Define the loss function L\n",
    "def loss_function(theta, model, criterion, tr_data, tr_label):\n",
    "    \n",
    "\n",
    "    loss = criterion(model(tr_data), tr_label) + () + ()\n",
    "    return loss\n",
    "\n",
    "# # Define the function to calculate the absolute mean difference\n",
    "# def abs_mean_difference(theta, mean_benign):\n",
    "#     return np.abs(np.mean(theta) - mean_benign)\n",
    "\n",
    "# # Define the function to calculate the absolute standard deviation difference\n",
    "# def abs_std_difference(theta, std_benign):\n",
    "#     return np.abs(np.std(theta) - std_benign)\n",
    "\n",
    "# # Objective function to be minimized\n",
    "# def objective(theta, mean_benign, std_benign, model, criterion, tr_data, tr_label, lambda_1 = 0.1, lambda_2 = 0.1):\n",
    "#     return (loss_function(theta, model, criterion, tr_data, tr_label) +\n",
    "#             lambda_1 * abs_mean_difference(theta, mean_benign) +\n",
    "#             lambda_2 * abs_std_difference(theta, std_benign))\n",
    "\n",
    "\n",
    "    \n",
    "def our_attack_mean_std(all_updates, model_re, n_attacker, fed_model, criterion, user_tr_data_tensors, user_tr_label_tensors):\n",
    "    \n",
    "    all_updates_deviation = all_updates.std(dim=1, keepdim=True) # [50, 1]\n",
    "    benign_std_avg = all_updates_deviation.mean() # [1, 1]\n",
    "\n",
    "    all_updates_abs_mean = all_updates.abs().mean(dim=1, keepdim=True)  # [50,1]\n",
    "    benign_abs_mean_avg = all_updates_abs_mean.mean() # [1, 1]\n",
    "\n",
    "    result_attacker_grads = torch.empty(0,0)\n",
    "    attacker_grads = all_updates[:n_attacker]\n",
    "\n",
    "    for index, attacker_grad in enumerate(attacker_grads):\n",
    "\n",
    "        attacker_tr_data_tensor = user_tr_data_tensors[index].cuda()\n",
    "        attacker_tr_label_tensor = user_tr_label_tensors[index].cuda()\n",
    "\n",
    "        def rosen(X):\n",
    "            X = X.cuda()\n",
    "            attacker_abs_mean = X.abs().mean()\n",
    "            attacker_std = X.abs().std()\n",
    "            \n",
    "            print(criterion(fed_model(attacker_tr_data_tensor), attacker_tr_label_tensor))\n",
    "            return -criterion(fed_model(attacker_tr_data_tensor), attacker_tr_label_tensor) + torch.abs(benign_std_avg - attacker_abs_mean) + torch.abs(benign_abs_mean_avg - attacker_std)\n",
    "        \n",
    "        from torchmin import minimize\n",
    "        res = minimize(\n",
    "            rosen, attacker_grad, \n",
    "            method='l-bfgs', \n",
    "            options=dict(line_search='strong-wolfe'),\n",
    "            max_iter=50,\n",
    "            disp=2\n",
    "        )\n",
    "\n",
    "    print(result_attacker_grads.shape)\n",
    "    return result_attacker_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute our AGR-tailored attack on Bulyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_attacker <class 'int'>\n",
      "n_attacker_ <class 'int'>\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "initial fval: -2.3029\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   1 - fval: -2.3030\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   2 - fval: -2.3030\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -2.302974\n",
      "         Iterations: 2\n",
      "         Function evaluations: 20\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "initial fval: -2.3032\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   1 - fval: -2.3032\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   2 - fval: -2.3032\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   3 - fval: -2.3032\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   4 - fval: -2.3032\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   5 - fval: -2.3032\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   6 - fval: -2.3032\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -2.303247\n",
      "         Iterations: 6\n",
      "         Function evaluations: 21\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "initial fval: -2.3025\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   1 - fval: -2.3025\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   2 - fval: -2.3026\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   3 - fval: -2.3026\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   4 - fval: -2.3026\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   5 - fval: -2.3026\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -2.302591\n",
      "         Iterations: 5\n",
      "         Function evaluations: 36\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "initial fval: -2.3015\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   1 - fval: -2.3016\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   2 - fval: -2.3016\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   3 - fval: -2.3016\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   4 - fval: -2.3016\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -2.301555\n",
      "         Iterations: 4\n",
      "         Function evaluations: 31\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "initial fval: -2.3031\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   1 - fval: -2.3031\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   2 - fval: -2.3031\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   3 - fval: -2.3031\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   4 - fval: -2.3031\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   5 - fval: -2.3031\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   6 - fval: -2.3031\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   7 - fval: -2.3031\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -2.303141\n",
      "         Iterations: 7\n",
      "         Function evaluations: 28\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "initial fval: -2.3026\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   1 - fval: -2.3027\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   2 - fval: -2.3027\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   3 - fval: -2.3027\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   4 - fval: -2.3027\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -2.302694\n",
      "         Iterations: 4\n",
      "         Function evaluations: 29\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "initial fval: -2.3034\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   1 - fval: -2.3034\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   2 - fval: -2.3035\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   3 - fval: -2.3035\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   4 - fval: -2.3035\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -2.303458\n",
      "         Iterations: 4\n",
      "         Function evaluations: 29\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "initial fval: -2.3028\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   1 - fval: -2.3028\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   2 - fval: -2.3028\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   3 - fval: -2.3028\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -2.302833\n",
      "         Iterations: 3\n",
      "         Function evaluations: 20\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "initial fval: -2.3020\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   1 - fval: -2.3021\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   2 - fval: -2.3021\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   3 - fval: -2.3021\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   4 - fval: -2.3021\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   5 - fval: -2.3021\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   6 - fval: -2.3021\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -2.302057\n",
      "         Iterations: 6\n",
      "         Function evaluations: 41\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "initial fval: -2.3031\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   1 - fval: -2.3031\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   2 - fval: -2.3031\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   3 - fval: -2.3032\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   4 - fval: -2.3032\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iter   5 - fval: -2.3032\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -2.303159\n",
      "         Iterations: 5\n",
      "         Function evaluations: 30\n",
      "torch.Size([0, 0])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 92\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m###\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \n\u001b[0;32m     89\u001b[0m     \u001b[38;5;66;03m# # original\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# mal_update = our_attack_mkrum(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\u001b[39;00m\n\u001b[0;32m     91\u001b[0m     mal_updates \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([mal_update] \u001b[38;5;241m*\u001b[39m n_attacker) \u001b[38;5;66;03m# n_attacker, should be 10 in this case # torch.Size([10, 2472266])\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m     malicious_grads \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmal_updates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_grads\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn_attacker\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# torch.Size([50, 2472266])\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (malicious_grads\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28mprint\u001b[39m(malicious_grads\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "\n",
    "nepochs=10\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='bulyan'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='our-agr'\n",
    "dev_type ='std'\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "past_mean = []\n",
    "past_abs_sum = []\n",
    "past_median = []\n",
    "past_lower_quartile = []\n",
    "past_upper_quartile = []\n",
    "past_std = []\n",
    "\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(nusers): # 50\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = targets.type(torch.LongTensor)   \n",
    "            \n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "       \n",
    "        if n_attacker > 0:\n",
    "            print(\"n_attacker\", type(n_attacker))\n",
    "            n_attacker_ = max(1, n_attacker**2//nusers)\n",
    "            print(\"n_attacker_\", type(n_attacker_))\n",
    "            \n",
    "            agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "\n",
    "            ###\n",
    "            mal_update = our_attack_mean_std(user_grads, agg_grads, n_attacker, fed_model, criterion, user_tr_data_tensors, user_tr_label_tensors)\n",
    "\n",
    "            ###\n",
    "\n",
    "            # # original\n",
    "            # mal_update = our_attack_mkrum(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker) # n_attacker, should be 10 in this case # torch.Size([10, 2472266])\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0) # torch.Size([50, 2472266])\n",
    "            \n",
    "        if not (malicious_grads.shape[0]==50):\n",
    "            print(malicious_grads.shape)\n",
    "            sys.exit()\n",
    "        \n",
    "        # Store history of malicious_grads\n",
    "        average_tensor = malicious_grads.mean(dim=1, keepdim=True)\n",
    "        median_tensor = malicious_grads.median(dim=1, keepdim=True).values\n",
    "        lower_quartile = malicious_grads.quantile(0.25, dim=1, keepdim=True)\n",
    "        upper_quartile = malicious_grads.quantile(0.75, dim=1, keepdim=True)\n",
    "        std = malicious_grads.std(dim=1, keepdim=True)\n",
    "\n",
    "        mean_abs_tensor = malicious_grads.abs().sum(dim=1, keepdim=True)\n",
    "\n",
    "        past_abs_sum.append(mean_abs_tensor)\n",
    "        past_mean.append(average_tensor)\n",
    "        past_median.append(median_tensor)\n",
    "        past_lower_quartile.append(lower_quartile)\n",
    "        past_upper_quartile.append(upper_quartile)\n",
    "        past_std.append(std)\n",
    "\n",
    "\n",
    "        pickle.dump(past_abs_sum,open('./past_abs_sum.pkl','wb'))\n",
    "        pickle.dump(past_mean,open('./past_mean.pkl','wb'))\n",
    "        pickle.dump(past_median,open('./past_median.pkl','wb'))\n",
    "        pickle.dump(past_lower_quartile,open('./past_lower_quartile.pkl','wb'))\n",
    "        pickle.dump(past_upper_quartile,open('./past_upper_quartile.pkl','wb'))\n",
    "        pickle.dump(past_std,open('./past_std.pkl','wb'))\n",
    "\n",
    "        agg_grads, krum_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        \n",
    "        print('epoch: %d, %s: at %s n_at %d n_mal_sel %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(epoch_num, aggregation, at_type, n_attacker, np.sum(krum_candidate < n_attacker), epoch_num, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "        if val_loss > 1000:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "            \n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# past_mean = []\n",
    "# past_abs_sum = []\n",
    "# past_median = []\n",
    "# past_lower_quartile = []\n",
    "# past_upper_quartile = []\n",
    "# past_std = []\n",
    "\n",
    "print(past_abs_sum[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_median[3].shape, past_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
