{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse, os, sys, csv, shutil, time, random, operator, pickle, ast, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.multiprocessing as mp\n",
    "import math\n",
    "sys.path.insert(0,'./../utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from cifar10_normal_train import *\n",
    "from cifar10_util import *\n",
    "from adam import Adam\n",
    "from sgd import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/mnt/nfs/work1/amir/vshejwalkar/cifar10_data/'\n",
    "# load the train dataset\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=train_transform)\n",
    "\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=train_transform)\n",
    "\n",
    "total_tr_len = len(cifar10_train)\n",
    "\n",
    "X={}\n",
    "Y=[]\n",
    "for i in range(len(cifar10_train)):\n",
    "    data = cifar10_train[i][0].numpy()\n",
    "    label = cifar10_train[i][1]\n",
    "\n",
    "    if label in X:\n",
    "        X[label].append(data)\n",
    "    else:\n",
    "        X[label] = []\n",
    "        X[label].append(data)\n",
    "        Y.append(label)\n",
    "\n",
    "# for i in range(len(cifar10_test)):\n",
    "#     data = cifar10_test[i][0].numpy()\n",
    "#     label = cifar10_test[i][1]\n",
    "\n",
    "#     if label in X:\n",
    "#         X[label].append(data)\n",
    "#     else:\n",
    "#         X[label] = []\n",
    "#         X[label].append(data)\n",
    "#         Y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user 0 has (617) train data, and (617) train labels\n",
      "user 1 has (1067) train data, and (1067) train labels\n",
      "user 2 has (1393) train data, and (1393) train labels\n",
      "user 3 has (778) train data, and (778) train labels\n",
      "user 4 has (864) train data, and (864) train labels\n",
      "user 5 has (976) train data, and (976) train labels\n",
      "user 6 has (991) train data, and (991) train labels\n",
      "user 7 has (869) train data, and (869) train labels\n",
      "user 8 has (1219) train data, and (1219) train labels\n",
      "user 9 has (494) train data, and (494) train labels\n",
      "user 10 has (1949) train data, and (1949) train labels\n",
      "user 11 has (612) train data, and (612) train labels\n",
      "user 12 has (832) train data, and (832) train labels\n",
      "user 13 has (1825) train data, and (1825) train labels\n",
      "user 14 has (931) train data, and (931) train labels\n",
      "user 15 has (753) train data, and (753) train labels\n",
      "user 16 has (659) train data, and (659) train labels\n",
      "user 17 has (890) train data, and (890) train labels\n",
      "user 18 has (1375) train data, and (1375) train labels\n",
      "user 19 has (1153) train data, and (1153) train labels\n",
      "user 20 has (1244) train data, and (1244) train labels\n",
      "user 21 has (882) train data, and (882) train labels\n",
      "user 22 has (913) train data, and (913) train labels\n",
      "user 23 has (1056) train data, and (1056) train labels\n",
      "user 24 has (1461) train data, and (1461) train labels\n",
      "user 25 has (632) train data, and (632) train labels\n",
      "user 26 has (977) train data, and (977) train labels\n",
      "user 27 has (1028) train data, and (1028) train labels\n",
      "user 28 has (1492) train data, and (1492) train labels\n",
      "user 29 has (712) train data, and (712) train labels\n",
      "user 30 has (737) train data, and (737) train labels\n",
      "user 31 has (1177) train data, and (1177) train labels\n",
      "user 32 has (818) train data, and (818) train labels\n",
      "user 33 has (894) train data, and (894) train labels\n",
      "user 34 has (893) train data, and (893) train labels\n",
      "user 35 has (1036) train data, and (1036) train labels\n",
      "user 36 has (659) train data, and (659) train labels\n",
      "user 37 has (1063) train data, and (1063) train labels\n",
      "user 38 has (585) train data, and (585) train labels\n",
      "user 39 has (762) train data, and (762) train labels\n",
      "user 40 has (2036) train data, and (2036) train labels\n",
      "user 41 has (719) train data, and (719) train labels\n",
      "user 42 has (968) train data, and (968) train labels\n",
      "user 43 has (769) train data, and (769) train labels\n",
      "user 44 has (1116) train data, and (1116) train labels\n",
      "user 45 has (799) train data, and (799) train labels\n",
      "user 46 has (1069) train data, and (1069) train labels\n",
      "user 47 has (1375) train data, and (1375) train labels\n",
      "user 48 has (1000) train data, and (1000) train labels\n",
      "user 49 has (881) train data, and (881) train labels\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "alpha = 1\n",
    "n_users = 50\n",
    "\n",
    "non_tensor_user_train_data_tensors = [[] for _ in range(n_users)]\n",
    "non_tensor_user_train_label_tensors = [[] for _ in range(n_users)]\n",
    "\n",
    "for label in Y:\n",
    "    alpha_list = [alpha for _ in range(n_users)]\n",
    "    probs = np.random.dirichlet(alpha_list)\n",
    "\n",
    "    taken_index = 0\n",
    "\n",
    "    for i, prob in enumerate(probs):\n",
    "        if i == n_users - 1:\n",
    "            non_tensor_user_train_data_tensors[i].extend(X[label][taken_index:])\n",
    "            non_tensor_user_train_label_tensors[i].extend([label for _ in range(len(X[label]) - taken_index)])\n",
    "        else:\n",
    "            n_sample = math.floor(prob * len(X[label]))\n",
    "\n",
    "            non_tensor_user_train_data_tensors[i].extend(X[label][taken_index : taken_index + n_sample])\n",
    "            non_tensor_user_train_label_tensors[i].extend([label for _ in range(n_sample)])\n",
    "            taken_index += n_sample\n",
    "\n",
    "user_train_data_tensors = []\n",
    "user_train_label_tensors = []\n",
    "\n",
    "for i in range(n_users):\n",
    "    print(f'user {i} has ({len(non_tensor_user_train_data_tensors[i])}) train data, and ({len(non_tensor_user_train_label_tensors[i])}) train labels')\n",
    "\n",
    "    user_train_data_tensors.append(torch.from_numpy(np.array(non_tensor_user_train_data_tensors[i])))\n",
    "    user_train_label_tensors.append(torch.from_numpy(np.array(non_tensor_user_train_label_tensors[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_label_with_most_sample(labels:list, taken_index:list, initial_num_data_per_label:dict, data:dict):\n",
    "    max_fraction = -9.99\n",
    "    result_label = None\n",
    "\n",
    "    for label in labels:\n",
    "        \n",
    "        if label in taken_index: \n",
    "            continue\n",
    "        \n",
    "        full = initial_num_data_per_label[label]\n",
    "        current = len(data[label])\n",
    "\n",
    "        if full == current:\n",
    "            return label\n",
    "        else:\n",
    "            unused_fraction = current / full \n",
    "            if unused_fraction > max_fraction:\n",
    "                max_fraction = unused_fraction\n",
    "                result_label = label\n",
    "\n",
    "    return result_label\n",
    "\n",
    "\n",
    "def split_data_dirichelt(num_user:int, num_class:int, num_data_per_user:int, alpha:int, data:dict):\n",
    "    \n",
    "    initial_num_data_per_label = {}\n",
    "    labels = []\n",
    "\n",
    "    for label in data:\n",
    "        initial_num_data_per_label[label] = len(data[label])\n",
    "        labels.append(label)\n",
    "\n",
    "    print(initial_num_data_per_label)\n",
    "\n",
    "    for i in range(num_user):\n",
    "        alpha_list = [alpha for _ in range(num_class)]\n",
    "        probs = np.random.dirichlet(alpha_list)\n",
    "\n",
    "        user_train_data_tensor=[]\n",
    "        user_train_label_tensor=[]\n",
    "\n",
    "        taken_index = []\n",
    "\n",
    "        for prob in probs:\n",
    "            n_sample = math.floor(prob * num_data_per_user[i])\n",
    "\n",
    "            # print(\"n_sample: \", n_sample)\n",
    "            result_label = get_label_with_most_sample(labels, taken_index, initial_num_data_per_label, data)\n",
    "\n",
    "            taken_index.append(result_label)\n",
    "            # print(f\"before label {result_label} in data has: \", len(data[result_label]))\n",
    "\n",
    "            if n_sample > len(data[result_label]):\n",
    "                sys.exit(f\"not enough data for user {num_user}, running out of data with label {label}\")\n",
    "\n",
    "            user_train_data_tensor.extend(data[result_label][:n_sample])\n",
    "            user_train_label_tensor.extend([result_label] * n_sample)\n",
    "            \n",
    "            data[result_label]= data[result_label][n_sample:]\n",
    "\n",
    "\n",
    "            # print(f\"after label {result_label} in data has: \", len(data[result_label]))\n",
    "\n",
    "        print(f\"user {i} has {len(user_train_data_tensor)} train data\")\n",
    "    \n",
    "    for label in data:\n",
    "        print(label, len(data[label]))\n",
    "        \n",
    "\n",
    "user_train_len= total_tr_len // n_users\n",
    "split_data_dirichelt(n_users, len(Y), [user_train_len - 100] * n_users, 1, X.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpen333",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
