{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMUR8ODgnrc3"
      },
      "source": [
        "# The notebook contains\n",
        "### Code for _Bulyan_ aggregation algorithm, *when gradient updates of benign clients are unknown to adversary*\n",
        "### Evaluation of all of the attacks (Fang, LIE, and our SOTA AGR-tailored and AGR-agnstic) on Bulyan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJu2Edmbnrc5"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wGSIzpf_nrc5",
        "outputId": "a3df9b0e-5fdc-45ae-9113-c850f3a453af"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:90% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AAU3fosynrc6"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import argparse, os, sys, csv, shutil, time, random, operator, pickle, ast, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.optim import Optimizer\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.multiprocessing as mp\n",
        "import math\n",
        "sys.path.insert(0,'./../utils/')\n",
        "from logger import *\n",
        "from eval import *\n",
        "from misc import *\n",
        "\n",
        "from cifar10_normal_train import *\n",
        "from cifar10_util import *\n",
        "from adam import Adam\n",
        "from sgd import SGD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGABIvW6nrc6"
      },
      "source": [
        "## Data split\n",
        "Divide cifar10 data among 50 clients in Non-IID fashion using Dirichlet distribution\n",
        "\n",
        "Graph the distribution of classes for each user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1Nyn5Xlnrc6",
        "outputId": "6c1ca582-c005-4dbc-9b7c-ef3b4684f4be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /mnt/nfs/work1/amir/vshejwalkar/cifar10_data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 40456831.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /mnt/nfs/work1/amir/vshejwalkar/cifar10_data/cifar-10-python.tar.gz to /mnt/nfs/work1/amir/vshejwalkar/cifar10_data/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "data_loc='/mnt/nfs/work1/amir/vshejwalkar/cifar10_data/'\n",
        "# load the train dataset\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=train_transform)\n",
        "\n",
        "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=train_transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RrUvgYfNnrc7"
      },
      "outputs": [],
      "source": [
        "total_tr_len = len(cifar10_train)\n",
        "\n",
        "X={}\n",
        "Y=[]\n",
        "for i in range(len(cifar10_train)):\n",
        "    data = cifar10_train[i][0].numpy()\n",
        "    label = cifar10_train[i][1]\n",
        "\n",
        "    if label in X:\n",
        "        X[label].append(data)\n",
        "    else:\n",
        "        X[label] = []\n",
        "        X[label].append(data)\n",
        "        Y.append(label)\n",
        "\n",
        "for label in X:\n",
        "    X[label] = np.array(X[label])\n",
        "Y=np.array(Y)\n",
        "\n",
        "alpha = 1\n",
        "n_users = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6osiZzKOnrc7",
        "outputId": "08a53d7c-3194-4f53-aef2-1fdb0d37ac34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user 0 has 824 train data, and 824 train labels\n",
            "user 1 has 474 train data, and 474 train labels\n",
            "user 2 has 796 train data, and 796 train labels\n",
            "user 3 has 669 train data, and 669 train labels\n",
            "user 4 has 557 train data, and 557 train labels\n",
            "user 5 has 681 train data, and 681 train labels\n",
            "user 6 has 825 train data, and 825 train labels\n",
            "user 7 has 774 train data, and 774 train labels\n",
            "user 8 has 637 train data, and 637 train labels\n",
            "user 9 has 1003 train data, and 1003 train labels\n",
            "user 10 has 894 train data, and 894 train labels\n",
            "user 11 has 772 train data, and 772 train labels\n",
            "user 12 has 1636 train data, and 1636 train labels\n",
            "user 13 has 1069 train data, and 1069 train labels\n",
            "user 14 has 754 train data, and 754 train labels\n",
            "user 15 has 982 train data, and 982 train labels\n",
            "user 16 has 1002 train data, and 1002 train labels\n",
            "user 17 has 1288 train data, and 1288 train labels\n",
            "user 18 has 617 train data, and 617 train labels\n",
            "user 19 has 1397 train data, and 1397 train labels\n",
            "user 20 has 695 train data, and 695 train labels\n",
            "user 21 has 777 train data, and 777 train labels\n",
            "user 22 has 1129 train data, and 1129 train labels\n",
            "user 23 has 1366 train data, and 1366 train labels\n",
            "user 24 has 690 train data, and 690 train labels\n",
            "user 25 has 683 train data, and 683 train labels\n",
            "user 26 has 1170 train data, and 1170 train labels\n",
            "user 27 has 973 train data, and 973 train labels\n",
            "user 28 has 733 train data, and 733 train labels\n",
            "user 29 has 820 train data, and 820 train labels\n",
            "user 30 has 665 train data, and 665 train labels\n",
            "user 31 has 766 train data, and 766 train labels\n",
            "user 32 has 631 train data, and 631 train labels\n",
            "user 33 has 699 train data, and 699 train labels\n",
            "user 34 has 899 train data, and 899 train labels\n",
            "user 35 has 1177 train data, and 1177 train labels\n",
            "user 36 has 1366 train data, and 1366 train labels\n",
            "user 37 has 2200 train data, and 2200 train labels\n",
            "user 38 has 1353 train data, and 1353 train labels\n",
            "user 39 has 1515 train data, and 1515 train labels\n",
            "user 40 has 1541 train data, and 1541 train labels\n",
            "user 41 has 955 train data, and 955 train labels\n",
            "user 42 has 944 train data, and 944 train labels\n",
            "user 43 has 773 train data, and 773 train labels\n",
            "user 44 has 1059 train data, and 1059 train labels\n",
            "user 45 has 1799 train data, and 1799 train labels\n",
            "user 46 has 1070 train data, and 1070 train labels\n",
            "user 47 has 1065 train data, and 1065 train labels\n",
            "user 48 has 1774 train data, and 1774 train labels\n",
            "user 49 has 1062 train data, and 1062 train labels\n"
          ]
        }
      ],
      "source": [
        "user_train_data_non_tensors = [[] for _ in range(n_users)]\n",
        "user_train_label_non_tensors = [[] for _ in range(n_users)]\n",
        "\n",
        "for label in Y:\n",
        "    alpha_list = [alpha for _ in range(n_users)]\n",
        "    probs = np.random.dirichlet(alpha_list)\n",
        "\n",
        "    taken_index = 0\n",
        "\n",
        "    for i, prob in enumerate(probs):\n",
        "        if i == n_users - 1:\n",
        "            user_train_data_non_tensors[i].extend(X[label][taken_index:])\n",
        "            user_train_label_non_tensors[i].extend([label for _ in range(len(X[label]) - taken_index)])\n",
        "        else:\n",
        "            n_sample = math.floor(prob * len(X[label]))\n",
        "\n",
        "            user_train_data_non_tensors[i].extend(X[label][taken_index : taken_index + n_sample])\n",
        "            user_train_label_non_tensors[i].extend([label for _ in range(n_sample)])\n",
        "            taken_index += n_sample\n",
        "\n",
        "user_train_data_tensors = []\n",
        "user_train_label_tensors = []\n",
        "\n",
        "user_tr_len = []\n",
        "\n",
        "for i in range(n_users):\n",
        "\n",
        "    if len(user_train_data_non_tensors[i]) != len(user_train_label_non_tensors[i]):\n",
        "        sys.exit(f\"Shape does not match user_train_data_non_tensors[i] hsa {len(user_train_data_non_tensors[i])}, while user_train_label_non_tensors[i] has {len(user_train_label_non_tensors[i])}\")\n",
        "\n",
        "\n",
        "    num_data = len(user_train_data_non_tensors[i])\n",
        "    user_tr_len.append(num_data)\n",
        "\n",
        "    user_train_data_tensors.append(torch.from_numpy(np.array(user_train_data_non_tensors[i])).type(torch.FloatTensor))\n",
        "    user_train_label_tensors.append(torch.from_numpy(np.array(user_train_label_non_tensors[i])).type(torch.LongTensor))\n",
        "\n",
        "    r=np.arange(num_data)\n",
        "\n",
        "    np.random.shuffle(r)\n",
        "\n",
        "    user_train_data_tensors[i] = user_train_data_tensors[i][r]\n",
        "    user_train_label_tensors[i] = user_train_label_tensors[i][r]\n",
        "\n",
        "    print(f'user {i} has {user_train_data_tensors[i].shape[0]} train data, and {user_train_label_tensors[i].shape[0]} train labels')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HSewQHUZnrc7"
      },
      "outputs": [],
      "source": [
        "X2 = []\n",
        "Y2 = []\n",
        "\n",
        "for i in range(len(cifar10_test)):\n",
        "    X2.append(cifar10_test[i][0].numpy())\n",
        "    Y2.append(cifar10_test[i][1])\n",
        "\n",
        "X2=np.array(X2)\n",
        "Y2=np.array(Y2)\n",
        "\n",
        "half_index = len(X2) // 2\n",
        "\n",
        "val_data= X2[:half_index]\n",
        "val_label= Y2[:half_index]\n",
        "\n",
        "te_data=X2[half_index:]\n",
        "te_label=Y2[half_index:]\n",
        "\n",
        "val_data_tensor=torch.from_numpy(val_data).type(torch.FloatTensor)\n",
        "val_label_tensor=torch.from_numpy(val_label).type(torch.LongTensor)\n",
        "\n",
        "te_data_tensor=torch.from_numpy(te_data).type(torch.FloatTensor)\n",
        "te_label_tensor=torch.from_numpy(te_label).type(torch.LongTensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8vVcXSRnrc7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Example data\n",
        "users = [f\"user {i}\" for i in range(50)]\n",
        "labels = [f\"label {i}\" for i in range(10)]\n",
        "\n",
        "\n",
        "percentages = []\n",
        "for user_data in user_train_label_non_tensors:\n",
        "    temp_holder = []\n",
        "    for label in range(10):\n",
        "        temp_holder.append(user_data.count(label))\n",
        "    percentages.append(temp_holder)\n",
        "\n",
        "percentages = np.array(percentages)\n",
        "\n",
        "# Assuming 'percentages' is populated correctly as shown previously\n",
        "# Convert percentages to cumulative sum for stacking\n",
        "cumulative = np.cumsum(percentages, axis=1)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Colors for each ethnicity, ensuring there are enough colors for all labels\n",
        "colors = plt.cm.Paired(range(len(labels))) # Repeating colors to match the number of labels\n",
        "\n",
        "# Create stacked bars\n",
        "for i in range(len(labels)):  # Iterate over the number of labels\n",
        "    if i == 0:\n",
        "        ax.bar(users, percentages[:, i], color=colors[i], label=labels[i])\n",
        "    else:\n",
        "        ax.bar(users, percentages[:, i], bottom=cumulative[:, i-1], color=colors[i], label=labels[i])\n",
        "\n",
        "# Add some text for labels, title, and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('Percentage')\n",
        "ax.set_title('Stacked Percentage Bar Chart by User and Label')\n",
        "ax.set_xticks(np.arange(len(users)))\n",
        "ax.set_xticklabels(users, rotation=90)  # Rotate labels if needed\n",
        "ax.legend()\n",
        "\n",
        "\n",
        "plt.savefig('./non_iid_user_label_distribution.png', bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOVMqc_9nrc8"
      },
      "source": [
        "## Our Aggregation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LMgKiRnynrc8"
      },
      "outputs": [],
      "source": [
        "def our_mean_defense(all_updates, n_attackers, history_updates):\n",
        "    discarded_history = get_discarded_index(n_attackers, history_updates)\n",
        "\n",
        "    mask = torch.ones(all_updates.size(0), dtype=torch.bool)\n",
        "    mask[discarded_history] = False\n",
        "    remaining_updates = all_updates[mask]\n",
        "\n",
        "    print('discarded index', discarded_history)\n",
        "\n",
        "    return torch.nanmean(remaining_updates, dim=0)\n",
        "\n",
        "\n",
        "def euclidean_distance(row1, row2):\n",
        "    return math.sqrt(sum((x - y) ** 2 for x, y in zip(row1, row2)))\n",
        "\n",
        "def get_discarded_index(n_attackers, history_updates):\n",
        "    n_users = len(history_updates)\n",
        "    distance_matrix = [[0 if i == j else euclidean_distance(history_updates[i], history_updates[j])\n",
        "                        for j in range(n_users)] for i in range(n_users)]\n",
        "\n",
        "    sum_of_distances = [sum(row) for row in distance_matrix]\n",
        "\n",
        "    discarded_history = []\n",
        "    for _ in range(n_attackers):\n",
        "        max_distance = max(sum_of_distances)\n",
        "        index_of_max_distance = sum_of_distances.index(max_distance)\n",
        "        discarded_history.append(index_of_max_distance)\n",
        "\n",
        "        for i in range(n_users):\n",
        "            sum_of_distances[i] -= distance_matrix[i][index_of_max_distance]\n",
        "        sum_of_distances[index_of_max_distance] = -1\n",
        "\n",
        "    return discarded_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yiHdOSknrc8"
      },
      "source": [
        "## Code for no defense aggregation algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bDqjWvS-nrc8"
      },
      "outputs": [],
      "source": [
        "def no_defense_aggregation(user_grads):\n",
        "    return torch.nanmean(user_grads, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh9-2QXLnrc8"
      },
      "source": [
        "## our_attack_mean\n",
        "$ argmin_\\theta(\\ -L(\\theta_{attacker(s)}) + \\lambda_1 |\\bar{\\theta_{abs\\ mean\\ benige\\ users}} - \\bar{\\theta_{abs\\ attacker(s)}}|$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_condition(lambda1, prev_grad, loss_grad, history, all_updates, n_users, n_attacker):\n",
        "    new_grad = lambda1 * (loss_grad + prev_grad)\n",
        "    history_updates = history\n",
        "    for i in range(n_attacker):\n",
        "        history[i].append(new_grad.abs().mean().cpu())\n",
        "\n",
        "    discarded_history = get_discarded_index(n_attacker, history_updates)\n",
        "    detected_mal = len([item for item in discarded_history if item < 10])\n",
        "    return detected_mal / len(discarded_history) <= n_attacker / len(history_updates)\n",
        "\n",
        "def find_upper_bound(lambda1, prev_grad, loss_grad, history, all_updates, n_users, n_attacker):\n",
        "    while check_condition(lambda1, prev_grad, loss_grad, history, all_updates, n_users, n_attacker):\n",
        "        lambda1 *= 2  # Exponentially increase lambda\n",
        "    return lambda1\n",
        "\n",
        "\n",
        "def projection(prev_grad, loss_grad, history_updates, all_updates, n_users, n_attacker):\n",
        "    updates_abs_mean = all_updates.abs().mean(dim=1, keepdim=True)\n",
        "    history = history_updates\n",
        "    for i in range(n_attacker, n_users):\n",
        "        history[i].append(updates_abs_mean[i].cpu())\n",
        "    lambda_low = 0\n",
        "    lambda_high = find_upper_bound(1, prev_grad, loss_grad, history, all_updates, n_users, n_attacker)\n",
        "    tolerance = 1e-6\n",
        "\n",
        "    while lambda_high - lambda_low > tolerance:\n",
        "        lambda_mid = (lambda_low + lambda_high) / 2\n",
        "        if check_condition(lambda_mid, prev_grad, loss_grad, history, all_updates, n_users, n_attacker):\n",
        "            lambda_low = lambda_mid + tolerance\n",
        "        else:\n",
        "            lambda_high = lambda_mid - tolerance\n",
        "\n",
        "    lambda1 = lambda_mid - tolerance\n",
        "    return lambda1 * (prev_grad + loss_grad)\n",
        "\n",
        "def our_attack_mean(n_user, history_updates, all_updates, model_params, n_attacker, arch, criterion, user_train_data_tensors, user_train_label_tensors):\n",
        "\n",
        "    attacker_tr_data_tensor = user_train_data_tensors[0].cuda()\n",
        "    attacker_tr_label_tensor = user_train_label_tensors[0].cuda()\n",
        "\n",
        "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
        "    fed_model.eval()  # Ensure the model is in evaluation mode\n",
        "\n",
        "    new_grad = -all_updates[0].clone().detach().requires_grad_(True)\n",
        "    prev_grad = np.zeros(len(new_grad))\n",
        "\n",
        "    while True:\n",
        "        outputs = fed_model(attacker_tr_data_tensor)\n",
        "        loss = criterion(outputs, attacker_tr_label_tensor)\n",
        "\n",
        "        loss.backward(retain_graph=True)\n",
        "        param_grad = []\n",
        "        for param in fed_model.parameters():\n",
        "            param_grad.append(param.grad.view(-1).detach())\n",
        "\n",
        "        param_grad = torch.cat(param_grad)\n",
        "        prev_grad = new_grad\n",
        "        new_grad = projection(prev_grad, param_grad, history_updates, all_updates, n_user, n_attacker)\n",
        "\n",
        "        difference = torch.sum(torch.abs(new_grad - prev_grad)).item()\n",
        "        if difference < 1e-6:\n",
        "            print(\"STOPPED! Difference is too small, stop finding best attack grad\")\n",
        "            break\n",
        "\n",
        "        fed_model.zero_grad()\n",
        "\n",
        "    result_attacker_grads = [torch.tensor(new_grad, dtype=torch.float32)] * 10\n",
        "    return torch.stack(result_attacker_grads).cuda()\n"
      ],
      "metadata": {
        "id": "22HIfqLWn0KR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CGm85rwnrc8"
      },
      "source": [
        "## Set number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1umXispUnrc9"
      },
      "outputs": [],
      "source": [
        "nepochs= 300"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## no attacker"
      ],
      "metadata": {
        "id": "30lMESBcTtGp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3msRt__odqBQ"
      },
      "outputs": [],
      "source": [
        "batch_size=250\n",
        "resume=0\n",
        "\n",
        "schedule=[1000]\n",
        "\n",
        "gamma=.5\n",
        "opt = 'sgd'\n",
        "fed_lr=0.5\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "aggregation='bulyan'\n",
        "multi_k = False\n",
        "candidates = []\n",
        "\n",
        "at_type='our-agr'\n",
        "dev_type ='std'\n",
        "n_attackers=[0]\n",
        "\n",
        "arch='alexnet'\n",
        "chkpt='./'+aggregation\n",
        "\n",
        "past_mean = []\n",
        "past_abs_sum = []\n",
        "past_median = []\n",
        "past_lower_quartile = []\n",
        "past_upper_quartile = []\n",
        "past_std = []\n",
        "\n",
        "\n",
        "for n_attacker in n_attackers:\n",
        "    epoch_num = 0\n",
        "    best_global_acc = 0\n",
        "    best_global_te_acc = 0\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
        "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
        "\n",
        "    df = pd.DataFrame(columns = ['epoch', 'loss', 'validation accuracy', 'best validation accuracy'])\n",
        "\n",
        "    history = [[] for _ in range(n_users)]\n",
        "    model_grads = []\n",
        "    flag = False\n",
        "    while epoch_num <= nepochs:\n",
        "        user_grads=[]\n",
        "\n",
        "        for i in range(n_users): # 50\n",
        "            nbatches = user_tr_len[i]//batch_size\n",
        "\n",
        "            inputs = user_train_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "            targets = user_train_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "\n",
        "            targets = targets.type(torch.LongTensor)\n",
        "\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
        "\n",
        "            outputs = fed_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            fed_model.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            param_grad=[]\n",
        "            for param in fed_model.parameters():\n",
        "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
        "\n",
        "\n",
        "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
        "\n",
        "        if epoch_num in schedule:\n",
        "            for param_group in optimizer_fed.param_groups:\n",
        "                param_group['lr'] *= gamma\n",
        "                print('New learnin rate ', param_group['lr'])\n",
        "\n",
        "        updates_abs_mean = user_grads.abs().mean(dim=1, keepdim=True)\n",
        "\n",
        "        if n_attacker > 0:\n",
        "            model_params = torch.cat([p.data.view(-1) for p in fed_model.parameters()])\n",
        "            mal_updates = our_attack_mean(n_users, history, user_grads, model_params, n_attacker, arch, criterion, user_train_data_tensors, user_train_label_tensors)\n",
        "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0) # torch.Size([50, 2472266])\n",
        "        else:   # No attack\n",
        "            malicious_grads = user_grads\n",
        "\n",
        "        if not (malicious_grads.shape[0]==50):\n",
        "            print(malicious_grads.shape)\n",
        "\n",
        "        updates_abs_mean = malicious_grads.abs().mean(dim=1, keepdim=True)\n",
        "\n",
        "        for i in range(n_users):\n",
        "            history[i].append(updates_abs_mean[i].cpu())\n",
        "\n",
        "        agg_grads = our_mean_defense(malicious_grads, n_attacker, history)\n",
        "\n",
        "\n",
        "        del user_grads\n",
        "\n",
        "        start_idx=0\n",
        "\n",
        "        optimizer_fed.zero_grad()\n",
        "\n",
        "        model_grads=[]\n",
        "\n",
        "        for i, param in enumerate(fed_model.parameters()):\n",
        "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
        "            start_idx=start_idx+len(param.data.view(-1))\n",
        "            param_=param_.cuda()\n",
        "            model_grads.append(param_)\n",
        "\n",
        "        optimizer_fed.step(model_grads)\n",
        "\n",
        "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
        "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
        "\n",
        "        is_best = best_global_acc < val_acc\n",
        "\n",
        "        best_global_acc = max(best_global_acc, val_acc)\n",
        "\n",
        "        if is_best:\n",
        "            best_global_te_acc = te_acc\n",
        "\n",
        "\n",
        "        print('epoch: %d, %s: at %s n_at %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(epoch_num, aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc))\n",
        "        new_row = pd.DataFrame([{\n",
        "            'epoch': epoch_num,\n",
        "            'loss': val_loss,\n",
        "            'validation accuracy': val_acc,\n",
        "            'best validation accuracy': best_global_acc\n",
        "            }])\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "        df.to_csv('./our attack + 10 attacker + our defense.csv', index=False)\n",
        "\n",
        "        if val_loss > 1000:\n",
        "            print('val loss %f too high'%val_loss)\n",
        "            break\n",
        "\n",
        "        epoch_num+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWTYqauonrc9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the data\n",
        "for col in ['loss', 'validation accuracy', 'best validation accuracy']:\n",
        "    plt.plot(df['epoch'].tolist(), df[col].tolist())\n",
        "    plt.title(\"no attack\"+col)\n",
        "    plt.savefig('./no attack ' + col + '.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## no defense"
      ],
      "metadata": {
        "id": "ilTYNiZMlvp4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5K48z4IdqBQ"
      },
      "outputs": [],
      "source": [
        "batch_size=250\n",
        "resume=0\n",
        "\n",
        "schedule=[1000]\n",
        "\n",
        "gamma=.5\n",
        "opt = 'sgd'\n",
        "fed_lr=0.5\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "aggregation='bulyan'\n",
        "multi_k = False\n",
        "candidates = []\n",
        "\n",
        "at_type='our-agr'\n",
        "dev_type ='std'\n",
        "n_attackers=[10]\n",
        "\n",
        "arch='alexnet'\n",
        "chkpt='./'+aggregation\n",
        "\n",
        "past_mean = []\n",
        "past_abs_sum = []\n",
        "past_median = []\n",
        "past_lower_quartile = []\n",
        "past_upper_quartile = []\n",
        "past_std = []\n",
        "\n",
        "\n",
        "for n_attacker in n_attackers:\n",
        "    epoch_num = 0\n",
        "    best_global_acc = 0\n",
        "    best_global_te_acc = 0\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
        "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
        "\n",
        "    df = pd.DataFrame(columns = ['epoch', 'loss', 'validation accuracy', 'best validation accuracy'])\n",
        "\n",
        "    history = [[] for _ in range(n_users)]\n",
        "    model_grads = []\n",
        "    flag = False\n",
        "    while epoch_num <= nepochs:\n",
        "        user_grads=[]\n",
        "\n",
        "        for i in range(n_users): # 50\n",
        "            nbatches = user_tr_len[i]//batch_size\n",
        "\n",
        "            inputs = user_train_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "            targets = user_train_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "\n",
        "            targets = targets.type(torch.LongTensor)\n",
        "\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
        "\n",
        "            outputs = fed_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            fed_model.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            param_grad=[]\n",
        "            for param in fed_model.parameters():\n",
        "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
        "\n",
        "\n",
        "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
        "\n",
        "        if epoch_num in schedule:\n",
        "            for param_group in optimizer_fed.param_groups:\n",
        "                param_group['lr'] *= gamma\n",
        "                print('New learnin rate ', param_group['lr'])\n",
        "\n",
        "        updates_abs_mean = user_grads.abs().mean(dim=1, keepdim=True)\n",
        "\n",
        "        if n_attacker > 0:\n",
        "            model_params = torch.cat([p.data.view(-1) for p in fed_model.parameters()])\n",
        "            mal_updates = our_attack_mean(n_users, history, user_grads, model_params, n_attacker, arch, criterion, user_train_data_tensors, user_train_label_tensors)\n",
        "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0) # torch.Size([50, 2472266])\n",
        "        else:   # No attack\n",
        "            malicious_grads = user_grads\n",
        "\n",
        "        if not (malicious_grads.shape[0]==50):\n",
        "            print(malicious_grads.shape)\n",
        "\n",
        "        updates_abs_mean = malicious_grads.abs().mean(dim=1, keepdim=True)\n",
        "\n",
        "        for i in range(n_users):\n",
        "            history[i].append(updates_abs_mean[i].cpu())\n",
        "\n",
        "        agg_grads = no_defense_aggregation(malicious_grads)\n",
        "\n",
        "        del user_grads\n",
        "\n",
        "        start_idx=0\n",
        "\n",
        "        optimizer_fed.zero_grad()\n",
        "\n",
        "        model_grads=[]\n",
        "\n",
        "        for i, param in enumerate(fed_model.parameters()):\n",
        "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
        "            start_idx=start_idx+len(param.data.view(-1))\n",
        "            param_=param_.cuda()\n",
        "            model_grads.append(param_)\n",
        "\n",
        "        optimizer_fed.step(model_grads)\n",
        "\n",
        "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
        "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
        "\n",
        "        is_best = best_global_acc < val_acc\n",
        "\n",
        "        best_global_acc = max(best_global_acc, val_acc)\n",
        "\n",
        "        if is_best:\n",
        "            best_global_te_acc = te_acc\n",
        "\n",
        "\n",
        "        print('epoch: %d, %s: at %s n_at %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(epoch_num, aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc))\n",
        "        new_row = pd.DataFrame([{\n",
        "            'epoch': epoch_num,\n",
        "            'loss': val_loss,\n",
        "            'validation accuracy': val_acc,\n",
        "            'best validation accuracy': best_global_acc\n",
        "            }])\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "        df.to_csv('./our attack + 10 attacker + our defense.csv', index=False)\n",
        "\n",
        "        if val_loss > 1000:\n",
        "            print('val loss %f too high'%val_loss)\n",
        "            break\n",
        "\n",
        "        epoch_num+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC6T3x8SdqBR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the data\n",
        "for col in ['loss', 'validation accuracy', 'best validation accuracy']:\n",
        "    plt.plot(df['epoch'].tolist(), df[col].tolist())\n",
        "    plt.title(\"our attack + 10 attacker + no defense_\"+col)\n",
        "    plt.savefig('./our attack + 10 attacker + no defense ' + col + '.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_CfL1Rlnrc9"
      },
      "source": [
        "## Execute our attack + 10 attacker + our defense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkLfF-h4nrc9"
      },
      "outputs": [],
      "source": [
        "batch_size=250\n",
        "resume=0\n",
        "\n",
        "schedule=[1000]\n",
        "\n",
        "gamma=.5\n",
        "opt = 'sgd'\n",
        "fed_lr=0.5\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "aggregation='bulyan'\n",
        "multi_k = False\n",
        "candidates = []\n",
        "\n",
        "at_type='our-agr'\n",
        "dev_type ='std'\n",
        "n_attackers=[10]\n",
        "\n",
        "arch='alexnet'\n",
        "chkpt='./'+aggregation\n",
        "\n",
        "past_mean = []\n",
        "past_abs_sum = []\n",
        "past_median = []\n",
        "past_lower_quartile = []\n",
        "past_upper_quartile = []\n",
        "past_std = []\n",
        "\n",
        "\n",
        "for n_attacker in n_attackers:\n",
        "    epoch_num = 0\n",
        "    best_global_acc = 0\n",
        "    best_global_te_acc = 0\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
        "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
        "\n",
        "    df = pd.DataFrame(columns = ['epoch', 'loss', 'validation accuracy', 'best validation accuracy'])\n",
        "\n",
        "    history = [[] for _ in range(n_users)]\n",
        "    model_grads = []\n",
        "    flag = False\n",
        "    while epoch_num <= nepochs:\n",
        "        user_grads=[]\n",
        "\n",
        "        for i in range(n_users): # 50\n",
        "            nbatches = user_tr_len[i]//batch_size\n",
        "\n",
        "            inputs = user_train_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "            targets = user_train_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "\n",
        "            targets = targets.type(torch.LongTensor)\n",
        "\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
        "\n",
        "            outputs = fed_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            fed_model.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            param_grad=[]\n",
        "            for param in fed_model.parameters():\n",
        "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
        "\n",
        "\n",
        "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
        "\n",
        "        if epoch_num in schedule:\n",
        "            for param_group in optimizer_fed.param_groups:\n",
        "                param_group['lr'] *= gamma\n",
        "                print('New learnin rate ', param_group['lr'])\n",
        "\n",
        "        updates_abs_mean = user_grads.abs().mean(dim=1, keepdim=True)\n",
        "\n",
        "        if n_attacker > 0:\n",
        "            model_params = torch.cat([p.data.view(-1) for p in fed_model.parameters()])\n",
        "            mal_updates = our_attack_mean(n_users, history, user_grads, model_params, n_attacker, arch, criterion, user_train_data_tensors, user_train_label_tensors)\n",
        "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0) # torch.Size([50, 2472266])\n",
        "        else:   # No attack\n",
        "            malicious_grads = user_grads\n",
        "\n",
        "        if not (malicious_grads.shape[0]==50):\n",
        "            print(malicious_grads.shape)\n",
        "\n",
        "        updates_abs_mean = malicious_grads.abs().mean(dim=1, keepdim=True)\n",
        "\n",
        "        for i in range(n_users):\n",
        "            history[i].append(updates_abs_mean[i].cpu())\n",
        "\n",
        "        agg_grads = our_mean_defense(malicious_grads, n_attacker, history)\n",
        "\n",
        "\n",
        "        del user_grads\n",
        "\n",
        "        start_idx=0\n",
        "\n",
        "        optimizer_fed.zero_grad()\n",
        "\n",
        "        model_grads=[]\n",
        "\n",
        "        for i, param in enumerate(fed_model.parameters()):\n",
        "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
        "            start_idx=start_idx+len(param.data.view(-1))\n",
        "            param_=param_.cuda()\n",
        "            model_grads.append(param_)\n",
        "\n",
        "        optimizer_fed.step(model_grads)\n",
        "\n",
        "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
        "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
        "\n",
        "        is_best = best_global_acc < val_acc\n",
        "\n",
        "        best_global_acc = max(best_global_acc, val_acc)\n",
        "\n",
        "        if is_best:\n",
        "            best_global_te_acc = te_acc\n",
        "\n",
        "\n",
        "        print('epoch: %d, %s: at %s n_at %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(epoch_num, aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc))\n",
        "        new_row = pd.DataFrame([{\n",
        "            'epoch': epoch_num,\n",
        "            'loss': val_loss,\n",
        "            'validation accuracy': val_acc,\n",
        "            'best validation accuracy': best_global_acc\n",
        "            }])\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "        df.to_csv('./our attack + 10 attacker + our defense.csv', index=False)\n",
        "\n",
        "        if val_loss > 1000:\n",
        "            print('val loss %f too high'%val_loss)\n",
        "            break\n",
        "\n",
        "        epoch_num+=1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "ne =  304\n",
        "\n",
        "for j in range(10):\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  x = np.arange(ne)\n",
        "  for i in range(40):\n",
        "      y = history[i+10]\n",
        "      plt.plot(x, y, color='green')\n",
        "\n",
        "  # for i in range(10):\n",
        "  #     y = history[i]\n",
        "  #     plt.plot(x, y, color='red')\n",
        "  y = history[j]\n",
        "  plt.plot(x, y, color='red')\n",
        "\n",
        "\n",
        "  plt.title('history')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('mean')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.savefig('./history_malicious_'+str(j)+'.png')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "IRytxw-8lVbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ne = 304\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "x = np.arange(ne)\n",
        "for i in range(40):\n",
        "    y = history[i+10]\n",
        "    plt.plot(x, y, color='green')\n",
        "\n",
        "for i in range(10):\n",
        "    y = history[i]\n",
        "    plt.plot(x, y, color='red')\n",
        "\n",
        "plt.title('history')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('mean')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mgixfWyxlOT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I72iJwnddqBQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the data\n",
        "for col in ['loss', 'validation accuracy', 'best validation accuracy']:\n",
        "    plt.plot(df['epoch'].tolist(), df[col].tolist())\n",
        "    plt.title(\"our attack + 10 attacker + our defense_\"+col)\n",
        "    plt.savefig('./our attack + 10 attacker + our defense ' + col + '.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bGl6DReVmP8z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}